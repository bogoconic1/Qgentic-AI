

=== FILE: github.com_pytorch_pytorch_releases_page=1.2025-10-28T12_50_57.517Z.md ===

[Skip to content](https://github.com/pytorch/pytorch/releases?page=1#start-of-content)

## Navigation Menu

Toggle navigation

[Homepage](https://github.com/)

[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Freleases%3Fpage%3D1)

Appearance settings

- Platform









- [GitHub Copilot\\
\\
\\
\\
Write better code with AI](https://github.com/features/copilot)
- [GitHub Spark\\
\\
\\
New\\
\\
\\
Build and deploy intelligent apps](https://github.com/features/spark)
- [GitHub Models\\
\\
\\
New\\
\\
\\
Manage and compare prompts](https://github.com/features/models)
- [GitHub Advanced Security\\
\\
\\
\\
Find and fix vulnerabilities](https://github.com/security/advanced-security)
- [Actions\\
\\
\\
\\
Automate any workflow](https://github.com/features/actions)

- [Codespaces\\
\\
\\
\\
Instant dev environments](https://github.com/features/codespaces)
- [Issues\\
\\
\\
\\
Plan and track work](https://github.com/features/issues)
- [Code Review\\
\\
\\
\\
Manage code changes](https://github.com/features/code-review)
- [Discussions\\
\\
\\
\\
Collaborate outside of code](https://github.com/features/discussions)
- [Code Search\\
\\
\\
\\
Find more, search less](https://github.com/features/code-search)

Explore

- [Why GitHub](https://github.com/why-github)
- [Documentation](https://docs.github.com/)
- [GitHub Skills](https://skills.github.com/)
- [Blog](https://github.blog/)

Integrations

- [GitHub Marketplace](https://github.com/marketplace)
- [MCP Registry](https://github.com/mcp)

[View all features](https://github.com/features)

- Solutions







By company size

- [Enterprises](https://github.com/enterprise)
- [Small and medium teams](https://github.com/team)
- [Startups](https://github.com/enterprise/startups)
- [Nonprofits](https://github.com/solutions/industry/nonprofits)

By use case

- [App Modernization](https://github.com/solutions/use-case/app-modernization)
- [DevSecOps](https://github.com/solutions/use-case/devsecops)
- [DevOps](https://github.com/solutions/use-case/devops)
- [CI/CD](https://github.com/solutions/use-case/ci-cd)
- [View all use cases](https://github.com/solutions/use-case)

By industry

- [Healthcare](https://github.com/solutions/industry/healthcare)
- [Financial services](https://github.com/solutions/industry/financial-services)
- [Manufacturing](https://github.com/solutions/industry/manufacturing)
- [Government](https://github.com/solutions/industry/government)
- [View all industries](https://github.com/solutions/industry)

[View all solutions](https://github.com/solutions)

- Resources







Topics

- [AI](https://github.com/resources/articles?topic=ai)
- [DevOps](https://github.com/resources/articles?topic=devops)
- [Security](https://github.com/resources/articles?topic=security)
- [Software Development](https://github.com/resources/articles?topic=software-development)
- [View all](https://github.com/resources/articles)

Explore

- [Learning Pathways](https://resources.github.com/learn/pathways)
- [Events & Webinars](https://github.com/resources/events)
- [Ebooks & Whitepapers](https://github.com/resources/whitepapers)
- [Customer Stories](https://github.com/customer-stories)
- [Partners](https://github.com/partners)
- [Executive Insights](https://github.com/solutions/executive-insights)

- Open Source









- [GitHub Sponsors\\
\\
\\
\\
Fund open source developers](https://github.com/sponsors)

- [The ReadME Project\\
\\
\\
\\
GitHub community articles](https://github.com/readme)

Repositories

- [Topics](https://github.com/topics)
- [Trending](https://github.com/trending)
- [Collections](https://github.com/collections)

- Enterprise









- [Enterprise platform\\
\\
\\
\\
AI-powered developer platform](https://github.com/enterprise)

Available add-ons

- [GitHub Advanced Security\\
\\
\\
\\
Enterprise-grade security features](https://github.com/security/advanced-security)
- [Copilot for business\\
\\
\\
\\
Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)
- [Premium Support\\
\\
\\
\\
Enterprise-grade 24/7 support](https://github.com/premium-support)

- [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search


Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

Cancel
Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).


Cancel
Create saved search

[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Freleases%3Fpage%3D1)

[Sign up](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Freleases%2Findex&source=header-repo&source_repo=pytorch%2Fpytorch)

Appearance settings

Resetting focus

You signed in with another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=1) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=1) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=1) to refresh your session.Dismiss alert

{{ message }}

[pytorch](https://github.com/pytorch)/ **[pytorch](https://github.com/pytorch/pytorch)** Public

- Couldn't load subscription status.
Retry











### Uh oh!







There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=1).

- [Fork\\
25.7k](https://github.com/login?return_to=%2Fpytorch%2Fpytorch)
- [Star\\
94.3k](https://github.com/login?return_to=%2Fpytorch%2Fpytorch)


- [Code](https://github.com/pytorch/pytorch)
- [Issues5k+](https://github.com/pytorch/pytorch/issues)
- [Pull requests1.5k](https://github.com/pytorch/pytorch/pulls)
- [Actions](https://github.com/pytorch/pytorch/actions)
- [Projects12](https://github.com/pytorch/pytorch/projects)
- [Wiki](https://github.com/pytorch/pytorch/wiki)
- [Security4](https://github.com/pytorch/pytorch/security)






[**Uh oh!**](https://github.com/pytorch/pytorch/security)

[There was an error while loading.](https://github.com/pytorch/pytorch/security) [Please reload this page](https://github.com/pytorch/pytorch/releases?page=1).

- [Insights](https://github.com/pytorch/pytorch/pulse)

Additional navigation options

- [Code](https://github.com/pytorch/pytorch)
- [Issues](https://github.com/pytorch/pytorch/issues)
- [Pull requests](https://github.com/pytorch/pytorch/pulls)
- [Actions](https://github.com/pytorch/pytorch/actions)
- [Projects](https://github.com/pytorch/pytorch/projects)
- [Wiki](https://github.com/pytorch/pytorch/wiki)
- [Security](https://github.com/pytorch/pytorch/security)
- [Insights](https://github.com/pytorch/pytorch/pulse)

# Releases: pytorch/pytorch

[Releases](https://github.com/pytorch/pytorch/releases) [Tags](https://github.com/pytorch/pytorch/tags)

Releases · pytorch/pytorch

## 2.9 Release Notes

2 weeks ago
15 Oct 17:12


![@seemethere](https://avatars.githubusercontent.com/u/1700823?s=40&v=4)[seemethere](https://github.com/seemethere)

[v2.9.0](https://github.com/pytorch/pytorch/tree/v2.9.0)

[`0fabc3b`](https://github.com/pytorch/pytorch/commit/0fabc3ba44823f257e70ce397d989c8de5e362c1)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Oct 8, 2025, 09:09 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=1).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[2.9 Release Notes](https://github.com/pytorch/pytorch/releases/tag/v2.9.0)[Latest](https://github.com/pytorch/pytorch/releases/latest)

[Latest](https://github.com/pytorch/pytorch/releases/latest)

# PyTorch 2.9.0 Release Notes

- [Highlights](https://github.com/pytorch/pytorch/releases?page=1#highlights)
- [Backwards Incompatible Changes](https://github.com/pytorch/pytorch/releases?page=1#backwards-incompatible-changes)
- [Deprecations](https://github.com/pytorch/pytorch/releases?page=1#deprecations)
- [New Features](https://github.com/pytorch/pytorch/releases?page=1#new-features)
- [Improvements](https://github.com/pytorch/pytorch/releases?page=1#improvements)
- [Bug fixes](https://github.com/pytorch/pytorch/releases?page=1#bug-fixes)
- [Performance](https://github.com/pytorch/pytorch/releases?page=1#performance)
- [Documentation](https://github.com/pytorch/pytorch/releases?page=1#documentation)
- [Developers](https://github.com/pytorch/pytorch/releases?page=1#developers)
- [Security](https://github.com/pytorch/pytorch/releases?page=1#security)

# Highlights

|     |
| --- |
| **Unstable (API-Unstable)** |
| Updates to the stable libtorch ABI for third-party C++/CUDA extensions |
| Symmetric memory that enables easy programming of multi-GPU kernels |
| The ability to arbitrarily toggle error or resume on graph breaks in torch.compile |
| Expanded wheel variant support to include ROCm, XPU and CUDA 13 |
| FlexAttention enablement on Intel GPUs |
| Flash decoding optimization based on FlexAttention on X86 CPU |
| ARM Platform improvements and optimizations |
| Enablement of Linux aarch64 binary wheel builds across all supported CUDA versions |

For more details about these highlighted features, you can look at the [release blogpost](https://pytorch.org/blog/pytorch-2-9/). Below are the full release notes for this release.

# Backwards Incompatible Changes

## Min supported Python version is now 3.10 ( [\#162310](https://github.com/pytorch/pytorch/pull/162310))

The minimum version of Python required for PyTorch 2.9.0 is 3.10. We also have 3.14 and 3.14t available as preview with this release.

## Undefined behavior when an output of a custom operator shares storage with an input

This is a reminder that outputs of PyTorch custom operators (that are registered using the `torch.library` or `TORCH_LIBRARY` APIs) are not allowed to return Tensors that share storage with input tensors. The violation of this condition leads to undefined behavior: sometimes the result will be correct, sometimes it will be garbage.

After [#163227](https://github.com/pytorch/pytorch/pull/163227), custom operators that violated this condition that previously returned correct results under `torch.compile` may now return silently incorrect results under `torch.compile`. Because this is changing the behavior of undefined behavior, we do not consider this to be a bug, but we are still documenting it in this section as a "potentially unexpected behavior change".

This is one of the conditions checked for by [`torch.library.opcheck`](https://docs.pytorch.org/docs/stable/library.html#testing-custom-ops) and is mentioned in [The Custom Operators Manual](https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#bookmark=id.4c0um7xkba6e)

### More details

Outputs of PyTorch custom operators are not allowed to return Tensors that share storage with input tensors

For example, the following two custom operators are not valid custom operators:

```
@torch.library.custom_op("mylib::foo", mutates_args=())
def foo(x: torch.Tensor) -> torch.Tensor:
    # the result of `foo` must not directly be an input to foo.
    return x

@torch.library.custom_op("mylib::bar", mutates_args=())
def bar(x: torch.Tensor) -> torch.Tensor:
    # the result of bar must not be a view of an input of bar
    return x.view(-1)
```

The easiest workaround is to add an extra `.clone()` to the outputs:

```
@torch.library.custom_op("mylib::foo", mutates_args=())
def foo(x: torch.Tensor) -> torch.Tensor:
    return x.clone()

@torch.library.custom_op("mylib::bar", mutates_args=())
def bar(x: torch.Tensor) -> torch.Tensor:
    return x.view(-1).clone()
```

A common way to get into this situation is for a user to want to create a custom operator that sometimes mutates the input in-place and sometimes returns a new Tensor, like in the following example.

```
@torch.library.custom_op("mylib::baz", mutates_args=["x"])
def baz(x: torch.Tensor) -> torch.Tensor:
    if inplace:
        x.sin_()
        return x
    else:
        return x.sin()
```

This dynamism is not supported and leads to undefined behavior. The workaround is to split the custom operator into two custom operators, one that always mutates the input in-place, and another that always returns a new Tensor.

```
@torch.library.custom_op("mylib::baz_outplace", mutates_args=())
def baz_outplace(x: torch.Tensor) -> torch.Tensor:
    return x.sin()

@torch.library.custom_op("mylib::baz_inplace", mutates_args=["x"])
def baz_inplace(x: torch.Tensor) -> torch.Tensor:
    x.sin_()

def baz(x):
    if inplace:
        baz_inplace(x)
        return x
    else:
        return baz_outplace(x)
```

## Build metal kernels of MacOS-14+ and remove all pre-MacOS-14 specific logic, requires MacOS-14+ going forward ( [\#159733](https://github.com/pytorch/pytorch/pull/159733), [\#159912](https://github.com/pytorch/pytorch/pull/159912))

PyTorch MPS is only supported on MacOS-14 or later. If you need to use MPS on MacOS Ventura, please avoid updating to Python-3.9 or above

## Upgrade to DLPack 1.0 ( [\#145000](https://github.com/pytorch/pytorch/pull/145000))

This upgrade is doing the same BC-breaking changes as the DLPack release. Objects in `torch.utils.dlpack` have been updated to reflect these changes, such as `DLDeviceType`.

See the PR for details on the exact changes and how to update your code.

## Raise appropriate errors in `torch.cat` ( [\#158249](https://github.com/pytorch/pytorch/pull/158249))

`torch.cat` now raises `ValueError`, `IndexError` or `TypeError` where appropriate instead of the generic `RuntimeError`. If you code was catching these errors, you can update to catch the new error type.

## Default to `dynamo=True` for ONNX exporter ( [\#159646](https://github.com/pytorch/pytorch/pull/159646), [\#162726](https://github.com/pytorch/pytorch/pull/162726))

Previously `torch.onnx.export(...)` used the legacy TorchScript exporter if no arguments were provied. The ONNX exporter now uses the newer `torch.export.export` pipeline by default ( `dynamo=True`). This change improves graph fidelity and future-proofs exports, but may surface graph capture errors that were previously masked or handled differently.

Previously in torch 2.8.0:

```
# API calls the legacy exporter with dynamo=False
torch.onnx.export(...)
```

Now in torch 2.9.0:

```
# To preserve the original behavior
torch.onnx.export(..., dynamo=False)

# Export onnx model through torch.export.export
torch.onnx.export(...)
```

Recommendation: first try the new default; only fall back if you hit blocking issues and report them upstream.

Long term solution: fix the root cause instead of relying on fallback or TorchScript exporter.

## Switch off runtime asserts by default in Export in favor of a shape guards function ( [\#160111](https://github.com/pytorch/pytorch/pull/160111), [\#161178](https://github.com/pytorch/pytorch/pull/161178), [\#161794](https://github.com/pytorch/pytorch/pull/161794))

To enable runtime asserts, use `export(..., prefer_deferred_runtime_asserts_over_guards=True)`. Also kills the `allow_complex_guards_as_runtime_asserts` flag, merging it into the former option.

Additionally, `exported_program.module()` will generate a call to a `_guards_fn` submodule that will run additional checks on inputs. Users who do not want this behavior can either remove this call in the graph, or do `exported_program.module(check_guards=False)` to avoid the generation.

## Set default opset to 20 in ONNX ( [\#158802](https://github.com/pytorch/pytorch/pull/158802))

Opset 20 enables newer operator definitions. If your tooling or downstream runtime only supports opset 18, pin it explicitly. For the latest ONNX operators, you can experiment with opset 23.

Previously in torch 2.8.0:

```
# opset_version=18
torch.onnx.export(...)
```

Now in torch 2.9.0:

```
# To preserve the original behavior
torch.onnx.export(..., opset_version=18)

# New: opset_version=20
torch.onnx.export(...)

# Use the latest supported opset: opset_version=23
torch.onnx.export(..., opset_version=23)
```

## Drop `draft_export` in exporter API ( [\#161454](https://github.com/pytorch/pytorch/pull/161454), [\#162225](https://github.com/pytorch/pytorch/pull/162225))

Remove implicit draft tracing from the default exporter path, achieving clearer behaviour and faster failures.

The expensive `torch.export.draft_export` diagnostic path is no longer auto-invoked (which could take hours on large models). You can still opt in for deep diagnostics:

Previously in torch 2.8.0:

```
# If both torch.export.export(..., strict=False) and
# torch.export.export(..., strict=True) fail to capture
# the model graph, torch.export.draft_export(...) will be triggered,
# and uses real tensor to trace/export the model.
#
# Inside export_to_onnx.py:
#  ... torch.onnx.export(..., dynamo=True)
python export_to_onnx.py
```

Now in torch 2.9.0:

```
# To trigger torch.export.draft_export once
# torch.export.export strict=False/True both
# fail:

TORCH_ONNX_ENABLE_DRAFT_EXPORT=True python export_to_onnx.py
```

## Remove `torch.onnx.dynamo_export` and the `onnxrt` torch compile backend ( [\#158130](https://github.com/pytorch/pytorch/pull/158130), [\#158258](https://github.com/pytorch/pytorch/pull/158258))

`torch.onnx.dynamo_export` is removed. Please use `torch.onnx.export` instead.

The experimental ONNX Runtime compile backend ( `torch.compile(backend="onnxrt")`) is no longer supported.

## Remove `torch.onnx.enable_fake_mode` ( [\#161222](https://github.com/pytorch/pytorch/pull/161222))

The `dynamo=True` mode uses `FakeTensor` s by default which is memory efficient.

## Some public facing ONNX utility APIs for the TorchScript based exporter are now private ( [\#161323](https://github.com/pytorch/pytorch/pull/161323))

Deprecated members in `torch.onnx.verification` are removed. Previously private `torch.onnx.symbolic_opsets*` functions will no longer be accessible. Consider making a copy of the source code if you need to access any private functions for compatibility with the TorchScript based exporter.

## Remove `torch.onnx.symbolic_caffe2` ( [\#157102](https://github.com/pytorch/pytorch/pull/157102))

Support for `caffe2` in the ONNX exporter has ended and is removed.

## Remove `/d2implyavx512upperregs` flag that slows build ( [\#159431](https://github.com/pytorch/pytorch/pull/159431))

Re-introduced AVX512 optimizations for Windows VS2022 builds, may cause issues with specific versions of VS2022, see [#145702](https://github.com/pytorch/pytorch/issues/145702)

## Add `ScalarType` to shim conversion and `stable::Tensor.scalar_type` ( [\#160557](https://github.com/pytorch/pytorch/pull/160557))

Before, user extensions could only in abstract...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.9.0)

Assets3

- [pytorch-v2.9.0.tar.gz](https://github.com/pytorch/pytorch/releases/download/v2.9.0/pytorch-v2.9.0.tar.gz)



sha256:c6980af3c0ea311f49f90987982be715e4d702539fea41e52f55ad7f0b105dc3



333 MB2 weeks ago2025-10-15T17:15:48Z

- [Source code(zip)](https://github.com/pytorch/pytorch/archive/refs/tags/v2.9.0.zip)

3 weeks ago2025-10-09T01:09:57Z

- [Source code(tar.gz)](https://github.com/pytorch/pytorch/archive/refs/tags/v2.9.0.tar.gz)

3 weeks ago2025-10-09T01:09:57Z


![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)18github-actions\[bot\], cyyever, nadzhou, little-sparkleeesss, obitodaitu, DefTruth, LuisMiSanVe, michaelnny, FomalhautWeisszwerg, ancestor-mithril, and 8 more reacted with thumbs up emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)39atalman, cxzhong, RaulPL, zhewenl, healy-hub, Finn-Hecker, khushi-411, akihironitta, CHC383, lckr, and 29 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)4shinGangan, nadzhou, uwu-420, and shokry110 reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)6github-actions\[bot\], nadzhou, Puiching-Memory, uwu-420, NoeFontana, and shokry110 reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)3shinGangan, HailToThee, and shokry110 reacted with eyes emoji

55 people reacted

## PyTorch 2.8.0 Release

Aug 6
06 Aug 17:06


![@jbschlosser](https://avatars.githubusercontent.com/u/75754324?s=40&v=4)[jbschlosser](https://github.com/jbschlosser)

[v2.8.0](https://github.com/pytorch/pytorch/tree/v2.8.0)

[`ba56102`](https://github.com/pytorch/pytorch/commit/ba56102387ef21a3b04b357e5b183d48f0afefc7)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Aug 4, 2025, 12:51 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.8.0 Release](https://github.com/pytorch/pytorch/releases/tag/v2.8.0)

# PyTorch 2.8.0 Release Notes

- [Highlights](https://github.com/pytorch/pytorch/releases?page=1#highlights)
- [Backwards Incompatible Changes](https://github.com/pytorch/pytorch/releases?page=1#backwards-incompatible-changes)
- [Deprecations](https://github.com/pytorch/pytorch/releases?page=1#deprecations)
- [New Features](https://github.com/pytorch/pytorch/releases?page=1#new-features)
- [Improvements](https://github.com/pytorch/pytorch/releases?page=1#improvements)
- [Bug fixes](https://github.com/pytorch/pytorch/releases?page=1#bug-fixes)
- [Performance](https://github.com/pytorch/pytorch/releases?page=1#performance)
- [Documentation](https://github.com/pytorch/pytorch/releases?page=1#documentation)
- [Developers](https://github.com/pytorch/pytorch/releases?page=1#developers)

# Highlights

|     |
| --- |
| **Unstable** |
| torch::stable::Tensor |
| High-performance quantized LLM inference on Intel CPUs with native PyTorch |
| Experimental Wheel Variant Support |
| Inductor CUTLASS backend support |
| Inductor Graph Partition for CUDAGraph |
| Control Flow Operator Library |
| HuggingFace SafeTensors support in PyTorch Distributed Checkpointing |
| SYCL support in PyTorch CPP Extension API |
| A16W4 on XPU Device |
| Hierarchical compilation with torch.compile |
| Intel GPU distributed backend (XCCL) support |

For more details about these highlighted features, you can look at the [release blogpost](https://pytorch.org/blog/pytorch-2-8/).

Below are the full release notes for this release.

# Tracked Regressions

### Windows wheel builds with CUDA 12.9.1 stack overflow during build ( [\#156181](https://github.com/pytorch/pytorch/issues/156181))

Due to a bug introduced in CUDA 12.9.1, we are unable to complete full Windows wheel builds with this

version, as compilation of `torch.segment_reduce()` crashes the build. Thus, we provide a wheel

without `torch.segment_reduce()` included in order to sidestep the issue. If you need support

for `torch.segment_reduce()`, please utilize a different version.

# Backwards Incompatible Changes

## CUDA Support

### Removed support for Maxwell and Pascal architectures with CUDA 12.8 and 12.9 builds ( [\#157517](https://github.com/pytorch/pytorch/issues/157517), [\#158478](https://github.com/pytorch/pytorch/pull/158478), [\#158744](https://github.com/pytorch/pytorch/pull/158744))

Due to binary size limitations, support for sm50 - sm60 architectures with CUDA 12.8 and 12.9 has

been dropped for the 2.8.0 release. If you need support for these architectures, please utilize

CUDA 12.6 instead.

## Python Frontend

### Calling an op with an input dtype that is unsupported now raises `NotImplementedError` instead of `RuntimeError` ( [\#155470](https://github.com/pytorch/pytorch/pull/155470))

Please update exception handling logic to reflect this.

In 2.7.0

```
try:
    torch.nn.Hardshrink()(torch.randint(0, 5, (10,)))
except RuntimeError:
    ...

```

In 2.8.0

```
try:
    torch.nn.Hardshrink()(torch.randint(0, 5, (10,)))
except NotImplementedError:
    ...

```

### Added missing in-place on view check to custom `autograd.Function` ( [\#153094](https://github.com/pytorch/pytorch/pull/153094))

In 2.8.0, if a custom `autograd.Function` mutates a view of a leaf requiring grad,

it now properly raises an error. Previously, it would silently leak memory.

```
   class Func(torch.autograd.Function):
        @staticmethod
        def forward(ctx, inp):
            inp.add_(1)
            ctx.mark_dirty(inp)
            return inp

        @staticmethod
        def backward(ctx, gO):
            pass

    a = torch.tensor([1.0, 2.0], requires_grad=True)
    b = a.view_as(a)
    Func.apply(b)

```

Output:

Version 2.7.0

```
Runs without error, but leaks memory

```

Version 2.8.0

```
RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation

```

### An error is now properly thrown for the out variant of `tensordot` when called with a `requires_grad=True` tensor ( [\#150270](https://github.com/pytorch/pytorch/pull/150270))

Please avoid passing an out tensor with `requires_grad=True` as gradients cannot be

computed for this tensor.

In 2.7.0

```
a = torch.empty((4, 2), requires_grad=True)
b = torch.empty((2, 4), requires_grad=True)
c = torch.empty((2, 2), requires_grad=True)
# does not error, but gradients for c cannot be computed
torch.tensordot(a, b, dims=([1], [0]), out=c)

```

In 2.8.0

```
a = torch.empty((4, 2), requires_grad=True)
b = torch.empty((2, 4), requires_grad=True)
c = torch.empty((2, 2), requires_grad=True)
torch.tensordot(a, b, dims=([1], [0]), out=c)
# RuntimeError: tensordot(): the 'out' tensor was specified and requires gradients, and
# its shape does not match the expected result. Either remove the 'out' argument, ensure
# it does not require gradients, or make sure its shape matches the expected output.

```

## torch.compile

### Specialization of a tensor shape with `mark_dynamic` applied now correctly errors ( [\#152661](https://github.com/pytorch/pytorch/pull/152661))

Prior to 2.8, it was possible for a guard on a symbolic shape to be incorrectly

omitted if the symbolic shape evaluation was previously tested with guards

suppressed (this often happens within the compiler itself). This has been fixed

in 2.8 and usually will just silently "do the right thing" and add the correct

guard. However, if the new guard causes a tensor marked with `mark_dynamic` to become

specialized, this can result in an error. One workaround is to use

`maybe_mark_dynamic` instead of `mark_dynamic`.

See the discussion in issue [#157921](https://github.com/pytorch/pytorch/issues/157921) for more

context.

Version 2.7.0

```
import torch

embed = torch.randn(2, 8192)
x = torch.zeros(8192)

torch._dynamo.mark_dynamic(x, 0)

@torch.compile
def f(embedding_indices, x):
    added_tokens_mask = torch.where(x > 10000, 1, 0)
    ei = torch.narrow(embedding_indices, 1, 0, x.size(0))
    return ei.clone()

f(embed, x)
```

Version 2.8.0

```
import torch

embed = torch.randn(2, 8192)
x = torch.zeros(8192)

torch._dynamo.maybe_mark_dynamic(x, 0)

@torch.compile
def f(embedding_indices, x):
    added_tokens_mask = torch.where(x > 10000, 1, 0)
    ei = torch.narrow(embedding_indices, 1, 0, x.size(0))
    return ei.clone()

f(embed, x)
```

### Several config variables related to `torch.compile` have been renamed or removed

- Dynamo config variable `enable_cpp_framelocals_guard_eval` has changed to no longer have any effect ( [#151008](https://github.com/pytorch/pytorch/pull/151008)).
- Inductor config variable `rocm.n_max_profiling_configs` is deprecated ( [#152341](https://github.com/pytorch/pytorch/pull/152341)).


Instead, use ck-tile based configs `rocm.ck_max_profiling_configs` and

`rocm.ck_tile_max_profiling_configs`.
- Inductor config variable `autotune_fallback_to_aten` is deprecated ( [#154331](https://github.com/pytorch/pytorch/pull/154331)).


Inductor will no longer silently fall back to `ATen`. Please add `"ATEN"` to

`max_autotune_gemm_backends` for the old behavior.
- Inductor config variables `use_mixed_mm` and `mixed_mm_choice` are deprecated ( [#152071](https://github.com/pytorch/pytorch/pull/152071)). Inductor now supports prologue fusion, so there is no need for


special cases now.
- Inductor config setting `descriptive_names = False` is deprecated ( [#151481](https://github.com/pytorch/pytorch/pull/151481)). Please use one of the other available


options: `"torch"`, `"original_aten"`, or `"inductor_node"`.
- `custom_op_default_layout_constraint` has moved from inductor config to functorch config ( [#148104](https://github.com/pytorch/pytorch/pull/148104)). Please reference it via

`torch._functorch.config.custom_op_default_layout_constraint` instead of

`torch._inductor.config.custom_op_default_layout_constraint`.
- AOTI config variable `emit_current_arch_binary` is deprecated ( [#155768](https://github.com/pytorch/pytorch/pull/155768)).
- AOTI config variable `aot_inductor.embed_cubin` has been renamed to `aot_inductor.embed_kernel_binary` ( [#154412](https://github.com/pytorch/pytorch/pull/154412)).
- AOTI config variable `aot_inductor.compile_wrapper_with_O0` has been renamed to `compile_wrapper_opt_level` ( [#148714](https://github.com/pytorch/pytorch/pull/148714)).

### Added a stricter aliasing/mutation check for `HigherOrderOperator` s (e.g. `cond`), which will explicitly error out if alias/mutation among inputs and outputs is unsupported ( [\#148953](https://github.com/pytorch/pytorch/pull/148953), [\#146658](https://github.com/pytorch/pytorch/pull/146658)).

For affected `HigherOrderOperator` s, add `.clone()` to aliased outputs to address this.

Version 2.7.0

```
import torch

@torch.compile(backend="eager")
def fn(x):
    return torch.cond(x.sum() > 0, lambda x: x, lambda x: x + 1, [x])

fn(torch.ones(3))
```

Version 2.8.0

```
import torch

@torch.compile(backend="eager")
def fn(x):
    return torch.cond(x.sum() > 0, lambda x: x.clone(), lambda x: x + 1, [x])

fn(torch.ones(3))
```

### `guard_or_x` and `definitely_x` have been consolidated ( [\#152463](https://github.com/pytorch/pytorch/pull/152463))

We removed `definitely_true` / `definitely_false` and associated APIs, replacing them with

`guard_or_true` / `guard_or_false`, which offer similar functionality and can be used to

achieve the same effect. Please migrate to the latter.

Version 2.7.0

```
from torch.fx.experimental.symbolic_shapes import definitely_false, definitely_true

...
if definitely_true(x):
  ...

if definitely_false(y):
  ...
```

Version 2.8.0

```
from torch.fx.experimental.symbolic_shapes import guard_or_false, guard_or_true

...
if guard_or_false(x):
  ...

# alternatively: if guard_or_false(torch.sym_not(y))
if not guard_or_true(y):
  ...
```

## torch.export

### `torch.export.export_for_inference` has been removed in favor of `torch.export.export_for_training().run_decompositions()` ( [\#149078](https://github.com/pytorch/pytorch/pull/149078))

Version 2.7.0

```
import torch

...
exported_program = torch.export.export_for_inference(mod, args, kwargs)
```

Version 2.8.0

```
import torch

...
exported_program = torch.export.export_for_training(
    mod, args, kwargs
).run_decompositions(decomp_table=decomp_table)
```

### Switched default to `strict=False` in `torch.export.export` and `export_for_training` ( [\#148790](https://github.com/pytorch/pytorch/pull/148790), [\#150941](https://github.com/pytorch/pytorch/pull/150941))

This differs from the previous release default of `strict=True`. To revert to the old default

behavior, please explicitly pass `strict=True`.

Version 2.7.0

```
import torch

# default behavior is strict=True
torch.export.export(...)
torch.export.export_for_training(...)
```

Version 2.8.0

```
import torch

# strict=True must be explicitly passed to get the old behavior
torch.export.export(..., strict=True)
torch.export.export_for_training(..., strict=True)
```

## ONNX

### Default opset in `torch.onnx.export` is now 18 ( [\#156023](https://github.com/pytorch/pytorch/pull/156023))

When `dynamo=False`, th...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.8.0)

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)40akihironitta, klemens-floege, nguyenphuminh, araffin, R0n12, Japyh, IlyasMoutawwakil, Kitsunp, benettia, tsukumijima, and 30 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)4knotgrass, wolegca, DingZhaohai, and NevermindNilas reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)23akihironitta, TyronMott, IlyasMoutawwakil, uygarpolat, Rohanjames1997, benettia, mihaimoga, yoshoku, trsvchn, github-actions\[bot\], and 13 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)15minpeter, akihironitta, thomasjo, IlyasMoutawwakil, mpecha, trsvchn, sanodmendis, DefTruth, Brensom, wolegca, and 5 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)21healy-hub, akihironitta, klemens-floege, nguyenphuminh, cm4ker, dmholtz, Hydran00, ScottTodd, trsvchn, github-actions\[bot\], and 11 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)6gugarosa, DefTruth, wolegca, DingZhaohai, shinGangan, and sonukaloshiya reacted with eyes emoji

70 people reacted

## PyTorch 2.7.1 Release, bug fix release

Jun 4
04 Jun 18:13


![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)

[v2.7.1](https://github.com/pytorch/pytorch/tree/v2.7.1)

[`e2d141d`](https://github.com/pytorch/pytorch/commit/e2d141dbde55c2a4370fac5165b0561b6af4798b)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on May 28, 2025, 09:23 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.7.1 Release, bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.7.1)

This release is meant to fix the following issues (regressions / silent correctness):

### Torch.compile

Fix Excessive cudagraph re-recording for HF LLM models ( [#152287](https://github.com/pytorch/pytorch/pull/152287))

Fix torch.compile on some HuggingFace models ( [#151154](https://github.com/pytorch/pytorch/pull/151154))

Fix crash due to Exception raised inside torch.autocast ( [#152503](https://github.com/pytorch/pytorch/pull/152503))

Improve Error logging in torch.compile ( [#149831](https://github.com/pytorch/pytorch/pull/149831))

Mark mutable custom operators as cacheable in torch.compile ( [#151194](https://github.com/pytorch/pytorch/pull/151194))

Implement workaround for a graph break with older version einops ( [#153925](https://github.com/pytorch/pytorch/pull/153925))

Fix an issue with tensor.view(dtype).copy\_(...) ( [#151598](https://github.com/pytorch/pytorch/pull/151598))

### Flex Attention

Fix assertion error due to inductor permuting inputs to flex attention ( [#151959](https://github.com/pytorch/pytorch/pull/151959))

Fix performance regression on nanogpt speedrun ( [#152641](https://github.com/pytorch/pytorch/pull/152641))

### Distributed

Fix extra CUDA context created by barrier ( [#149144](https://github.com/pytorch/pytorch/pull/149144))

Fix an issue related to Distributed Fused Adam in Rocm/APEX when using nccl\_ub feature ( [#150010](https://github.com/pytorch/pytorch/pull/150010))

Add a workaround random hang in non-blocking API mode in NCCL 2.26 ( [#154055](https://github.com/pytorch/pytorch/pull/154055))

### MacOS

Fix MacOS compilation error with Clang 17 ( [#151316](https://github.com/pytorch/pytorch/pull/151344))

Fix binary kernels produce incorrect results when one of the tensor arguments is from a wrapped scalar on MPS devices ( [#152997](https://github.com/pytorch/pytorch/pull/152997))

### Other

Improve PyTorch Wheel size due to introduction of addition of 128 bit vectorization ( [#148320](https://github.com/pytorch/pytorch/pull/148320)) ( [#152396](https://github.com/pytorch/pytorch/pull/152396))

Fix fmsub function definition ( [#152075](https://github.com/pytorch/pytorch/pull/152075))

Fix Floating point exception in torch.mkldnn\_max\_pool2d ( [#151848](https://github.com/pytorch/pytorch/pull/151848))

Fix abnormal inference output with XPU:1 device ( [#153067](https://github.com/pytorch/pytorch/pull/153067))

Fix Illegal Instruction Caused by grid\_sample on Windows ( [#152613](https://github.com/pytorch/pytorch/pull/152613))

Fix ONNX decomposition does not preserve custom CompositeImplicitAutograd ops ( [#151826](https://github.com/pytorch/pytorch/pull/151826))

Fix error with dynamic linking of libgomp library ( [#150084](https://github.com/pytorch/pytorch/pull/150084))

Fix segfault in profiler with Python 3.13 ( [#153848](https://github.com/pytorch/pytorch/pull/153848))

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)30akihironitta, D0n-A, NeilTohno, Anuvadak, alexchenfeng, github-actions\[bot\], Dengda98, Dinesh-Mareedu, HuayuChen2004, 3294734448, and 20 more reacted with thumbs up emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)8akihironitta, mihaimoga, github-actions\[bot\], hassonofer, wanderingeek, okoge-kaz, zangyook, and farazkh80 reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)4akihironitta, 1taroh, Brensom, and VitthalGupta reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)7AjayKMehta, akihironitta, uygarpolat, github-actions\[bot\], hossein-ahmadzadeh, farazkh80, and ParamDeshpande reacted with rocket emoji

40 people reacted

## PyTorch 2.7.0 Release

Apr 23
23 Apr 16:16


![@janeyx99](https://avatars.githubusercontent.com/u/31798555?s=40&v=4)[janeyx99](https://github.com/janeyx99)

[v2.7.0](https://github.com/pytorch/pytorch/tree/v2.7.0)

[`1341794`](https://github.com/pytorch/pytorch/commit/134179474539648ba7dee1317959529fbd0e7f89)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Apr 15, 2025, 07:01 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.7.0 Release](https://github.com/pytorch/pytorch/releases/tag/v2.7.0)

# PyTorch 2.7.0 Release Notes

- [Highlights](https://github.com/pytorch/pytorch/releases?page=1#highlights)
- [Tracked Regressions](https://github.com/pytorch/pytorch/releases?page=1#tracked-regressions)
- [Backwards Incompatible Changes](https://github.com/pytorch/pytorch/releases?page=1#backwards-incompatible-changes)
- [Deprecations](https://github.com/pytorch/pytorch/releases?page=1#deprecations)
- [New Features](https://github.com/pytorch/pytorch/releases?page=1#new-features)
- [Improvements](https://github.com/pytorch/pytorch/releases?page=1#improvements)
- [Bug fixes](https://github.com/pytorch/pytorch/releases?page=1#bug-fixes)
- [Performance](https://github.com/pytorch/pytorch/releases?page=1#performance)
- [Documentation](https://github.com/pytorch/pytorch/releases?page=1#documentation)
- [Developers](https://github.com/pytorch/pytorch/releases?page=1#developers)

# Highlights

|     |     |
| --- | --- |
| **Beta** | **Prototype** |
| Torch.Compile support for Torch Function Modes | NVIDIA Blackwell Architecture Support |
| Mega Cache | PyTorch Native Context Parallel |
|  | Enhancing Intel GPU Acceleration |
|  | FlexAttention LLM first token processing on X86 CPUs |
|  | FlexAttention LLM throughput mode optimization on X86 CPUs |
|  | Foreach Map |
|  | Flex Attention for Inference |
|  | Prologue Fusion Support in Inductor |

For more details about these highlighted features, you can look at the [release blogpost](https://pytorch.org/blog/pytorch2-7/).

Below are the full release notes for this release.

# Tracked Regressions

### NCCL init hits CUDA failure 'invalid argument' on 12.2 driver

Some users with 12.2 CUDA driver (535 version) report seeing "CUDA driver error: invalid argument" during NCCL or Symmetric Memory initialization. This issue is currently under investigation, see [#150852](https://github.com/pytorch/pytorch/issues/150852). If you use PyTorch from source, a known workaround is to rebuild PyTorch with CUDA 12.2 toolkit. Otherwise, you can try upgrading the CUDA driver on your system.

# Backwards Incompatible Changes

### Dropped support for Triton < 2.2.0. Removed Support for CUDA 12.4, Anaconda in CI/CD.

- Removed CUDA 12.4 support in CI/CD in favor of 12.8 ( [#148895](https://github.com/pytorch/pytorch/pull/148895), [#142856](https://github.com/pytorch/pytorch/pull/142856), [#144118](https://github.com/pytorch/pytorch/pull/144118), [#145566](https://github.com/pytorch/pytorch/pull/145566), [#145844](https://github.com/pytorch/pytorch/pull/145844), [#148602](https://github.com/pytorch/pytorch/pull/148602), [#143076](https://github.com/pytorch/pytorch/pull/143076), [#148717](https://github.com/pytorch/pytorch/pull/148717))
- Removed Anaconda support in CI/CD ( [#144870](https://github.com/pytorch/pytorch/pull/144870), [#145015](https://github.com/pytorch/pytorch/pull/145015), [#147792](https://github.com/pytorch/pytorch/pull/147792))
- Dropped support for Triton < 2.2.0 (versions without ASTSource) ( [#143817](https://github.com/pytorch/pytorch/pull/143817))

### C++ Extensions `py_limited_api=True` is now built with `-DPy_LIMITED_API` ( [\#145764](https://github.com/pytorch/pytorch/pull/145764))

We formally began respecting the `py_limited_api=True` kwarg in 2.6 and stopped linking `libtorch_python.so` when the flag was specified, as libtorch\_python.so does not guarantee using APIs from from the stable Python limited API. In 2.7, we go further by specifying the `-DPy_LIMITED_API` flag which will enforce that the extension is buildable with the limited API. As a result of this enforcement, **custom extensions that set `py_limited_api=True` but do not abide by the limited API may fail to build**. For an example, see [#152243](https://github.com/pytorch/pytorch/issues/152243).

This is strictly better behavior as it is sketchy to claim CPython agnosticism without enforcing with the flag. If you run into this issue, please ensure that the extension you are building does not use any APIs which are outside of the Python limited API, e.g., `pybind`.

### Change `torch.Tensor.new_tensor()` to be on the given Tensor's device by default ( [\#144958](https://github.com/pytorch/pytorch/pull/144958))

This function was always creating the new Tensor on the "cpu" device and will now use the same device as the current Tensor object. This behavior is now consistent with other `.new_*` methods.

### Use Manylinux 2.28 and CXX11\_ABI=1 for future released Linux wheel builds.

With Migration to manylinux\_2\_28 (AlmaLinux 8 based), we can no longer support OS distros with glibc2\_26. These include popular Amazon Linux 2 and CentOS 7. ( [#143423](https://github.com/pytorch/pytorch/pull/143423), [#146200](https://github.com/pytorch/pytorch/pull/146200), [#148028](https://github.com/pytorch/pytorch/pull/148028), [#148135](https://github.com/pytorch/pytorch/pull/148135), [#148195](https://github.com/pytorch/pytorch/pull/148195), [#148129](https://github.com/pytorch/pytorch/pull/148129))

### `torch.onnx.dynamo_export` now uses the ExportedProgram logic path ( [\#137296](https://github.com/pytorch/pytorch/pull/137296))

Users using the `torch.onnx.dynamo_export` API may see some `ExportOptions` become

unsupported due to an internal switch to use `torch.onnx.export(..., dynamo=True)`: `diagnostic_options`, `fake_context` and `onnx_registry` are removed/ignored by `ExportOptions`. Only `dynamic_shapes` is retained.

Users should move to use the `dynamo=True` option on `torch.onnx.export` as

`torch.onnx.dynamo_export` is now deprecated. Leverage the [`dynamic_shapes`](https://pytorch.org/docs/stable/export.html#torch.export.export) argument in `torch.onnx.export` for specifying dynamic shapes on the model.

Version 2.6.0

```
torch.onnx.dynamo_export(model, *args, **kwargs)
```

Version 2.7.0

```
torch.onnx.export(model, args, kwargs=kwargs, dynamo=True)
```

### Finish deprecation of `LRScheduler.print_lr()` along with the `verbose` kwarg to the LRScheduler constructor. ( [\#147301](https://github.com/pytorch/pytorch/pull/147301))

Both APIs have been deprecated since 2.2. Please use `LRScheduler.get_last_lr()` to access the learning rate instead. `print_lr` and `verbose` were confusing, not properly documented and were little used, as described in [#99270](https://github.com/pytorch/pytorch/issues/99270), so we deprecated them in 2.2. Now, we complete the deprecation by removing them completely. To access and print the learning rate of a LRScheduler:

Version 2.6.0

```
optim = ...
lrsched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, verbose=True)
// lrsched will internally call print_lr() and print the learning rate
```

Version 2.7.0

```
optim = ...
lrsched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim)
print(lrsched.get_last_lr())
```

### libtorch\_python.so symbols are now invisible by default on all platforms except Apple ( [\#142214](https://github.com/pytorch/pytorch/pull/142214))

Previously, the symbols in libtorch\_python.so were exposed with default visibility. We have transitioned to being more intentional about what we expose as public symbols for our python API in C++. After [#142214](https://github.com/pytorch/pytorch/pull/142214), public symbols will be marked explicitly while everything else will be hidden. Some extensions using private symbols will see linker failures with this change.

### Please use `torch.export.export` instead of `capture_pre_autograd_graph` to export the model for pytorch 2 export quantization ( [\#139505](https://github.com/pytorch/pytorch/pull/139505))

`capture_pre_autograd_graph` was a temporary API in `torch.export`. Since now we have a better longer term API: `export` available, we can deprecate it.

Version 2.6.0

```
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantize_pt2e import prepare_pt2e
from torch.ao.quantization.quantizer.xnnpack_quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)
quantizer = XNNPACKQuantizer().set_global(
    get_symmetric_quantization_config()
)
m = capture_pre_autograd_graph(m, *example_inputs)
m = prepare_pt2e(m, quantizer)
```

Version 2.7.0

```
from torch.export import export
from torch.ao.quantization.quantize_pt2e import prepare_pt2e
# please get xnnpack quantizer from executorch (https://github.com/pytorch/executorch/)
from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import (
    XNNPACKQuantizer,
    get_symmetric_quantization_config,
)
quantizer = XNNPACKQuantizer().set_global(
    get_symmetric_quantization_config()
)
m = export(m, *example_inputs)
m = prepare_pt2e(m, quantizer)
```

### New interface for `torch.fx.passes.graph_transform_observer.GraphTransformObserver` to enable Node Level provenance tracking ( [\#144277](https://github.com/pytorch/pytorch/pull/144277))

We now track a mapping between the nodes in the pre-grad and post-grad graph. See the issue for an example frontend to visualize the transformations. To update your `GraphTransformObserver` subclasses, instead of overriding `on_node_creation` and `on_node_erase`, there are new functions `get_node_creation_hook`, `get_node_erase_hook`, `get_node_replace_hook` and `get_deepcopy_hook`. These are registered on the `GraphModule` member of the `GraphTransformObserver` upon entry and exit of a `with` block

Version 2.6.0

```
class MyPrintObserver(GraphTransformObserver):
    def on_node_creation(self, node: torch.fx.Node):
        print(node)
```

Version 2.7.0

```
class MyPrintObserver(GraphTransformObserver):
    def get_node_creation_hook(self):
        def hook(node: torch.fx.Node):
            print(node)
        return hook
```

### `torch.ao.quantization.pt2e.graph_utils.get_control_flow_submodules` is no longer public ( [\#141612](https://github.com/pytorch/pytorch/pull/141612))

We are planning to make all functions under `torch.ao.quantization.pt2e.graph_utils` private. This update marks `get_control_flow_submodules` as a private API. If you have to or want to continue using `get_control_flow_submodules`, please make a private call by using `_get_control_flow_submodules`.

**Example:**

Version 2.6:

```
>>> from torch.ao.quantization.pt2e.graph_utils import get_control_flow_submodules
```

Version 2.7:

```
>>> from torch.ao.quantization.pt2e.graph_utils import get_control_flow_submodules
ImportError: cannot import name 'get_control_flow_submodules' from 'torch.ao.quantization.pt2e.graph_utils'
>>> from torch.ao.quantization.pt2e.graph_utils import _get_control_flow_submodules  # Note: Use _get_control_flow_submodules for private access
```

# Deprecations

### `torch.onnx.dynamo_export` is deprecated ( [\#146425](https://github.com/pytorch/pytorch/pull/146425), [\#146639](https://github.com/pytorch/pytorch/pull/146639), [\#146923](https://github.com/pytorch/pytorch/pull/146923))

Users should use the `dynamo=True` option on `torch.onnx.export`.

Version 2.6.0

```
torch.onnx.dynamo_export(model, *args, **kwargs)
```

Version 2.7.0

```
torch.onnx.export(model, args, kwargs=kwargs, dynamo=True)
```

### `XNNPACKQuantizer` is deprecated in PyTorch and moved to ExecuTorch, please use it from `executorch.backends.xnnpack.quantizer.xnnpack_quantizer` instead of `torch.ao.quantization.quantizer.xnnpack_quantizer`. ( [\#144940](https://github.com/pytorch/pytorch/pull/144940))

`XNNPACKQuantizer` is a quantizer for xnnpack that was added into pytorch/pytorch for initial development. Ho...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.7.0)

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)73akshaytrikha, wanderingeek, logicwong, holycowdude, yuygfgg, glevv, andrey-khropov, healy-hub, Jumaron, Aunali321, and 63 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)10651961, heindrickdumdum0217, Achilles718611, healy-hub, wolegca, GoodCoder666, cataluna84, obitodaitu, AlirezaSR, and PyroKing39 reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)31kmaehashi, healy-hub, Jumaron, Aunali321, akihironitta, Silv3S, IndigoW0lf, Red-Eyed, ghchris2021, hotchpotch, and 21 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)18akihironitta, gui-miotto, Red-Eyed, ghchris2021, 651961, heindrickdumdum0217, Achilles718611, healy-hub, andre-brainn, wolegca, and 8 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)17tmshv, ghchris2021, brian316, 651961, ErcinDedeoglu, heindrickdumdum0217, Achilles718611, LiPingYen, healy-hub, Anuvadak, and 7 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)8651961, gugarosa, wolegca, GoodCoder666, cataluna84, obitodaitu, PyroKing39, and Fitanium reacted with eyes emoji

99 people reacted

## PyTorch 2.6.0 Release

Jan 29
29 Jan 17:18


![@HDCharles](https://avatars.githubusercontent.com/u/39544797?s=40&v=4)[HDCharles](https://github.com/HDCharles)

[v2.6.0](https://github.com/pytorch/pytorch/tree/v2.6.0)

[`1eba9b3`](https://github.com/pytorch/pytorch/commit/1eba9b3aa3c43f86f4a2c807ac8e12c4a7767340)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Jan 28, 2025, 07:09 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.6.0 Release](https://github.com/pytorch/pytorch/releases/tag/v2.6.0)

- Highlights
- Tracked Regressions
- Backwards Incompatible Change
- Deprecations
- New Features
- Improvements
- Bug fixes
- Performance
- Documentation
- Developers

## **Highlights**

We are excited to announce the release of PyTorch® 2.6 ( [release notes](https://github.com/pytorch/pytorch/releases/tag/v2.6.0))! This release features multiple improvements for PT2: `torch.compile` can now be used with Python 3.13; new performance-related knob `torch.compiler.set_stance`; several AOTInductor enhancements. Besides the PT2 improvements, another highlight is FP16 support on X86 CPUs.

NOTE: Starting with this release we are not going to publish on Conda, please see [\[Announcement\] Deprecating PyTorch’s official Anaconda channel](https://github.com/pytorch/pytorch/issues/138506) for the details.

For this release the experimental Linux binaries shipped with CUDA 12.6.3 (as well as Linux Aarch64, Linux ROCm 6.2.4, and Linux XPU binaries) are built with CXX11\_ABI=1 and are [using the Manylinux 2.28 build platform](https://dev-discuss.pytorch.org/t/pytorch-linux-wheels-switching-to-new-wheel-build-platform-manylinux-2-28-on-november-12-2024/2581). If you build PyTorch extensions with custom C++ or CUDA extensions, please update these builds to use CXX\_ABI=1 as well and report any issues you are seeing. For the next PyTorch 2.7 release we plan to switch all Linux builds to Manylinux 2.28 and CXX11\_ABI=1, please see [\[RFC\] PyTorch next wheel build platform: manylinux-2.28](https://github.com/pytorch/pytorch/issues/123649) for the details and discussion.

Also in this release as an important security improvement measure we have changed the default value for `weights_only` parameter of `torch.load`. This is a backward compatibility-breaking change, please see [this forum post](https://dev-discuss.pytorch.org/t/bc-breaking-change-torch-load-is-being-flipped-to-use-weights-only-true-by-default-in-the-nightlies-after-137602/2573) for more details.

This release is composed of 3892 commits from 520 contributors since PyTorch 2.5. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve PyTorch. More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.

|     |     |
| --- | --- |
| Beta | Prototype |
| torch.compiler.set\_stance | Improved PyTorch user experience on Intel GPUs |
| torch.library.triton\_op | FlexAttention support on X86 CPU for LLMs |
| torch.compile support for Python 3.13 | Dim.AUTO |
| New packaging APIs for AOTInductor | CUTLASS and CK GEMM/CONV Backends for AOTInductor |
| AOTInductor: minifier |  |
| AOTInductor: ABI-compatible mode code generation |  |
| FP16 support for X86 CPUs |  |

\*To see a full list of public feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).

### BETA FEATURES

#### **\[Beta\] torch.compiler.set\_stance**

This feature enables the user to specify different behaviors (“stances”) that `torch.compile` can take between different invocations of compiled functions. One of the stances, for example, is

“eager\_on\_recompile”, that instructs PyTorch to code eagerly when a recompile is necessary, reusing cached compiled code when possible.

For more information please refer to the [set\_stance documentation](https://pytorch.org/docs/2.6/generated/torch.compiler.set_stance.html#torch.compiler.set_stance) and the [Dynamic Compilation Control with torch.compiler.set\_stance](https://pytorch.org/tutorials/recipes/torch_compiler_set_stance_tutorial.html) tutorial.

**\[Beta\] torch.library.triton\_op**

`torch.library.triton_op` offers a standard way of creating custom operators that are backed by user-defined triton kernels.

When users turn user-defined triton kernels into custom operators, `torch.library.triton_op` allows `torch.compile` to peek into the implementation, enabling `torch.compile` to optimize the triton kernel inside it.

For more information please refer to the [triton\_op documentation](https://pytorch.org/docs/2.6/library.html#torch.library.triton_op) and the [Using User-Defined Triton Kernels with torch.compile](https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html) tutorial.

**\[Beta\] torch.compile support for Python 3.13**

`torch.compile` previously only supported Python up to version 3.12. Users can now optimize models with `torch.compile` in Python 3.13.

**\[Beta\] New packaging APIs for AOTInductor**

A new package format, “ [PT2 archive](https://docs.google.com/document/d/1RQ4cmywilnFUT1VE-4oTGxwXdc8vowCSZsrRgo3wFA8/edit?usp=sharing)”, has been introduced. This essentially contains a zipfile of all the files that need to be used by AOTInductor, and allows users to send everything needed to other environments. There is also functionality to package multiple models into one artifact, and to store additional metadata inside of the package.

For more details please see the updated [torch.export AOTInductor Tutorial for Python runtime](https://pytorch.org/tutorials/recipes/torch_export_aoti_python.html).

**\[Beta\] AOTInductor: minifier**

If a user encounters an error while using AOTInductor APIs, AOTInductor Minifier allows creation of a minimal nn.Module that reproduces the error.

For more information please see the [AOTInductor Minifier documentation](https://pytorch.org/docs/2.6/torch.compiler_aot_inductor_minifier.html).

**\[Beta\] AOTInductor: ABI-compatible mode code generation**

AOTInductor-generated model code has dependency on Pytorch cpp libraries. As Pytorch evolves quickly, it’s important to make sure previously AOTInductor compiled models can continue to run on newer Pytorch versions, i.e. AOTInductor is backward compatible.

In order to guarantee application binary interface (ABI) backward compatibility, we have carefully defined a set of stable C interfaces in libtorch and make sure AOTInductor generates code that only refers to the specific set of APIs and nothing else in libtorch. We will keep the set of C APIs stable across Pytorch versions and thus provide backward compatibility guarantees for AOTInductor-compiled models.

**\[Beta\] FP16 support for X86 CPUs (both eager and Inductor modes)**

Float16 datatype is commonly used for reduced memory usage and faster computation in AI inference and training. CPUs like the recently launched [Intel® Xeon® 6 with P-Cores](https://www.intel.com/content/www/us/en/products/details/processors/xeon/xeon6-p-cores.html) support Float16 datatype with native accelerator [AMX](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html). Float16 support on X86 CPUs was introduced in PyTorch 2.5 as a prototype feature, and now it has been further improved for both eager mode and Torch.compile + Inductor mode, making it Beta level feature with both functionality and performance verified with a broad scope of workloads.

### PROTOTYPE FEATURES

**\[Prototype\] Improved PyTorch user experience on Intel GPUs**

PyTorch user experience on Intel GPUs is further improved with simplified installation steps, Windows release binary distribution and expanded coverage of supported GPU models including the latest Intel® Arc™ B-Series discrete graphics. Application developers and researchers seeking to fine-tune, inference and develop with PyTorch models on [Intel® Core™ Ultra AI PCs](https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/ai-pc.html) and [Intel® Arc™ discrete graphics](https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html) will now be able to directly install PyTorch with binary releases for Windows, Linux and Windows Subsystem for Linux 2.

- Simplified Intel GPU software stack setup to enable one-click installation of the torch-xpu PIP wheels to run deep learning workloads in an out of the box fashion, eliminating the complexity of installing and activating Intel GPU development software bundles.
- Windows binary releases for torch core, torchvision and torchaudio have been made available for Intel GPUs, and the supported GPU models have been expanded from Intel® Core™ Ultra Processors with Intel® Arc™ Graphics, [Intel® Core™ Ultra Series 2 with Intel® Arc™ Graphics](https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html) and [Intel® Arc™ A-Series Graphics](https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/a-series/overview.html) to the latest GPU hardware [Intel® Arc™ B-Series graphics](https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/desktop/b-series/overview.html).
- Further enhanced coverage of Aten operators on Intel GPUs with SYCL\* kernels for smooth eager mode execution, as well as bug fixes and performance optimizations for torch.compile on Intel GPUs.

For more information regarding Intel GPU support, please refer to [Getting Started Guide](https://pytorch.org/docs/main/notes/get_start_xpu.html).

**\[Prototype\] FlexAttention support on X86 CPU for LLMs**

FlexAttention was initially introduced in PyTorch 2.5 to provide optimized implementations for Attention variants with a flexible API. In PyTorch 2.6, X86 CPU support for FlexAttention was added through TorchInductor CPP backend. This new feature leverages and extends current CPP template abilities to support...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.6.0)

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)54D0n-A, inikishev, akihironitta, davidbuterez, Forbu, Geo99pro, Dahvikiin, CrasCris, RobinKa, ErcinDedeoglu, and 44 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)11heindrickdumdum0217, 651961, Enigmatisms, bryanlimy, ShotaDeguchi, binbjz, lin72h, cataluna84, GoodCoder666, xingchensong, and chuckles201 reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)34Anuvadak, madpeh, jeongseok-meta, jjerphan, azevedoguigo, joshdavham, mihaimoga, akihironitta, mplatzer, Olney1, and 24 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)20mplatzer, Olney1, PeterCalifano, Geo99pro, CrasCris, RobinKa, Naeem1144, Di-Is, heindrickdumdum0217, KaSaNaa, and 10 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)23painebenjamin, Maurux01, pustam-egr, joshdavham, akihironitta, Olney1, davidbuterez, atalman, CrasCris, RobinKa, and 13 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)5mrverdant13, binbjz, cataluna84, GoodCoder666, and chuckles201 reacted with eyes emoji

87 people reacted

## PyTorch 2.5.1: bug fix release

Oct 29, 2024
29 Oct 17:58


![@kit1980](https://avatars.githubusercontent.com/u/420184?s=40&v=4)[kit1980](https://github.com/kit1980)

[v2.5.1](https://github.com/pytorch/pytorch/tree/v2.5.1)

[`a8d6afb`](https://github.com/pytorch/pytorch/commit/a8d6afb511a69687bbb2b7e88a3cf67917e1697e)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Nov 7, 2024, 10:42 AM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.5.1: bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.5.1)

This release is meant to fix the following regressions:

- Wheels from PyPI are unusable out of the box on PRM-based Linux distributions: [#138324](https://github.com/pytorch/pytorch/issues/138324)
- PyPI arm64 distribution logs cpuinfo error on import: [#138333](https://github.com/pytorch/pytorch/issues/138333)
- Crash When Using torch.compile with Math scaled\_dot\_product\_attention in AMP Mode: [#133974](https://github.com/pytorch/pytorch/issues/133974)
- \[MPS\] Internal crash due to the invalid buffer size computation if sliced API is used: [#137800](https://github.com/pytorch/pytorch/issues/137800)
- Several issues related to CuDNN Attention: [#138522](https://github.com/pytorch/pytorch/pull/138522)

Besides the regression fixes, the release includes several documentation updates.

See release tracker [#132400](https://github.com/pytorch/pytorch/issues/132400) for additional information.

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)46voxlol, rino2000, Denisskas, a-gn, rhiskey, etiennelndr, carlthome, iceychris, leslie-fang-intel, ErcinDedeoglu, and 36 more reacted with thumbs up emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)23mihaimoga, seemethere, Denisskas, QuantumChemist, rhiskey, iceychris, ErcinDedeoglu, binbjz, 651961, wanderingeek, and 13 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)24voxlol, Denisskas, QuantumChemist, rsadwick, grib0ed0v, iceychris, syu-tan, binbjz, wanderingeek, Anuvadak, and 14 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)19voxlol, Denisskas, QuantumChemist, PeaBrane, iceychris, ErcinDedeoglu, Puiching-Memory, binbjz, wanderingeek, ngdlmk, and 9 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)8Puiching-Memory, Paxsenix0, bryanlimy, binbjz, Joul1285, dhkim0225, sachithdickwella, and JoEarl reacted with eyes emoji

74 people reacted

## PyTorch 2.5.0 Release, SDPA CuDNN backend, Flex Attention

Oct 17, 2024
17 Oct 16:26


![@jainapurva](https://avatars.githubusercontent.com/u/19538305?s=40&v=4)[jainapurva](https://github.com/jainapurva)

[v2.5.0](https://github.com/pytorch/pytorch/tree/v2.5.0)

[`32f585d`](https://github.com/pytorch/pytorch/commit/32f585d9346e316e554c8d9bf7548af9f62141fc)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Nov 4, 2024, 09:12 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.5.0 Release, SDPA CuDNN backend, Flex Attention](https://github.com/pytorch/pytorch/releases/tag/v2.5.0)

# PyTorch 2.5 Release Notes

- Highlights
- Backwards Incompatible Change
- Deprecations
- New Features
- Improvements
- Bug fixes
- Performance
- Documentation
- Developers
- Security

## Highlights

We are excited to announce the release of PyTorch® 2.5! This release features a new CuDNN backend for SDPA, enabling speedups by default for users of SDPA on H100s or newer GPUs. As well, regional compilation of torch.compile offers a way to reduce the cold start up time for torch.compile by allowing users to compile a repeated nn.Module (e.g. a transformer layer in LLM) without recompilations. Finally, TorchInductor CPP backend offers solid performance speedup with numerous enhancements like FP16 support, CPP wrapper, AOT-Inductor mode, and max-autotune mode.

This release is composed of 4095 commits from 504 contributors since PyTorch 2.4. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.5. More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.

As well, please check out our new ecosystem projects releases with [TorchRec](https://github.com/pytorch/torchrec) and [TorchFix](https://github.com/pytorch-labs/torchfix/releases/tag/v0.6.0).

| Beta | Prototype |
| --- | --- |
| CuDNN backend for SDPA | FlexAttention |
| torch.compile regional compilation without recompilations | Compiled Autograd |
| TorchDynamo added support for exception handling & MutableMapping types | Flight Recorder |
| TorchInductor CPU backend optimization | Max-autotune Support on CPU with GEMM Template |
|  | TorchInductor on Windows |
|  | FP16 support on CPU path for both eager mode and TorchInductor CPP backend |
|  | Autoload Device Extension |
|  | Enhanced Intel GPU support |

\*To see a full list of public feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?gid=949287277#gid=949287277).

### BETA FEATURES

#### \[Beta\] CuDNN backend for SDPA

The cuDNN "Fused Flash Attention" backend was landed for `torch.nn.functional.scaled_dot_product_attention`. On NVIDIA H100 GPUs this can provide up to 75% speed-up over FlashAttentionV2. This speedup is enabled by default for all users of SDPA on H100 or newer GPUs.

#### \[Beta\] _torch.compile_ regional compilation without recompilations

Regional compilation without recompilations, via `torch._dynamo.config.inline_inbuilt_nn_modules` which default to True in 2.5+. This option allows users to compile a repeated nn.Module (e.g. a transformer layer in LLM) without recompilations. Compared to compiling the full model, this option can result in smaller compilation latencies with 1%-5% performance degradation compared to full model compilation.

See the [tutorial](https://pytorch.org/tutorials/recipes/regional_compilation.html) for more information.

#### \[Beta\] TorchInductor CPU backend optimization

This feature advances Inductor’s CPU backend optimization, including CPP backend code generation and FX fusions with customized CPU kernels. The Inductor CPU backend supports vectorization of common data types and all Inductor IR operations, along with the static and symbolic shapes. It is compatible with both Linux and Windows OS and supports the default Python wrapper, the CPP wrapper, and AOT-Inductor mode.

Additionally, it extends the max-autotune mode of the GEMM template (prototyped in 2.5), offering further performance gains. The backend supports various FX fusions, lowering to customized kernels such as oneDNN for Linear/Conv operations and SDPA. The Inductor CPU backend consistently achieves performance speedups across three benchmark suites—TorchBench, Hugging Face, and timms—outperforming eager mode in 97.5% of the 193 models tested.

### PROTOTYPE FEATURES

#### \[Prototype\] FlexAttention

We've introduced a flexible API that enables implementing various attention mechanisms such as Sliding Window, Causal Mask, and PrefixLM with just a few lines of idiomatic PyTorch code. This API leverages torch.compile to generate a fused FlashAttention kernel, which eliminates extra memory allocation and achieves performance comparable to handwritten implementations. Additionally, we automatically generate the backwards pass using PyTorch's autograd machinery. Furthermore, our API can take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.

For more information and examples, please refer to the [official blog post](https://pytorch.org/blog/flexattention/) and [Attention Gym](https://github.com/pytorch-labs/attention-gym).

#### \[Prototype\] Compiled Autograd

Compiled Autograd is an extension to the PT2 stack allowing the capture of the entire backward pass. Unlike the backward graph traced by AOT dispatcher, Compiled Autograd tracing is deferred until backward execution time, which makes it impervious to forward pass graph breaks, and allows it to record backward hooks into the graph.

Please refer to the [tutorial](https://pytorch.org/tutorials/intermediate/compiled_autograd_tutorial.html) for more information.

#### \[Prototype\] Flight Recorder

Flight recorder is a new debugging tool that helps debug stuck jobs. The tool works by continuously capturing information about collectives as they run. Upon detecting a stuck job, the information can be used to quickly identify misbehaving ranks/machines along with code stack traces.

For more information please refer to the following [tutorial](https://pytorch.org/tutorials/prototype/flight_recorder_tutorial.html).

#### \[Prototype\] Max-autotune Support on CPU with GEMM Template

Max-autotune mode for the Inductor CPU backend in torch.compile profiles multiple implementations of operations at compile time and selects the best-performing one. This is particularly beneficial for GEMM-related operations, using a C++ template-based GEMM implementation as an alternative to the ATen-based approach with oneDNN and MKL libraries. We support FP32, BF16, FP16, and INT8 with epilogue fusions for x86 CPUs. We’ve seen up to 7% geomean speedup on the dynamo benchmark suites and up to 20% boost in next-token latency for LLM inference.

For more information please refer to the [tutorial](https://pytorch.org/tutorials/prototype/max_autotune_on_CPU_tutorial.html).

#### \[Prototype\] TorchInductor CPU on Windows

Inductor CPU backend in torch.compile now works on Windows. We support MSVC (cl), clang (clang-cl) and Intel compiler (icx-cl) for Windows inductor currently.

See the [tutorial](https://pytorch.org/tutorials/prototype/inductor_windows_cpu.html) for more details.

#### \[Prototype\] FP16 support on CPU path for both eager mode and TorchInductor CPP backend

Float16 is a commonly used reduced floating point type for performance improvement in neural network inference/training. Since this release, float16 for both eager and TorchInductor is supported on the CPU path.

#### \[Prototype\] Autoload Device Extension

PyTorch now supports autoloading for out-of-tree device extensions, streamlining integration by eliminating the need for manual imports. This feature, enabled through the torch.backends entrypoint, simplifies usage by ensuring seamless extension loading, while allowing users to disable it via an environment variable if needed.

See the [tutorial](https://pytorch.org/tutorials/prototype/python_extension_autoload.html) for more information.

#### \[Prototype\] Enhanced Intel GPU support

Intel GPUs support enhancement is now available for both Intel® Data Center GPU Max Series and Intel® Client GPUs (Intel® Core™ Ultra processors with built-in Intel® Arc™ graphics and Intel® Arc™ Graphics for dGPU parts), which is to make it easier to accelerate your Machine Learning workflows on Intel GPUs in PyTorch 2.5 release. We also enabled the initial support of PyTorch on Windows for Intel® Client GPUs in this release.

- Expanded PyTorch hardware backend support matrix to include both Intel Data Center and Client GPUs.
- The implementation of SYCL\* kernels to enhance coverage and execution of Aten operators on Intel GPUs to boost performance in PyTorch eager mode.
- Enhanced Intel GPU backend of torch.compile to improve inference and training performance for a wide range of deep learning workloads.

These features are available through PyTorch preview and nightly binary PIP wheels. For more information regarding Intel GPU support, please refer to [documentation](https://pytorch.org/docs/main/notes/get_start_xpu.html).

## Backwards Incompatible changes

### Distributed

- \[c10d\] Remove Option for ProcessGroup and Expose backend Options to reflect the correct code structure ( [#132931](https://github.com/pytorch/pytorch/pull/132931))


  - We released Dispatchable collectives in 2.0 and we will use Backend Option for Backend initialization and the PG options are not needed any more.
  - In 2.4 and before, users can do:

```
# Users can pass in a basic option when creating an instance of ProcessGroup
base_pg_options = ProcessGroup.Options(backend=str(backend))
base_pg_options._timeout = timeout

pg: ProcessGroup = ProcessGroup(
  store, rank, group_size, base_pg_options
)

# Users then need to create a backend option to create the comm backend (e.g., ProcessGroupNCCL)
pg_options = ProcessGroupNCCL.Options()
backend = ProcessGroupNCCL(
  store, rank, group_size, pg_options
)
```

  - But from 2.5 onwards, users don’t need to pass in an option to create an instance of ProcessGroup and user can still set default backend for the pg since users still try to get default backend in the code:

```
# No basic option is passed in when creating a instance of ProcessGroup
pg: ProcessGroup = ProcessGroup(store, rank, group_size)
pg._set_default_backend(...
```

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.5.0)

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)76voxlol, duanzhiihao, johnnynunez, bryanlimy, mhyrzt, leonardodepaula, SagatdinovEmil, etiennelndr, mrverdant13, jepjoo, and 66 more reacted with thumbs up emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)35QuantumChemist, jjerphan, Puiching-Memory, bryanlimy, mihaimoga, fcogidi, AYUSHMIT, johnnynunez, mrverdant13, parlance-zz, and 25 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)20atalman, cuicaihao, parlance-zz, iceychris, bryanlimy, GraceKafuu, Denisskas, inikishev, GoodCoder666, sa-y-an, and 10 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)22Puiching-Memory, bryanlimy, johnnynunez, mrverdant13, parlance-zz, cuicaihao, gau-nernst, iceychris, wangling1820, VictorSantos674, and 12 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)9Paxsenix0, GraceKafuu, Denisskas, fpaupier, GoodCoder666, binbjz, Geo99pro, wanderingeek, and wbigat reacted with eyes emoji

105 people reacted

## PyTorch 2.4.1 Release, bug fix release

Sep 4, 2024
04 Sep 19:59


![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)

[v2.4.1](https://github.com/pytorch/pytorch/tree/v2.4.1)

[`ee1b680`](https://github.com/pytorch/pytorch/commit/ee1b6804381c57161c477caa380a840a84167676)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Nov 4, 2024, 11:42 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.4.1 Release, bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.4.1)

This release is meant to fix the following issues (regressions / silent correctness):

### Breaking Changes:

- The pytorch/pytorch docker image now installs the PyTorch package through pip and has switch its conda installation from miniconda to [miniforge](https://github.com/conda-forge/miniforge) ( [#134274](https://github.com/pytorch/pytorch/pull/134274))

### Windows:

- Fix performance regression on Windows related to MKL static linking ( [#130619](https://github.com/pytorch/pytorch/issues/130619)) ( [#130697](https://github.com/pytorch/pytorch/pull/130697))
- Fix error during loading on Windows: \[WinError 126\] The specified module could not be found. ( [#131662](https://github.com/pytorch/pytorch/issues/131662)) ( [#130697](https://github.com/pytorch/pytorch/pull/130697))

### MPS:

- Fix tensor.clamp produces wrong values ( [#130226](https://github.com/pytorch/pytorch/pull/130226))
- Fix Incorrect result from batch norm with sliced inputs ( [#133610](https://github.com/pytorch/pytorch/pull/133610))

### ROCM:

- Fix for launching kernel invalid config error when calling embedding with large index ( [#130994](https://github.com/pytorch/pytorch/pull/130994))
- Added a check and a warning when attempting to use hipBLASLt on an unsupported architecture ( [#128753](https://github.com/pytorch/pytorch/pull/128753))
- Fix image corruption with Memory Efficient Attention when running HuggingFace Diffusers Stable Diffusion 3 pipeline ( [#133331](https://github.com/pytorch/pytorch/pull/133331))

### Distributed:

- Fix FutureWarning when using torch.load internally ( [#130663](https://github.com/pytorch/pytorch/pull/130663))
- Fix FutureWarning when using torch.cuda.amp.autocast internally ( [#130660](https://github.com/pytorch/pytorch/pull/130660))

### Torch.compile:

- Fix exception with torch compile when onnxruntime-training and deepspeed packages are installed. ( [#131194](https://github.com/pytorch/pytorch/pull/131194))
- Fix silent incorrectness with torch.library.custom\_op with mutable inputs and torch.compile ( [#133452](https://github.com/pytorch/pytorch/pull/133452))
- Fix SIMD detection on Linux ARM ( [#129075](https://github.com/pytorch/pytorch/pull/129075))
- Do not use C++20 features in cpu\_inducotr code ( [#130816](https://github.com/pytorch/pytorch/pull/130816))

### Packaging:

- Fix for exposing statically linked libstdc++ CXX11 ABI symbols ( [#134494](https://github.com/pytorch/pytorch/pull/134494))
- Fix error while building pytorch from source due to not missing QNNPACK module ( [#131864](https://github.com/pytorch/pytorch/pull/133177))
- Make PyTorch buildable from source on PowerPC ( [#129736](https://github.com/pytorch/pytorch/pull/129736))
- Fix XPU extension building ( [#132847](https://github.com/pytorch/pytorch/pull/132847))

### Other:

- Fix warning when using pickle on a nn.Module that contains tensor attributes ( [#130246](https://github.com/pytorch/pytorch/pull/130246))
- Fix NaNs return in MultiheadAttention when need\_weights=False ( [#130014](https://github.com/pytorch/pytorch/pull/130014))
- Fix nested tensor MHA produces incorrect results ( [#130196](https://github.com/pytorch/pytorch/issues/130196))
- Fix error when using torch.utils.flop\_counter.FlopCounterMode ( [#134467](https://github.com/pytorch/pytorch/pull/134467))

### Tracked Regressions:

- The experimental remote caching feature for Inductor's autotuner (enabled via TORCHINDUCTOR\_AUTOTUNE\_REMOTE\_CACHE) is known to still be broken in this release and actively worked on in main. Following Error is generated: redis.exceptions.DataError: Invalid input of type: 'dict'. Please use nightlies if you need this feature (reported and Fixed by PR: [#134032](https://github.com/pytorch/pytorch/pull/134032))

Release tracker [#132400](https://github.com/pytorch/pytorch/issues/132400) contains all relevant pull requests related to this release as well as links to related issues.

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)43etiennelndr, Jared-woodruff, D0n-A, moh-salih, Robbin1998s, banyan-god, dvquy13, vilsonrodrigues, VidathChamikara, zhouzaida, and 33 more reacted with thumbs up emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)17yoshoku, 651961, Robbin1998s, Denisskas, ken-morel, Atharvkote, ancestor-mithril, wanderingeek, iceychris, binbjz, and 7 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)27hammaad2002, bigcat88, K-H-Ismail, Benetti-Hub, Burhan-Q, QuantumChemist, Alarmod, twoertwein, Heliodex, 651961, and 17 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)4InhabitancyCocoon, GnafiY, AmeenAli, and sudoflex reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)8Paxsenix0, Puiching-Memory, Denisskas, wanderingeek, binbjz, brian316, lunnyliu, and Nullkooland reacted with eyes emoji

77 people reacted

## PyTorch 2.4: Python 3.12, AOTInductor freezing, libuv backend for TCPStore

Jul 24, 2024
24 Jul 18:39


![@vmoens](https://avatars.githubusercontent.com/u/25529882?s=40&v=4)[vmoens](https://github.com/vmoens)

[v2.4.0](https://github.com/pytorch/pytorch/tree/v2.4.0)

[`d990dad`](https://github.com/pytorch/pytorch/commit/d990dada86a8ad94882b5c23e859b88c0c255bda)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Nov 4, 2024, 06:27 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.4: Python 3.12, AOTInductor freezing, libuv backend for TCPStore](https://github.com/pytorch/pytorch/releases/tag/v2.4.0)

# PyTorch 2.4 Release Notes

- Highlights
- Tracked Regressions
- Backward incompatible changes
- Deprecations
- New features
- Improvements
- Bug Fixes
- Performance
- Documentation
- Developers
- Security

## Highlights

We are excited to announce the release of PyTorch® 2.4!

PyTorch 2.4 adds support for the latest version of Python (3.12) for `torch.compile`.

AOTInductor freezing gives developers running AOTInductor more performance based optimizations by allowing the

serialization of MKLDNN weights. As well, a new default TCPStore server backend utilizing `libuv` has been introduced

which should significantly reduce initialization times for users running large-scale jobs.

Finally, a new Python Custom Operator API makes it easier than before to integrate custom kernels

into PyTorch, especially for `torch.compile`.

This release is composed of 3661 commits and 475 contributors since PyTorch 2.3. We want to sincerely thank our

dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we

improve 2.4. More information about how to get started with the PyTorch 2-series can be found at our

[Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.

|     |     |     |
| --- | --- | --- |
| Beta | Prototype | Performance Improvements |
| Python 3.12 support for torch.compile | FSDP2: DTensor-based per-parameter-sharding FSDP | torch.compile optimizations for AWS Graviton (aarch64-linux) processors |
| AOTInductor Freezing for CPU | torch.distributed.pipelining, simplified pipeline parallelism | BF16 symbolic shape optimization in TorchInductor |
| New Higher-level Python Custom Operator API | Intel GPU is available through source build | Performance optimizations for GenAI projects utilizing CPU devices |
| Switching TCPStore’s default server backend to libuv |  |  |
|  |  |  |

\*To see a full list of public feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).

## Tracked Regressions

### Subproc exception with torch.compile and onnxruntime-training

There is a reported issue ( [#131070](https://github.com/pytorch/pytorch/issues/131070)) when using `torch.compile` if `onnxruntime-training` lib is

installed. The issue will be fixed ( [#131194](https://github.com/pytorch/pytorch/pull/131194)) in v2.4.1. It can be solved locally by setting the environment variable

`TORCHINDUCTOR_WORKER_START=fork` before executing the script.

### cu118 wheels will not work with pre-cuda12 drivers

It was also reported ( [#130684](https://github.com/pytorch/pytorch/issues/130684)) that the new version of triton uses cuda features that are not compatible with pre-cuda12 drivers.

In this case, the [workaround](https://github.com/triton-lang/triton/pull/4335#issuecomment-2232505298) is to set

`TRITON_PTXAS_PATH` manually as follows (adapt the code according to the local installation path):

```
TRITON_PTXAS_PATH=/usr/local/lib/python3.10/site-packages/torch/bin/ptxas  python script.py
```

## Backwards Incompatible Change

### Python frontend

#### Default `TreadPool` size to number of physical cores ( [\#125963](https://github.com/pytorch/pytorch/pull/125963))

Changed the default number of threads used for intra-op parallelism from the number of logical cores to the number of

physical cores. This should reduce core oversubscribing when running CPU workload and improve performance.

Previous behavior can be recovered by using torch.set\_num\_threads to set the number of threads to the desired value.

#### Fix `torch.quasirandom.SobolEngine.draw` default dtype handling ( [\#126781](https://github.com/pytorch/pytorch/pull/126781))

The default dtype value has been changed from `torch.float32` to the current default dtype as given by

`torch.get_default_dtype()` to be consistent with other APIs.

#### Forbid subclassing `torch._C._TensorBase` directly ( [\#125558](https://github.com/pytorch/pytorch/pull/125558))

This is an internal subclass that a user used to be able to create an object that is almost a Tensor in Python and was

advertised as such in some tutorials. This is not allowed anymore to improve consistency and all users should

subclass torch.Tensor directly.

### Composability

#### Non-compositional usages of as\_strided + mutation under `torch.compile` will raise an error ( [\#122502](https://github.com/pytorch/pytorch/pull/122502))

The `torch.compile` flow involves functionalizing any mutations inside the region being compiled. Torch.as\_strided is

an existing view op that can be used non-compositionally: meaning when you call x.as\_strided(...), as\_strided will only

consider the underlying storage size of x, and ignore its current size/stride/storage\_offset when creating a new view.

This makes it difficult to safely functionalize mutations on views of as\_strided that are created non-compositionally,

so we ban them rather than risking silent correctness issues under torch.compile.

An example of a non-compositional usage of as\_strided followed by mutation that we will error on is below. You can avoid

this issue by re-writing your usage of as\_strided so that it is compositional (for example: either use a different set

of view ops instead of as\_strided, or call as\_strided directly on the base tensor instead of an existing view of it).

```
@torch.compile
def foo(a):
    e = a.diagonal()
    # as_strided is being called on an existing view (e),
    # making it non-compositional. mutations to f under torch.compile
    # are not allowed, as we cannot easily functionalize them safely
    f = e.as_strided((2,), (1,), 0)
    f.add_(1.0)
    return a
```

#### We now verify schemas of custom ops at registration time ( [\#124520](https://github.com/pytorch/pytorch/pull/124520))

Previously, you could register a custom op through the operator registration APIs, but give it a schema that contained

types unknown to the PyTorch Dispatcher. This behavior came from TorchScript, where “unknown” types were implicitly

treated by the TorchScript interpreter as type variables. However, calling such a custom op through regular pytorch

would result in an error later. As of 2.4, we will raise an error at registration time, when you first register the

custom operator. You can get the old behavior by constructing the schema with allow\_typevars=true.

```
TORCH_LIBRARY(my_ns, m) {
  // this now raises an error at registration time: bar/baz are unknown types
  m.def("my_ns::foo(bar t) -> baz");
  // you can get back the old behavior with the below flag
  m.def(torch::schema("my_ns::foo(bar t) -> baz", /*allow_typevars*/ true));
}

```

### Autograd frontend

#### Delete torch.autograd.function.traceable APIs ( [\#122817](https://github.com/pytorch/pytorch/pull/122817))

The torch.autograd.function.traceable(...) API, which sets the is\_traceable class attribute

on a torch.autograd.Function class was deprecated in 2.3 and is now being deleted.

This API does not do anything and was only meant for internal purposes.

The following raised an warning in 2.3, and now errors because the API has been deleted:

```
@torch.autograd.function.traceable
class Func(torch.autograd.Function):
    ...
```

### Release engineering

- Remove caffe2 db and distributed from build system ( [#125092](https://github.com/pytorch/pytorch/pull/125092))

### Optim

- Remove `SparseAdam` weird allowance of raw Tensor input ( [#127081](https://github.com/pytorch/pytorch/pull/127081)).

### Distributed

#### DeviceMesh

Update get\_group and add get\_all\_groups ( [#128097](https://github.com/pytorch/pytorch/pull/128097))

In 2.3 and before, users can do:

```
mesh_2d = init_device_mesh(
    "cuda", (2, 2), mesh_dim_names=("dp", "tp")
)
mesh_2d.get_group()  # This will return all sub-pgs within the mesh
assert mesh_2d.get_group()[0] == mesh_2d.get_group(0)
assert mesh_2d.get_group()[1] == mesh_2d.get_group(1)
```

But from 2.4 forward, if users call `get_group` without passing in the dim, users will get a `RuntimeError`.

Instead, they should use `get_all_groups`:

```
mesh_2d = init_device_mesh(
    "cuda", (2, 2), mesh_dim_names=("dp", "tp")
)
mesh_2d.get_group()  # This will throw a RuntimeError
assert mesh_2d.get_all_groups()[0] == mesh_2d.get_group(0)
assert mesh_2d.get_all_groups()[1] == mesh_2d.get_group(1)
```

#### Pipelining

Retire torch.distributed.pipeline ( [#127354](https://github.com/pytorch/pytorch/pull/127354))

In 2.3 and before, users can do:

```
import torch.distributed.pipeline # warning saying that this will be removed and users need to migrate to torch.distributed.pipelining
```

But from 2.4 forward, if users write the code above, users will get a `ModuleNotFound` error.

Instead, they should use `torch.distributed.pipelining`:

```
import torch.distributed.pipeline # -> ModuleNotFoundError
import torch.distributed.pipelining
```

### jit

- Fix serialization/deepcopy behavior for tensors that are aliasing but not equal ( [#126126](https://github.com/pytorch/pytorch/pull/126126))

### Fx

Complete revamp of float/promotion sympy handling ( [#126905](https://github.com/pytorch/pytorch/pull/126905))

### ONNX

- Remove caffe2 contrib and experiments ( [#125038](https://github.com/pytorch/pytorch/pull/125038))

## Deprecations

### Python frontend

- User warning when using `torch.load` with default `weights_only=False` value ( [#129239](https://github.com/pytorch/pytorch/pull/129239), [#129396](https://github.com/pytorch/pytorch/pull/129396), [#129509](https://github.com/pytorch/pytorch/pull/129509)).


A warning is now raised if the weights\_only value is not specified during a call to torch.load, encouraging users to


adopt the safest practice when loading weights.
- Deprecate device-specific autocast API ( [#126062](https://github.com/pytorch/pytorch/pull/126062))


All the autocast APIs are unified under torch.amp and it can be used as a drop-in replacement for torch.{device}.amp APIs


(passing a device argument where applicable)..
- Export torch.newaxis=None for Python Array API/Numpy consistency ( [#125026](https://github.com/pytorch/pytorch/pull/125026))

### Composability

- Deprecate calling FakeTensor.data\_ptr in eager-mode. FakeTensors are tensors without a valid data pointer, so in


general their data pointer is not safe to access. This makes it easier for `torch.compile` to provide a nice error


message when tracing custom ops into a graph that are not written in a PT2-friendly way (bec...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.4.0)

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)54tolgacangoz, Borda, redradist, bryanlimy, D0n-A, xsa-dev, akihironitta, etiennelndr, Mickychen00, binbjz, and 44 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)1shang-mt reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)30tolgacangoz, nviraj, redradist, bryanlimy, nairbv, akihironitta, saeedark, Mickychen00, atalman, xuchenhao001, and 20 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)20tolgacangoz, redradist, bryanlimy, ashim-mahara, hammaad2002, akashaero, khushi-411, akihironitta, Kakaymi10, Mickychen00, and 10 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)19tolgacangoz, debnath-d, redradist, bryanlimy, gau-nernst, akihironitta, jamesETsmith, binbjz, orion160, qcind, and 9 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)10tolgacangoz, bryanlimy, Paxsenix0, binbjz, andre-brainn, waheedi, Denisskas, james-banks, AndrewDiMola, and GoodCoder666 reacted with eyes emoji

89 people reacted

## PyTorch 2.3.1 Release, bug fix release

Jun 5, 2024
05 Jun 19:16


![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)

[v2.3.1](https://github.com/pytorch/pytorch/tree/v2.3.1)

[`63d5e92`](https://github.com/pytorch/pytorch/commit/63d5e9221bedd1546b7d364b5ce4171547db12a9)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Nov 7, 2024, 10:42 AM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

Filter

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.3.1 Release, bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.3.1)

This release is meant to fix the following issues (regressions / silent correctness):

### Torch.compile:

- Remove runtime dependency on JAX/XLA, when importing `torch.__dynamo` ( [#124634](https://github.com/pytorch/pytorch/pull/124634))
- Hide `Plan failed with a cudnnException` warning ( [#125790](https://github.com/pytorch/pytorch/pull/125790))
- Fix CUDA memory leak ( [#124238](https://github.com/pytorch/pytorch/pull/124238)) ( [#120756](https://github.com/pytorch/pytorch/pull/120756))

### Distributed:

- Fix `format_utils executable`, which was causing it to run as a no-op ( [#123407](https://github.com/pytorch/pytorch/pull/123407))
- Fix regression with `device_mesh` in 2.3.0 during initialization causing memory spikes ( [#124780](https://github.com/pytorch/pytorch/pull/124780))
- Fix crash of `FSDP + DTensor` with `ShardingStrategy.SHARD_GRAD_OP` ( [#123617](https://github.com/pytorch/pytorch/pull/123617))
- Fix failure with distributed checkpointing + FSDP if at least 1 forward/backward pass has not been run. ( [#121544](https://github.com/pytorch/pytorch/pull/121544)) ( [#127069](https://github.com/pytorch/pytorch/pull/127069))
- Fix error with distributed checkpointing + FSDP, and with `use_orig_params = False` and activation checkpointing ( [#124698](https://github.com/pytorch/pytorch/pull/124698)) ( [#126935](https://github.com/pytorch/pytorch/pull/126935))
- Fix `set_model_state_dict` errors on compiled module with non-persistent buffer with distributed checkpointing ( [#125336](https://github.com/pytorch/pytorch/pull/125336)) ( [#125337](https://github.com/pytorch/pytorch/pull/125337))

### MPS:

- Fix data corruption when coping large (>4GiB) tensors ( [#124635](https://github.com/pytorch/pytorch/pull/124635))
- Fix `Tensor.abs()` for complex ( [#125662](https://github.com/pytorch/pytorch/pull/125662))

### Packaging:

- Fix UTF-8 encoding on Windows `.pyi` files ( [#124932](https://github.com/pytorch/pytorch/pull/124932))
- Fix `import torch` failure when wheel is installed for a single user on Windows( [#125684](https://github.com/pytorch/pytorch/pull/125684))
- Fix compatibility with torchdata 0.7.1 ( [#122616](https://github.com/pytorch/pytorch/pull/122616))
- Fix aarch64 docker publishing to [https://ghcr.io](https://ghcr.io/) ( [#125617](https://github.com/pytorch/pytorch/pull/125617))
- Fix performance regression an aarch64 linux ( [pytorch/builder#1803](https://github.com/pytorch/builder/pull/1803))

### Other:

- Fix DeepSpeed transformer extension build on ROCm ( [#121030](https://github.com/pytorch/pytorch/pull/121030))
- Fix kernel crash on `tensor.dtype.to_complex()` after ~100 calls in ipython kernel ( [#125154](https://github.com/pytorch/pytorch/pull/125154))

Release tracker [#125425](https://github.com/pytorch/pytorch/issues/125425) contains all relevant pull requests related to this release as well as links to related issues.

Assets3

Loading

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)54rino2000, function2-llx, antoinebrl, arvinsingh, zhanwenchen, hammaad2002, kmaehashi, wanderingeek, ZZHanyu, VermiIIi0n, and 44 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)19bow reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)15viktor765, yoshoku, kmaehashi, wanderingeek, qcind, khushi-411, tobygh, andre-brainn, john-pixforce, johnnv1, and 5 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)13tobygh, andre-brainn, hammaad2002, 9bow, M0nteCarl0, avdhoeke, tuningManBin, tamimuddin, sachithdickwella, JoelPasapera, and 3 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)14qcind, khushi-411, tobygh, AntonioBerna, andre-brainn, vilsonrodrigues, wanderingeek, N-Friederich, 9bow, sachithdickwella, and 4 more reacted with rocket emoji

70 people reacted

Previous _1_ [2](https://github.com/pytorch/pytorch/releases?page=2) [3](https://github.com/pytorch/pytorch/releases?page=3) [4](https://github.com/pytorch/pytorch/releases?page=4) [5](https://github.com/pytorch/pytorch/releases?page=5) [6](https://github.com/pytorch/pytorch/releases?page=6) [7](https://github.com/pytorch/pytorch/releases?page=7) [Next](https://github.com/pytorch/pytorch/releases?page=2)

Previous [Next](https://github.com/pytorch/pytorch/releases?page=2)

## Footer

[GitHub Homepage](https://github.com/)
© 2025 GitHub, Inc.




=== FILE: github.com_pytorch_pytorch_releases_page=2.2025-10-28T12_55_15.256Z.md ===

[Skip to content](https://github.com/pytorch/pytorch/releases?page=2#start-of-content)

## Navigation Menu

Toggle navigation

[Homepage](https://github.com/)

[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Freleases%3Fpage%3D2)

Appearance settings

- Platform









- [GitHub Copilot\\
\\
\\
\\
Write better code with AI](https://github.com/features/copilot)
- [GitHub Spark\\
\\
\\
New\\
\\
\\
Build and deploy intelligent apps](https://github.com/features/spark)
- [GitHub Models\\
\\
\\
New\\
\\
\\
Manage and compare prompts](https://github.com/features/models)
- [GitHub Advanced Security\\
\\
\\
\\
Find and fix vulnerabilities](https://github.com/security/advanced-security)
- [Actions\\
\\
\\
\\
Automate any workflow](https://github.com/features/actions)

- [Codespaces\\
\\
\\
\\
Instant dev environments](https://github.com/features/codespaces)
- [Issues\\
\\
\\
\\
Plan and track work](https://github.com/features/issues)
- [Code Review\\
\\
\\
\\
Manage code changes](https://github.com/features/code-review)
- [Discussions\\
\\
\\
\\
Collaborate outside of code](https://github.com/features/discussions)
- [Code Search\\
\\
\\
\\
Find more, search less](https://github.com/features/code-search)

Explore

- [Why GitHub](https://github.com/why-github)
- [Documentation](https://docs.github.com/)
- [GitHub Skills](https://skills.github.com/)
- [Blog](https://github.blog/)

Integrations

- [GitHub Marketplace](https://github.com/marketplace)
- [MCP Registry](https://github.com/mcp)

[View all features](https://github.com/features)

- Solutions







By company size

- [Enterprises](https://github.com/enterprise)
- [Small and medium teams](https://github.com/team)
- [Startups](https://github.com/enterprise/startups)
- [Nonprofits](https://github.com/solutions/industry/nonprofits)

By use case

- [App Modernization](https://github.com/solutions/use-case/app-modernization)
- [DevSecOps](https://github.com/solutions/use-case/devsecops)
- [DevOps](https://github.com/solutions/use-case/devops)
- [CI/CD](https://github.com/solutions/use-case/ci-cd)
- [View all use cases](https://github.com/solutions/use-case)

By industry

- [Healthcare](https://github.com/solutions/industry/healthcare)
- [Financial services](https://github.com/solutions/industry/financial-services)
- [Manufacturing](https://github.com/solutions/industry/manufacturing)
- [Government](https://github.com/solutions/industry/government)
- [View all industries](https://github.com/solutions/industry)

[View all solutions](https://github.com/solutions)

- Resources







Topics

- [AI](https://github.com/resources/articles?topic=ai)
- [DevOps](https://github.com/resources/articles?topic=devops)
- [Security](https://github.com/resources/articles?topic=security)
- [Software Development](https://github.com/resources/articles?topic=software-development)
- [View all](https://github.com/resources/articles)

Explore

- [Learning Pathways](https://resources.github.com/learn/pathways)
- [Events & Webinars](https://github.com/resources/events)
- [Ebooks & Whitepapers](https://github.com/resources/whitepapers)
- [Customer Stories](https://github.com/customer-stories)
- [Partners](https://github.com/partners)
- [Executive Insights](https://github.com/solutions/executive-insights)

- Open Source









- [GitHub Sponsors\\
\\
\\
\\
Fund open source developers](https://github.com/sponsors)

- [The ReadME Project\\
\\
\\
\\
GitHub community articles](https://github.com/readme)

Repositories

- [Topics](https://github.com/topics)
- [Trending](https://github.com/trending)
- [Collections](https://github.com/collections)

- Enterprise









- [Enterprise platform\\
\\
\\
\\
AI-powered developer platform](https://github.com/enterprise)

Available add-ons

- [GitHub Advanced Security\\
\\
\\
\\
Enterprise-grade security features](https://github.com/security/advanced-security)
- [Copilot for business\\
\\
\\
\\
Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)
- [Premium Support\\
\\
\\
\\
Enterprise-grade 24/7 support](https://github.com/premium-support)

- [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search


Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

Cancel
Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).


Cancel
Create saved search

[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Freleases%3Fpage%3D2)

[Sign up](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Freleases%2Findex&source=header-repo&source_repo=pytorch%2Fpytorch)

Appearance settings

Resetting focus

You signed in with another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=2) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=2) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=2) to refresh your session.Dismiss alert

{{ message }}

[pytorch](https://github.com/pytorch)/ **[pytorch](https://github.com/pytorch/pytorch)** Public

- Couldn't load subscription status.
Retry

- [Fork\\
25.7k](https://github.com/login?return_to=%2Fpytorch%2Fpytorch)
- [Star\\
94.3k](https://github.com/login?return_to=%2Fpytorch%2Fpytorch)


- [Code](https://github.com/pytorch/pytorch)
- [Issues5k+](https://github.com/pytorch/pytorch/issues)
- [Pull requests1.5k](https://github.com/pytorch/pytorch/pulls)
- [Actions](https://github.com/pytorch/pytorch/actions)
- [Projects12](https://github.com/pytorch/pytorch/projects)
- [Wiki](https://github.com/pytorch/pytorch/wiki)
- [Security4](https://github.com/pytorch/pytorch/security)






[**Uh oh!**](https://github.com/pytorch/pytorch/security)

[There was an error while loading.](https://github.com/pytorch/pytorch/security) [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

- [Insights](https://github.com/pytorch/pytorch/pulse)

Additional navigation options

- [Code](https://github.com/pytorch/pytorch)
- [Issues](https://github.com/pytorch/pytorch/issues)
- [Pull requests](https://github.com/pytorch/pytorch/pulls)
- [Actions](https://github.com/pytorch/pytorch/actions)
- [Projects](https://github.com/pytorch/pytorch/projects)
- [Wiki](https://github.com/pytorch/pytorch/wiki)
- [Security](https://github.com/pytorch/pytorch/security)
- [Insights](https://github.com/pytorch/pytorch/pulse)

# Releases: pytorch/pytorch

[Releases](https://github.com/pytorch/pytorch/releases) [Tags](https://github.com/pytorch/pytorch/tags)

Releases · pytorch/pytorch

## PyTorch 2.3: User-Defined Triton Kernels in torch.compile, Tensor Parallelism in Distributed

Apr 24, 2024
24 Apr 16:12


![@andrewor14](https://avatars.githubusercontent.com/u/2133137?s=40&v=4)[andrewor14](https://github.com/andrewor14)

[v2.3.0](https://github.com/pytorch/pytorch/tree/v2.3.0)

[`97ff6cf`](https://github.com/pytorch/pytorch/commit/97ff6cfd9c86c5c09d7ce775ab64ec5c99230f5d)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Nov 6, 2024, 03:25 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.3: User-Defined Triton Kernels in torch.compile, Tensor Parallelism in Distributed](https://github.com/pytorch/pytorch/releases/tag/v2.3.0)

# PyTorch 2.3 Release notes

- Highlights
- Backwards Incompatible Changes
- Deprecations
- New Features
- Improvements
- Bug fixes
- Performance
- Documentation

# Highlights

We are excited to announce the release of PyTorch® 2.3! PyTorch 2.3 offers support for user-defined Triton kernels in torch.compile, allowing for users to migrate their own Triton kernels from eager without experiencing performance complications or graph breaks. As well, Tensor Parallelism improves the experience for training Large Language Models using native PyTorch functions, which has been validated on training runs for 100B parameter models.

This release is composed of 3393 commits and 426 contributors since PyTorch 2.2. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.3. More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.

|     |     |     |     |
| --- | --- | --- | --- |
| **Stable** | **Beta** | **Prototype** | **Performance Improvements** |
|  | User-defined Triton kernels in torch.compile | torch.export adds new API to specify dynamic\_shapes | Weight-Only-Quantization introduced into Inductor CPU backend |
|  | Tensor parallelism within PyTorch Distributed | Asynchronous checkpoint generation |  |
|  | Support for semi-structured sparsity |  |  |

\*To see a full list of public feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).

# Tracked Regressions

### **torch.compile on MacOS is considered unstable for 2.3 as there are known cases where it will hang ( [\#124497](https://github.com/pytorch/pytorch/issues/124497))**

### **torch.compile imports many unrelated packages when it is invoked ( [\#123954](https://github.com/pytorch/pytorch/issues/123954))**

This can cause significant first-time slowdown and instability when these packages are not fully compatible with PyTorch within a single process.

### **torch.compile is not supported on Python 3.12 ( [\#120233](https://github.com/pytorch/pytorch/issues/120233))**

PyTorch support for Python 3.12 in general is considered experimental. Please use Python version between 3.8 and 3.11 instead. This is an existing issue since PyTorch 2.2.

# Backwards Incompatible Changes

### **Change default torch\_function behavior to be disabled when torch\_dispatch is defined ( [\#120632](https://github.com/pytorch/pytorch/pull/120632))**

Defining a subclass with a **torch\_dispatch** entry will now automatically set **torch\_function** to be disabled. This aligns better with all the use cases we’ve observed for subclasses. The main change of behavior is that the result of the torch\_dispatch handler will not go through the default torch\_function handler anymore, wrapping it into the current subclass. This allows in particular for your subclass to return a plain Tensor or another subclass from any op.

The original behavior can be recovered by adding the following to your Tensor subclass:

```
@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
      return super().__torch_function__(func, types, args, kwargs)
```

### **ProcessGroupNCCL removes multi-device-per-thread support from C++ level ( [\#119099](https://github.com/pytorch/pytorch/pull/119099), [\#118674](https://github.com/pytorch/pytorch/pull/118674))**

- Python level support was removed in 2.2.
- To simplify ProcessGroupNCCL’s code, we remove support for multiple cuda devices per thread. To our knowledge, this is not an active use case, but it adds a large burden to our codebase. If you are relying on this, there is no workaround other than rewriting your pytorch program to use one device per process or one device per thread (multi-threads per process is still supported).

### **Removes `no_dist` and `coordinator_rank` from public DCP API's ( [\#121317](https://github.com/pytorch/pytorch/pull/121317))**

As part of an overall effort to simplify our public facing API's for Distributed Checkpointing, we've decided to deprecate usage of the `coordinator_rank` and `no_dist` parameters under `torch.distributed.checkpoint`. In our opinion, these parameters can lead to confusion around the intended effect during API usage, and have limited value to begin with. One concrete example is here, [#118337](https://github.com/pytorch/pytorch/issues/118337), where there is ambiguity in which Process Group is referenced by the coordinator rank (additional context: [#118337](https://github.com/pytorch/pytorch/issues/118337)). In the case of the `no_dist` parameter, we consider this an implementation detail which should be hidden from the user. Starting in this release, `no_dist` is inferred from the initialized state of the process group, assuming the intention is to use collectives if a process group is initialized, and assuming the opposite in the case it is not.

|     |     |
| --- | --- |
| **2.2** | **2.3** |
| ```<br># Version 2.2.2<br>import torch.distributed.checkpoint as dcp<br>dcp.save(<br>	state_dict={"model": model.state_dict()},<br>       checkpoint_id="path_to_model_checkpoint"<br>       no_dist=True,<br>       coordinator_rank=0<br>)<br># ...<br>dcp.load(<br>	state_dict={"model": model.state_dict()},<br>       checkpoint_id="path_to_model_checkpoint"<br>       no_dist=True,<br>       coordinator_rank=0<br>)<br>``` | ```<br># Version 2.2.3<br># no dist is assumed from pg state, and rank 0 is always coordinator.<br>import torch.distributed.checkpoint as dcp<br>dcp.save(<br>	state_dict={"model": model.state_dict()},<br>       checkpoint_id="path_to_model_checkpoint"<br>) <br># ...<br>dcp.load(<br>	state_dict={"model": model.state_dict()},<br>       checkpoint_id="path_to_model_checkpoint"<br>)<br>``` |

### **Remove deprecated tp\_mesh\_dim arg ( [\#121432](https://github.com/pytorch/pytorch/pull/121432))**

Starting from PyTorch 2.3, `parallelize_module` API only accepts a DeviceMesh (the `tp_mesh_dim` argument has been removed). If having a N-D DeviceMesh for multi-dimensional parallelism, you can use `mesh_nd["tp"]` to obtain a 1-D DeviceMesh for tensor parallelism.

## **torch.export**

- Users must pass in an nn.Module to torch.export.export. The reason is that we have several invariants the ExportedProgram that are ambiguous if the top-level object being traced is a function, such as how we guarantee that every call\_function node has an nn\_module\_stack populated, and we offer ways to access the state\_dict/parameters/buffers of the exported program. We'd like torch.export to offer strong invariants—the value proposition of export is that you can trade flexibility for stronger guarantees about your model. ( [#117528](https://github.com/pytorch/pytorch/pull/117528))
- Removed constraints in favor of dynamic\_shapes ( [#117573](https://github.com/pytorch/pytorch/pull/117573), [#117917](https://github.com/pytorch/pytorch/pull/117917), [#117916](https://github.com/pytorch/pytorch/pull/117916), [#120981](https://github.com/pytorch/pytorch/pull/120981), [#120979](https://github.com/pytorch/pytorch/pull/120979))
- ExportedProgram is no longer a callable. Instead users will need to use .module() to call the ExportedProgram. This is to prevent users from treating ExportedPrograms as torch.nn.Modules as we do not plan to support all features that torch.nn.Modules have, like hooks. Instead users can create a proper torch.nn.Module through exported\_program.module() and use that as a callable. ( [#120019](https://github.com/pytorch/pytorch/pull/120019), [#118425](https://github.com/pytorch/pytorch/pull/118425), [#119105](https://github.com/pytorch/pytorch/pull/119105))
- Remove equality\_constraints from ExportedProgram as it is not used or useful anymore. Dimensions with equal constraints will now have the same symbol. ( [#116979](https://github.com/pytorch/pytorch/pull/116979))
- Remove torch.\_export.export in favor of torch.export.export ( [#119095](https://github.com/pytorch/pytorch/pull/119095))
- Remove CallSpec ( [#117671](https://github.com/pytorch/pytorch/pull/117671))

### **Enable fold\_quantize by default in PT2 Export Quantization ( [\#118701](https://github.com/pytorch/pytorch/pull/118701), [\#118605](https://github.com/pytorch/pytorch/pull/118605), [\#119425](https://github.com/pytorch/pytorch/pull/119425), [\#117797](https://github.com/pytorch/pytorch/pull/117797))**

Previously, the PT2 Export Quantization flow did not generate quantized weight by default, but instead used fp32 weight in the quantized model in this pattern: `fp32 weight -> q -> dq -> linear`. Setting `fold_quantize=True` produces a graph with quantized weights in the quantized model in this pattern by default after convert\_pt2e, and users will see a reduction in the model size: `int8 weight -> dq -> linear`.

|     |     |
| --- | --- |
| **2.2** | **2.3** |
| ```<br>folded_model = convert_pt2e(model, fold_quantize=True)<br>non_folded_model = convert_pt2e(model)<br>``` | ```<br>folded_model = convert_pt2e(model)<br>non_folded_model = convert_pt2e(model, fold_quantize=False)<br>``` |

### **Remove deprecated torch.jit.quantized APIs ( [\#118406](https://github.com/pytorch/pytorch/pull/118406))**

All functions and classes under `torch.jit.quantized` will now raise an error if called/instantiated. This API has long been deprecated in favor of `torch.ao.nn.quantized`.

|     |     |
| --- | --- |
| **2.2** | **2.3** |
| ```<br># torch.jit.quantized APIs<br>torch.jit.quantized.quantize_rnn_cell_modules<br>torch.jit.quantized.quantize_rnn_modules<br>torch.jit.quantized.quantize_linear_modules<br>torch.jit.quantized.QuantizedLinear<br>torch.jit.QuantizedLinearFP16<br>torch.jit.quantized.QuantizedGRU<br>torch.jit.quantized.QuantizedGRUCell<br>torch.jit.quantized.QuantizedLSTM<br>torch.jit.quantized.QuantizedLSTMCell<br>``` | ```<br># Corresponding torch.ao.quantization APIs<br>torch.ao.nn.quantized.dynamic.RNNCell<br>torch.ao.quantization.quantize_dynamic APIs<br>torch.ao.nn.quantized.dynamic.Linear<br>torch.ao.nn.quantized.dynamic.GRU<br>torch.ao.nn.quantized.dynamic.GRUCell<br>torch.ao.nn.quantized.dynamic.LSTM<br>``` |

### ...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.3.0)

Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

etiennelndr, 651961, matth-blt, johnnynunez, shink, Burhan-Q, oraluben, Abellegese, Pedro-e, holdjun, and 29 more reacted with thumbs up emoji651961, matth-blt, Burhan-Q, Abellegese, wanderingeek, vantienpham, akihironitta, bryanlimy, Chubercik, gavin-hyl, and 7 more reacted with hooray emojijulian-8897, atalman, ab-smith, yzhangcs, cauliyang, kanishkanarch, vilsonrodrigues, mcaccin, madpeh, Rohanjames1997, and 40 more reacted with heart emojiatalman, Separius, kanishkanarch, wanchaol, debnath-d, luncliff, Neleka, nick-konovalchuk, fkouteib, zichunhao, and 25 more reacted with rocket emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)39 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)17 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)50 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)35 reactions

100 people reacted

## PyTorch 2.2.2 Release, bug fix release

Mar 27, 2024
27 Mar 22:27


![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)

[v2.2.2](https://github.com/pytorch/pytorch/tree/v2.2.2)

[`39901f2`](https://github.com/pytorch/pytorch/commit/39901f229520a5256505ec24782f716ee7ddc843)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Nov 7, 2024, 10:42 AM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.2.2 Release, bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.2.2)

This release is meant to fix the following issues (regressions / silent correctness):

- Properly raise an error when trying to use inductor backend on non-supported platforms such as Windows ( [#115969](https://github.com/pytorch/pytorch/pull/115969))
- Fix mkldnn performance issue on Windows platform ( [#121618](https://github.com/pytorch/pytorch/pull/121618))
- Fix `RuntimeError: cannot create std::vector larger than max_size()` in `torch.nn.functional.conv1d` on non-contiguous cpu inputs by patching OneDNN ( [pytorch/builder#1742](https://github.com/pytorch/builder/pull/1742)) ( [pytorch/builder#1744](https://github.com/pytorch/builder/pull/1744))
- Add support for `torch.distributed.fsdp.StateDictType.FULL_STATE_DICT` for when using `torch.distributed.fsdp.FullyShardedDataParallel` with the `device_mesh` argument ( [#120837](https://github.com/pytorch/pytorch/pull/120837))
- Fix `make triton` command on release branch for users building the release branch from source ( [#121169](https://github.com/pytorch/pytorch/pull/121169))
- Ensure gcc>=9.0 for build from source and cpp\_extensions ( [#120126](https://github.com/pytorch/pytorch/pull/120126))
- Fix cxx11-abi build in release branch ( [pytorch/builder#1709](https://github.com/pytorch/builder/pull/1709))
- Fix building from source on Windows source MSVC 14.38 - VS 2022 ( [#122120](https://github.com/pytorch/pytorch/pull/122120))

Release tracker [#120999](https://github.com/pytorch/pytorch/issues/120999) contains all relevant pull requests related to this release as well as links to related issues.

Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

zhenrong-wang, bailidongjun, huydhn, wanderingeek, lucadiliello, ibrahim324, XJAUJSJZZY, jojuo123, zenmhui, JoshuaEPSamuel, and 6 more reacted with thumbs up emojijwrh, azevedoguigo, zhenrong-wang, bailidongjun, zenmhui, lin72h, csukuangfj, and Youcantstopme2744 reacted with laugh emojihelderc, arvinsingh, unadlib, bayesianbrad, 651961, cdluminate, thr3a, prantoran, ngdlmk, fkouteib, and 28 more reacted with hooray emojizhenrong-wang, bailidongjun, black0017, zenmhui, santhoshkammari, lin72h, csukuangfj, vikram71198, and Youcantstopme2744 reacted with heart emojiN-Friederich, zhenrong-wang, bailidongjun, wanderingeek, lin72h, csukuangfj, and Youcantstopme2744 reacted with rocket emojileo-smi, zhenrong-wang, bailidongjun, and csukuangfj reacted with eyes emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)16 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)8 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)38 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)9 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)7 reactions
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)4 reactions

55 people reacted

## PyTorch 2.2.1 Release, bug fix release

Feb 22, 2024
22 Feb 21:15


![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)

[v2.2.1](https://github.com/pytorch/pytorch/tree/v2.2.1)

[`6c8c5ad`](https://github.com/pytorch/pytorch/commit/6c8c5ad5eaf47a62fafbb4a2747198cbffbf1ff0)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.


GPG key ID: B5690EEEBB952194

Verified
on Nov 7, 2024, 08:35 AM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.2.1 Release, bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.2.1)

This release is meant to fix the following issues (regressions / silent correctness):

- Fix missing OpenMP support on Apple Silicon binaries ( [pytorch/builder#1697](https://github.com/pytorch/builder/pull/1697))
- Fix crash when mixing lazy and non-lazy tensors in one operation ( [#117653](https://github.com/pytorch/pytorch/pull/117653))
- Fix PyTorch performance regression on Linux aarch64 ( [pytorch/builder#1696](https://github.com/pytorch/builder/pull/1696))
- Fix silent correctness in DTensor `_to_copy` operation ( [#116426](https://github.com/pytorch/pytorch/pull/116426))
- Fix properly assigning `param.grad_fn` for next forward ( [#116792](https://github.com/pytorch/pytorch/pull/116792))
- Ensure gradient clear out pending `AsyncCollectiveTensor` in FSDP Extension ( [#116122](https://github.com/pytorch/pytorch/pull/116122))
- Fix processing unflatten tensor on compute stream in FSDP Extension ( [#116559](https://github.com/pytorch/pytorch/pull/116559))
- Fix FSDP `AssertionError` on tensor subclass when setting `sync_module_states=True` ( [#117336](https://github.com/pytorch/pytorch/pull/117336))
- Fix DCP state\_dict cannot correctly find FQN when the leaf module is wrapped by FSDP ( [#115592](https://github.com/pytorch/pytorch/pull/115592))
- Fix OOM when when returning a AsyncCollectiveTensor by forcing `_gather_state_dict()` to be synchronous with respect to the mian stream. ( [#118197](https://github.com/pytorch/pytorch/pull/118197)) ( [#119716](https://github.com/pytorch/pytorch/pull/119716))
- Fix Windows runtime `torch.distributed.DistNetworkError`: \[WinError 32\] The process cannot access the file because it is being used by another process ( [#118860](https://github.com/pytorch/pytorch/pull/118860))
- Update supported python versions in package description ( [#119743](https://github.com/pytorch/pytorch/pull/119743))
- Fix SIGILL crash during `import torch` on CPUs that do not support SSE4.1 ( [#116623](https://github.com/pytorch/pytorch/issues/116623))
- Fix DCP RuntimeError in `get_state_dict` and `set_state_dict` ( [#119573](https://github.com/pytorch/pytorch/pull/119573))
- Fixes for HSDP + TP integration with device\_mesh ( [#112435](https://github.com/pytorch/pytorch/pull/112435)) ( [#118620](https://github.com/pytorch/pytorch/pull/118620)) ( [#119064](https://github.com/pytorch/pytorch/pull/119064)) ( [#118638](https://github.com/pytorch/pytorch/pull/118638)) ( [#119481](https://github.com/pytorch/pytorch/pull/119481))
- Fix numerical error with `mixedmm` on NVIDIA V100 ( [#118591](https://github.com/pytorch/pytorch/pull/118591))
- Fix RuntimeError when using SymInt input invariant when splitting graphs ( [#117406](https://github.com/pytorch/pytorch/pull/117406))
- Fix compile `DTensor.from_local` in trace\_rule\_look up ( [#119659](https://github.com/pytorch/pytorch/pull/119659))
- Improve torch.compile integration with CUDA-11.8 binaries ( [#119750](https://github.com/pytorch/pytorch/pull/119750))

Release tracker [#119295](https://github.com/pytorch/pytorch/issues/119295) contains all relevant pull requests related to this release as well as links to related issues.

Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

etiennelndr, vantienpham, xsa-dev, arvinsingh, wanderingeek, 651961, BryansApple11, Arijaa, khlaifiabilel, N-Friederich, and 38 more reacted with thumbs up emojileo-smi, jwrh, Blacksuan19, htinaunglu, Rusteam, Muhtasham, lin72h, and 0-Angela-Boone1984 reacted with laugh emojiwanchaol, wanderingeek, Santhoshlm10, tugui, khlaifiabilel, andre-brainn, Mindstan, brnaguiar, leo-smi, ThomasRetornaz, and 9 more reacted with hooray emoji2kha, andre-brainn, leo-smi, hammaad2002, javierBarandiaran, Muhtasham, lin72h, willianpaixao, and 0-Angela-Boone1984 reacted with heart emojiBryansApple11, araffin, MARD1NO, AntonioBerna, andre-brainn, Neleka, leo-smi, Muhtasham, lin72h, and willianpaixao reacted with rocket emojileo-smi and gooloosk reacted with eyes emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)48 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)8 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)19 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)9 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)10 reactions
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)2 reactions

72 people reacted

## PyTorch 2.2: FlashAttention-v2, AOTInductor

Jan 30, 2024
30 Jan 17:58


![@jcaip](https://avatars.githubusercontent.com/u/8041643?s=40&v=4)[jcaip](https://github.com/jcaip)

[v2.2.0](https://github.com/pytorch/pytorch/tree/v2.2.0)

[`8ac9b20`](https://github.com/pytorch/pytorch/commit/8ac9b20d4b090c213799e81acf48a55ea8d437d6)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.
The key has expired.


GPG key ID: 4AEE18F83AFDEB23

Expired

Verified
on Jan 16, 2024, 02:59 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.2: FlashAttention-v2, AOTInductor](https://github.com/pytorch/pytorch/releases/tag/v2.2.0)

# PyTorch 2.2 Release Notes

- Highlights
- Backwards Incompatible Changes
- Deprecations
- New Features
- Improvements
- Bug fixes
- Performance
- Documentation

# Highlights

We are excited to announce the release of PyTorch® 2.2! PyTorch 2.2 offers ~2x performance improvements to `scaled_dot_product_attention` via FlashAttention-v2 integration, as well as AOTInductor, a new ahead-of-time compilation and deployment tool built for non-python server-side deployments.

This release also includes improved torch.compile support for Optimizers, a number of new inductor optimizations, and a new logging mechanism called TORCH\_LOGS.

**Please note that we are [deprecating macOS x86 support](https://github.com/pytorch/pytorch/issues/114602), and PyTorch 2.2.x will be the last version that supports macOS x64.**

Along with 2.2, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog.

This release is composed of 3,628 commits and 521 contributors since PyTorch 2.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.2. More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.

Summary:

- `scaled_dot_product_attention` (SDPA) now supports FlashAttention-2, yielding around 2x speedups compared to previous versions.
- PyTorch 2.2 introduces a new ahead-of-time extension of TorchInductor called AOTInductor, designed to compile and deploy PyTorch programs for non-python server-side.
- `torch.distributed` supports a new abstraction for initializing and representing ProcessGroups called device\_mesh.
- PyTorch 2.2 ships a standardized, configurable logging mechanism called TORCH\_LOGS.
- A number of torch.compile improvements are included in PyTorch 2.2, including improved support for compiling Optimizers and improved TorchInductor fusion and layout optimizations.
- Please note that we are deprecating macOS x86 support, and PyTorch 2.2.x will be the last version that supports macOS x64.
- `torch.ao.quantization` now offers a prototype `torch.export` based flow

|     |     |     |     |
| --- | --- | --- | --- |
| **Stable** | **Beta** | **Prototype** | **Performance Improvements** |
|  | FlashAttentionV2 backend for scaled dot product attention | PT 2 Quantization | Inductor optimizations |
|  | AOTInductor | Scaled dot product attention support for jagged layout NestedTensors | aarch64-linux optimizations (AWS Graviton) |
|  | TORCH\_LOGS |  |  |
|  | torch.distributed.device\_mesh |  |  |
|  | torch.compile + Optimizers |  |  |

\*To see a full list of public 2.2 - 1.12 feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).

# Tracked Regressions

### **Performance reduction when using NVLSTree algorithm in NCCL 2.19.3 ( [\#117748](https://github.com/pytorch/pytorch/issues/117748))**

We have noticed a performance regression introduced to all-reduce in NCCL 2.19.3. Please use version 2.19.1 instead.

### **Poor numeric stability of loss when training with FSDP + DTensor ( [\#117471](https://github.com/pytorch/pytorch/issues/117471))**

We observe the loss will flatline randomly while training with FSDP + DTensor in some instances.

# Backwards Incompatible Changes

### **Building PyTorch from source now requires GCC 9.4 or newer ( [\#112858](https://github.com/pytorch/pytorch/pull/112858))**

GCC 9.4 is the oldest version fully compatible with C++17, which the PyTorch codebase has migrated to from C++14.

### **Updated flash attention kernel in `scaled_dot_product_attention` to use Flash Attention v2 ( [\#105602](https://github.com/pytorch/pytorch/pull/105602))**

Previously, the v1 Flash Attention kernel had a Windows implementation. So if a user on Windows had explicitly forced the flash attention kernel to be run by using `sdp_kernel` context manager with only flash attention enabled, it would work. In 2.2, if the `sdp_kernel` context manager must be used, use the memory efficient or math kernel if on Windows.

```
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
  torch.nn.functional.scaled_dot_product_attention(q,k,v)
```

```
# Don't force flash attention to be used if using sdp_kernel on Windows
with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):
  torch.nn.functional.scaled_dot_product_attention(q,k,v)
```

### **Rewrote DTensor (Tensor Parallel) APIs to improve UX ( [\#114732](https://github.com/pytorch/pytorch/pull/114732))**

In PyTorch 2.1 or before, users can use ParallelStyles like `PairwiseParallel` and specify input/output layout with functions like `make_input_replicate_1d` or `make_output_replicate_1d`. And we have default values for \_prepare\_input and \_prepare\_output. The UX of Tensor Parallel was like:

```
from torch.distributed.tensor.parallel.style import (
    ColwiseParallel,
    make_input_replicate_1d,
    make_input_reshard_replicate,
    make_input_shard_1d,
    make_input_shard_1d_last_dim,
    make_sharded_output_tensor,
    make_output_replicate_1d,
    make_output_reshard_tensor,
    make_output_shard_1d,
    make_output_tensor,
    PairwiseParallel,
    parallelize_module,
)
from torch.distributed.tensor import DeviceMesh

module = DummyModule()
device_mesh = DeviceMesh("cuda", list(range(self.world_size)))
parallelize_module(module, device_mesh, PairwiseParallel(_prepare_input=make_input_replicate_1d))
...
```

Starting from PyTorch 2.2, we simplified parallel styles to only contain `ColwiseParallel` and `RowwiseParallel` because other ParallelStyle can consist of these two. We also deleted the input/output functions, and started using `input_layouts` and `output_layouts` as kwargs instead to specify the sharding layout of both input/output tensors. Finally, added PrepareModuleInput/PrepareModuleOutput style, and no default arguments for layouts in these two styles and users need to specify them to think about the sharding layouts.

```
from torch.distributed.tensor.parallel.style import (
    ColwiseParallel,
    PrepareModuleInput,
    RowwiseParallel,
    parallelize_module,
)
from torch.distributed._tensor import init_device_mesh

module = SimpleMLPModule()
device_mesh = init_device_mesh("cuda", (self.world_size,)))
parallelize_module(
   module,
   device_mesh,
   {
      "fqn": PrepareModuleInput(
                input_layouts=Shard(0),
                desired_input_layouts=Replicate()
             ),
      "fqn.net1": ColwiseParallel(),
      "fqn.net2": RowwiseParallel(output_layouts=Shard(0)),
   }
)
...
```

### **`UntypedStorage.resize_` now uses the original device instead of the current device context ( [\#113386](https://github.com/pytorch/pytorch/pull/113386))**

Before this PR, `UntypedStorage.resize_` would move data to the current CUDA device index (given by `torch.cuda.current_device()`).

Now, `UntypedStorage.resize_()` keeps the data on the same device index that it was on before, regardless of the current device index.

| 2.1 | 2.2 |
| --- | --- |
| ```<br>>>> import torch<br>>>> with torch.cuda.device('cuda:0'):<br>...:     a = torch.zeros(0, device='cuda:1')<br>...:     print(a.device)<br>...:     a = a.untyped_storage().resize_(0)<br>...:     print(a.device)<br>cuda:1<br>cuda:0<br>``` | ```<br>>>> import torch<br>>>> with torch.cuda.device('cuda:0'):<br>...:     a = torch.zeros(0, device='cuda:1')<br>...:     print(a.device)<br>...:     a = a.untyped_storage().resize_(0)<br>...:     print(a.device)<br>cuda:1<br>cuda:1<br>``` |

### **Wrapping a function with set\_grad\_enabled will consume its global mutation ( [\#113359](https://github.com/pytorch/pytorch/pull/113359))**

This bc-breaking change fixes some unexpected behavior when `set_grad_enabled` is used as a decorator.

| 2.1 | 2.2 |
| --- | --- |
| ```<br>>>> import torch<br>>>> @torch.set_grad_enabled(False)  # unexpectedly, this mutates the grad mode!<br>    def inner_func(x):<br>        return x.sin()<br>>>> torch.is_grad_enabled()<br>True<br>``` | ```<br>>>> import torch<br>>>> @torch.set_grad_enabled(False)  # unexpectedly, this mutates the grad mode!<br>    def inner_func(x):<br>        return x.sin()<br>>>> torch.is_grad_enabled()<br>False<br>``` |

### **Deprecated `verbose` parameter in `LRscheduler` constructors ( [\#111302](https://github.com/pytorch/pytorch/pull/111302))**

As part of our decision to move towards a consolidated logging system, we are deprecating the `verbose` flag in `LRScheduler`.

If you would like to print the learning rate during execution, please use `get_last_lr()`

| 2.1 | 2.2 |
| --- | --- |
| ```<br>optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)<br>scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True)<br>for epoch in range(10):<br>    train(...)<br>    val_loss = validate(...)<br>    # Note that step should be called after validate()<br>    scheduler.step(val_loss)<br>``` | ```<br>optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)<br>scheduler = ReduceLROnPlateau(optimizer, 'min')<br>for epoch in range(10):<br>    train(...)<br>    val_loss = validate(...)<br>    # Note that step should be called after validate()<br>    scheduler.step(val_loss)<br>	print(f"Epoch {epoch} has concluded with lr of {scheduler.get_last_lr()}")<br>```<br></td... |

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.2.0)

Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

Fissium, cdluminate, akihironitta, ideaguy3d, wanderingeek, gallegogt, realiti4, cauliyang, K-H-Ismail, rvlobato, and 78 more reacted with thumbs up emojizhiqwang, xavierdmello, jwrh, and color88 reacted with laugh emojirabinadk1, akihironitta, ideaguy3d, wanderingeek, AngryLoki, skandermoalla, johnnv1, Totto16, Jamim, tymokvo, and 17 more reacted with hooray emojiakihironitta, ideaguy3d, samils7, Totto16, devanshkv, Jamim, evdcush, anthonyalayo, ArdeshirV, thomasjo, and 13 more reacted with heart emojiakihironitta, ideaguy3d, atalman, kayzliu, Totto16, gau-nernst, Jamim, evdcush, wanchaol, notnitsuj, and 17 more reacted with rocket emojizhiqwang, xavierdmello, gugarosa, and j316chuck reacted with eyes emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)88 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)4 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)27 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)23 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)27 reactions
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)4 reactions

129 people reacted

## PyTorch 2.1.2 Release, bug fix release

Dec 14, 2023
15 Dec 01:59


![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)

[v2.1.2](https://github.com/pytorch/pytorch/tree/v2.1.2)

[`a8e7c98`](https://github.com/pytorch/pytorch/commit/a8e7c98cb95ff97bb30a728c6b2a1ce6bff946eb)

Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.1.2 Release, bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.1.2)

This release is meant to fix the following issues (regressions / silent correctness):

- Fix crashes for float16 empty tensors ( [#115183](https://github.com/pytorch/pytorch/pull/115183))
- Fix MPS memory corruption when working with tensor slices ( [#114838](https://github.com/pytorch/pytorch/pull/114838))
- Fix crashes during Conv backward pass on MPS devices ( [#113398](https://github.com/pytorch/pytorch/pull/113398))
- Partially fix nn.Linear behavior on AArch64 platform ( [#110150](https://github.com/pytorch/pytorch/pull/110150))
- Fix cosine\_similarity for tensors of different sizes ( [#109363](https://github.com/pytorch/pytorch/pull/109363))
- Package missing headers needed for extension development ( [#113055](https://github.com/pytorch/pytorch/pull/113055))
- Improve error handling of `torch.set_num_threads` ( [#113684](https://github.com/pytorch/pytorch/pull/113684))
- Fix profiling traces generation ( [#113763](https://github.com/pytorch/pytorch/pull/113763))

The Cherry pick tracker [#113962](https://github.com/pytorch/pytorch/issues/113962) contains all relevant pull requests related to this release as well as links to related issues.

Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

luncliff, tolgacangoz, rino2000, yhyu13, Abeautifulsnow, sirius777coder, YuCao16, vantienpham, botbw, BEaN001, and 29 more reacted with thumbs up emojijwrh and felipegargal reacted with laugh emojiMARD1NO, zhenrong-wang, anaximeno, gdippolito, 651961, DarkDumpTruck, hammaad2002, lucadiliello, and gadhvirushiraj reacted with hooray emojianurag12-webster, AhmadHakami, teuncm, vilsonrodrigues, Sai-Suraj-27, AntonioBerna, azevedoguigo, mjamroz, and donaldKelly17 reacted with heart emojiwangg12, gau-nernst, and lsrock1 reacted with rocket emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)39 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)2 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)9 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)9 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)3 reactions

58 people reacted

## PyTorch 2.1.1 Release, bug fix release

Nov 15, 2023
15 Nov 22:59


![@jerryzh168](https://avatars.githubusercontent.com/u/4958441?s=40&v=4)[jerryzh168](https://github.com/jerryzh168)

[v2.1.1](https://github.com/pytorch/pytorch/tree/v2.1.1)

[`4c55dc5`](https://github.com/pytorch/pytorch/commit/4c55dc50355d5e923642c59ad2a23d6ad54711e7)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.
The key has expired.


GPG key ID: 4AEE18F83AFDEB23

Expired

Verified
on Jan 16, 2024, 02:59 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.1.1 Release, bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.1.1)

This release is meant to fix the following issues (regressions / silent correctness):

- Remove spurious warning in comparison ops ( [#112170](https://github.com/pytorch/pytorch/pull/112170))
- Fix segfault in foreach\_\* operations when input list length does not match ( [#112349](https://github.com/pytorch/pytorch/pull/112349))
- Fix cuda driver API to load the appropriate .so file ( [#112996](https://github.com/pytorch/pytorch/pull/112996))
- Fix missing CUDA initialization when calling FFT operations ( [#110326](https://github.com/pytorch/pytorch/pull/110326))
- Ignore beartype==0.16.0 within the onnx package as it is incompatible ( [#111861](https://github.com/pytorch/pytorch/pull/111861))
- Fix the behavior of torch.new\_zeros in onnx due to TorchScript behavior change ( [#111694](https://github.com/pytorch/pytorch/pull/111694))
- Remove unnecessary slow code in `torch.distributed.checkpoint.optimizer.load_sharded_optimizer_state_dict` ( [#111687](https://github.com/pytorch/pytorch/pull/111687))
- Add `planner` argument to `torch.distributed.checkpoint.optimizer.load_sharded_optimizer_state_dict` ( [#111393](https://github.com/pytorch/pytorch/pull/111393))
- Continue if param not exist in sharded load in `torch.distributed.FSDP` ( [#109116](https://github.com/pytorch/pytorch/pull/109116))
- Fix handling of non-contiguous bias\_mask in `torch.nn.functional.scaled_dot_product_attention` ( [#112673](https://github.com/pytorch/pytorch/pull/112673))
- Fix the meta device implementation for `nn.functional.scaled_dot_product_attention` ( [#110893](https://github.com/pytorch/pytorch/pull/110893))
- Fix copy from mps to cpu device when storage\_offset is non-zero ( [#109557](https://github.com/pytorch/pytorch/pull/109557))
- Fix segfault in `torch.sparse.mm` for non-contiguous inputs ( [#111742](https://github.com/pytorch/pytorch/pull/111742))
- Fix circular import between Dynamo and einops ( [#110575](https://github.com/pytorch/pytorch/pull/110575))
- Verify flatbuffer module fields are initialized for mobile deserialization ( [#109794](https://github.com/pytorch/pytorch/pull/109794))

The [#110961](https://github.com/pytorch/pytorch/issues/110961) contains all relevant pull requests related to this release as well as links to related issues.

Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)60tayden, wanderingeek, jerryzh168, leslie-fang-intel, gau-nernst, YuanZhaohan, BrenoAV, ppogg, light-abc, luncliff, and 50 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)2jwrh and zhenrong-wang reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)9wanderingeek, atalman, GrantorShadow, zhenrong-wang, ngdlmk, andrewlee302, markf94, SocioPJ, and HemanthSai7 reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)10MadScie254, wilhelmagren, xAlpharax, SocioPJ, M0nteCarl0, DanteDeLordran, HemanthSai7, cesardz1q84, yrmo, and Ramineon reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)11Nesasio, zhenrong-wang, Dxig, nairbv, debnath-d, SocioPJ, ianpark318, HemanthSai7, PasNinii, dangbert, and pavoltravnik reacted with rocket emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)60 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)2 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)9 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)10 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)11 reactions

78 people reacted

## PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing

Oct 4, 2023
04 Oct 17:32


![@jerryzh168](https://avatars.githubusercontent.com/u/4958441?s=40&v=4)[jerryzh168](https://github.com/jerryzh168)

[v2.1.0](https://github.com/pytorch/pytorch/tree/v2.1.0)

[`7bcf7da`](https://github.com/pytorch/pytorch/commit/7bcf7da3a268b435777fe87c7794c382f444e86d)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.
The key has expired.


GPG key ID: 4AEE18F83AFDEB23

Expired

Verified
on Jan 16, 2024, 02:59 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing](https://github.com/pytorch/pytorch/releases/tag/v2.1.0)

# PyTorch 2.1 Release Notes

- Highlights
- Backwards Incompatible Change
- Deprecations
- New Features
- Improvements
- Bug fixes
- Performance
- Documentation
- Developers
- Security

# Highlights

We are excited to announce the release of PyTorch® 2.1! PyTorch 2.1 offers automatic dynamic shape support in torch.compile, torch.distributed.checkpoint for saving/loading distributed training jobs on multiple ranks in parallel, and torch.compile support for the NumPy API.

In addition, this release offers numerous performance improvements (e.g. CPU inductor improvements, AVX512 support, scaled-dot-product-attention support) as well as a prototype release of torch.export, a sound full-graph capture mechanism, and `torch.export`-based quantization.

Along with 2.1, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog.

This release is composed of 6,682 commits and 784 contributors since 2.0. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.1. More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.

Summary:

- `torch.compile` now includes automatic support for detecting and minimizing recompilations due to tensor shape changes using automatic dynamic shapes.
- `torch.distributed.checkpoint` enables saving and loading models from multiple ranks in parallel, as well as resharding due to changes in cluster topology.
- `torch.compile` can now compile NumPy operations via translating them into PyTorch-equivalent operations.
- `torch.compile` now includes improved support for Python 3.11.
- New CPU performance features include inductor improvements (e.g. bfloat16 support and dynamic shapes), AVX512 kernel support, and scaled-dot-product-attention kernels.
- `torch.export`, a sound full-graph capture mechanism is introduced as a prototype feature, as well as torch.export-based quantization.
- `torch.sparse` now includes prototype support for semi-structured (2:4) sparsity on NVIDIA® GPUs.

|     |     |     |     |
| --- | --- | --- | --- |
| **Stable** | **Beta** | **Prototype** | **Performance Improvements** |
|  | Automatic Dynamic Shapes | torch.export() | AVX512 kernel support |
|  | torch.distributed.checkpoint | torch.export-based Quantization | CPU optimizations for scaled-dot-product-attention (SDPA) |
|  | torch.compile + NumPy | semi-structured (2:4) sparsity | CPU optimizations for bfloat16 |
|  | torch.compile + Python 3.11 | cpp\_wrapper for torchinductor |  |
|  | torch.compile + autograd.Function |  |  |
|  | third-party device integration: PrivateUse1 |  |  |

\*To see a full list of public 2.1, 2.0, and 1.13 feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).

For more details about these highlighted features, you can look at the release blogpost.

Below are the full release notes for this release.

# Backwards Incompatible Changes

### Building PyTorch from source now requires C++ 17 ( [\#100557](https://github.com/pytorch/pytorch/pull/100557))

The PyTorch codebase has migrated from the C++14 to the C++17 standard, so a C++17 compatible compiler is now required to compile PyTorch, to integrate with libtorch, or to implement a C++ PyTorch extension.

### Disable `torch.autograd.{backward, grad}` for complex scalar output ( [\#92753](https://github.com/pytorch/pytorch/pull/92753))

Gradients are not defined for functions that don't return real outputs; we now raise an error if you try to call backward on complex outputs. Previously, the complex component of the output was implicitly ignored. If you wish to preserve this behavior, you must now explicitly call `.real` on your complex outputs before calling `.grad()` or `.backward()`.

#### Example

```
def fn(x):
    return (x * 0.5j).sum()

x = torch.ones(1, dtype=torch.double, requires_grad=True)
o = fn(x)
```

#### 2.0.1

```
o.backward()
```

#### 2.1

```
o.real.backward()
```

### Update non-reentrant checkpoint to allow nesting and support `autograd.grad` ( [\#90105](https://github.com/pytorch/pytorch/pull/90105))

As a part of a larger refactor to `torch.utils.checkpoint`, we changed the interaction activation checkpoint and `retain_graph=True`. Previously in 2.0.1, recomputed activations are kept alive if `retain_graph=True`, in PyTorch 2.1, non-reentrant impl now clears recomputed tensors on backward immediately upon unpack, even if `retain_graph=True`. This has the following additional implications: (1) Accessing `ctx.saved_tensor` twice in the same backward will now raise an error. (2) Accessing `_saved_tensors` multiple times will silently recompute forward multiple times.

#### 2.1

```
class Func(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x):
        out = x.exp()
        ctx.save_for_backward(out)
        return out

    @staticmethod
    def backward(ctx, x);
        out, = ctx.saved_tensors
        # Calling ctx.saved_tensors again will raise in 2.1
        out, = ctx.saved_tensors
        return out

a = torch.tensor(1., requires_grad=True)

def fn(x):
    return Func.apply(x)

out = torch.utils.checkpoint(fn, (a,), use_reentrant=False)

def fn2(x):
    return x.exp()

out = torch.utils.checkpoint(fn2, (a,), use_reentrant=False)

out.grad_fn._saved_result
# Calling _saved_result will trigger another unpack, and lead to forward being
# recomputed again
out.grad_fn._saved_result
```

### Only sync buffers when `broadcast_buffers` is True ( [\#100729](https://github.com/pytorch/pytorch/pull/100729))

- In PyTorch 2.0.1 and previous releases, when users use DistributedDataParallel (DDP), all buffers were synced automatically even if users set flag `broadcast_buffers` to be `False`:

```
from torch.nn.parallel import DistributedDataParallel as DDP
module = torch.nn.Linear(4, 8)
module = DDP(module) # Buffer is synchronized across all devices.
module = DDP(module, broadcast_buffers=False) # Buffer is synchronized across all devices.
...
```

- Starting with PyTorch 2.1, if users specify the flag `broadcast_buffers` to be `False`, we don’t sync the buffer across devices:

```
from torch.nn.parallel import DistributedDataParallel as DDP
module = torch.nn.Linear(4, 8)
module = DDP(module) # Buffer is synchronized across all devices.
module = DDP(module, broadcast_buffers=False) # Buffer is NOT synchronized across all devices
...
```

### Remove store barrier after PG init ( [\#99937](https://github.com/pytorch/pytorch/pull/99937))

- In PyTorch 2.0.1 and previous releases, after we initialize PG, we always call store based barrier:

```
from torch.distributed.distributed_c10d import init_process_group
init_process_group(...) # Will call _store_based_barrier in the end.
...
```

- Starting with PyTorch 2.1, after we initialize PG, the environment variable `TORCH_DIST_INIT_BARRIER` controls whether we call store based barrier or not:

```
from torch.distributed.distributed_c10d import init_process_group
import os
os.environ["TORCH_DIST_INIT_BARRIER"] = "1" # This is the default behavior
init_process_group(...) # Will call _store_based_barrier in the end.
os.environ["TORCH_DIST_INIT_BARRIER"] = "0"
init_process_group(...) # Will not call _store_based_barrier in the end.
...
```

### Disallow non-bool masks in `torch.masked_{select, scatter, fill}` ( [\#96112](https://github.com/pytorch/pytorch/pull/96112), [\#97999](https://github.com/pytorch/pytorch/pull/97999), [\#96594](https://github.com/pytorch/pytorch/pull/96594))

Finish the deprecation cycle for non-bool masks. Functions now require the `dtype` of the mask to be `torch.bool`.

```
>>> # 2.0.1
>>> inp = torch.rand(3)
>>> mask = torch.tensor([0, 1, 0], dtype=torch.uint8)
>>> torch.masked_select(inp, mask)
UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1855.)
  torch.masked_select(inp, mask)

>>> torch.masked_select(inp, mask.to(dtype=torch.bool))
# Works fine

>>> correct_mask = torch.tensor([0, 1, 0], dtype=torch.bool)
>>> torch.masked_select(inp, correct_mask)
# Works fine

>>> # 2.1
>>> inp = torch.rand(3)
>>> mask = torch.tensor([0, 1, 0], dtype=torch.uint8)
>>> torch.masked_select(inp, mask)
RuntimeError: masked_select: expected BoolTensor for mask

>>> correct_mask = torch.tensor([0, 1, 0], dtype=torch.bool)
>>> torch.masked_select(inp, correct_mask)
# Works fine

>>> torch.masked_select(inp, mask.to(dtype=torch.bool))
# Works fine
```

### Fix the result of `torch.unique` to make it consistent with NumPy when `dim` is specified ( [\#101693](https://github.com/pytorch/pytorch/pull/101693))

The `dim` argument was clarified and its behavior aligned to match the one from NumPy to signify which sub-tensor to consider when considering uniqueness. See the documentation for more details, [https://pytorch.org/docs/stable/generated/torch.unique.html](https://pytorch.org/docs/stable/generated/torch.unique.html)

### Make the Index Rounding Mode Consistent Between the 2D and 3D GridSample Nearest Neighbor Interpolations ( [\#97000](https://github.com/pytorch/pytorch/pull/97000))

Prior to this change, for `torch.nn.functional.grid_sample(mode='nearest')` the forward 2D kernel used `std::nearbyint` whereas the forward 3D kernel used `std::round` in order to determine the nearest pixel locations after un-normalization of the grid. Additionally, the backward kernels for both ...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.1.0)

Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)80kit1980, zhxchen17, qing-shang, egesko, alexeykudinkin, Fissium, QuanyiLi, vfdev-5, iceychris, ferraridamiano, and 70 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)59bow, zhiqwang, Abiodun-code, xfo-0, and zhangboSJTU reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)46khushi-411, nviraj, duanzhiihao, janosh, khiner, atalman, lazyoracle, iceychris, QuanyiLi, dineshpinto, and 36 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)29khushi-411, nviraj, simpleParadox, duanzhiihao, khiner, egesko, iceychris, 9bow, prm-james-hill, toastertaster, and 19 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)29ilya16, gugarosa, ngdlmk, iceychris, rishub-tamirisa, HennerM, Separius, 9bow, George614, prm-james-hill, and 19 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)9wangg12, zhiqwang, luncliff, csukuangfj, Abiodun-code, risingwen, tolgacangoz, clecust, and zhangboSJTU reacted with eyes emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)80 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)5 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)46 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)29 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)29 reactions
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)9 reactions

138 people reacted

## PyTorch 2.0.1 Release, bug fix release

May 8, 2023
08 May 19:55


![@drisspg](https://avatars.githubusercontent.com/u/32754868?s=40&v=4)[drisspg](https://github.com/drisspg)

[v2.0.1](https://github.com/pytorch/pytorch/tree/v2.0.1)

[`e9ebda2`](https://github.com/pytorch/pytorch/commit/e9ebda29d87ce0916ab08c06ab26fd3766a870e5)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.
The key has expired.


GPG key ID: 4AEE18F83AFDEB23

Expired

Verified
on Nov 6, 2024, 01:28 AM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.0.1 Release, bug fix release](https://github.com/pytorch/pytorch/releases/tag/v2.0.1)

This release is meant to fix the following issues (regressions / silent correctness):

- Fix `_canonical_mask` throws warning when bool masks passed as input to TransformerEncoder/TransformerDecoder ( [#96009](https://github.com/pytorch/pytorch/pull/96009), [#96286](https://github.com/pytorch/pytorch/pull/96286))
- Fix Embedding bag max\_norm=-1 causes leaf Variable that requires grad is being used in an in-place operation [#95980](https://github.com/pytorch/pytorch/pull/95980)
- Fix type hint for torch.Tensor.grad\_fn, which can be a torch.autograd.graph.Node or None. [#96804](https://github.com/pytorch/pytorch/pull/96804)
- Can’t convert float to int when the input is a scalar np.ndarray. [#97696](https://github.com/pytorch/pytorch/pull/97696)
- Revisit torch.\_six.string\_classes removal [#97863](https://github.com/pytorch/pytorch/pull/97863)
- Fix module backward pre-hooks to actually update gradient [#97983](https://github.com/pytorch/pytorch/pull/97983)
- Fix load\_sharded\_optimizer\_state\_dict error on multi node [#98063](https://github.com/pytorch/pytorch/pull/98063)
- Warn once for TypedStorage deprecation [#98777](https://github.com/pytorch/pytorch/pull/98777)
- cuDNN V8 API, Fix incorrect use of emplace in the benchmark cache [#97838](https://github.com/pytorch/pytorch/pull/97838)

### Torch.compile:

- Add support for Modules with custom **getitem** method to torch.compile [#97932](https://github.com/pytorch/pytorch/pull/97932)
- Fix improper guards with on list variables. [#97862](https://github.com/pytorch/pytorch/pull/97862)
- Fix Sequential nn module with duplicated submodule [#98880](https://github.com/pytorch/pytorch/pull/98880)

### Distributed:

- Fix distributed\_c10d's handling of custom backends [#95072](https://github.com/pytorch/pytorch/pull/95072)
- Fix MPI backend not properly initialized [#98545](https://github.com/pytorch/pytorch/pull/98545)

### NN\_frontend:

- Update Multi-Head Attention's doc string [#97046](https://github.com/pytorch/pytorch/pull/97046)
- Fix incorrect behavior of `is_causal` paremeter for torch.nn.TransformerEncoderLayer.forward [#97214](https://github.com/pytorch/pytorch/pull/97214)
- Fix error for SDPA on sm86 and sm89 hardware [#99105](https://github.com/pytorch/pytorch/pull/99105)
- Fix nn.MultiheadAttention mask handling [#98375](https://github.com/pytorch/pytorch/pull/98375)

### DataLoader:

- Fix regression for pin\_memory recursion when operating on bytes [#97737](https://github.com/pytorch/pytorch/pull/97737)
- Fix collation logic [#97789](https://github.com/pytorch/pytorch/pull/97789)
- Fix Ppotentially backwards incompatible change with DataLoader and is\_shardable Datapipes [#97287](https://github.com/pytorch/pytorch/pull/97287)

### MPS:

- Fix LayerNorm crash when input is in float16 [#96208](https://github.com/pytorch/pytorch/pull/96208)
- Add support for cumsum on int64 input [#96733](https://github.com/pytorch/pytorch/pull/96733)
- Fix issue with setting BatchNorm to non-trainable [#98794](https://github.com/pytorch/pytorch/pull/98794)

### Functorch:

- Fix Segmentation Fault for vmaped function accessing BatchedTensor.data [#97237](https://github.com/pytorch/pytorch/pull/97237)
- Fix index\_select support when dim is negative [#97916](https://github.com/pytorch/pytorch/pull/97916)
- Improve docs for autograd.Function support [#98020](https://github.com/pytorch/pytorch/pull/98020)
- Fix Exception thrown when running Migration guide example for jacrev [#97746](https://github.com/pytorch/pytorch/pull/97746)

### Releng:

- Fix Convolutions for CUDA-11.8 wheel builds [#99451](https://github.com/pytorch/pytorch/pull/99451)
- Fix Import torchaudio + torch.compile crashes on exit [#96231](https://github.com/pytorch/pytorch/pull/96231)
- Linux aarch64 wheels are missing the mkldnn+acl backend support - [pytorch/builder@ `54931c2`](https://github.com/pytorch/builder/commit/54931c264ed3e7346899f547a272c4329cc8933b)
- Linux aarch64 torchtext 0.15.1 wheels are missing for aarch64\_linux platform - [pytorch/builder#1375](https://github.com/pytorch/builder/issues/1375)
- Enable ROCm 5.4.2 manywheel and python 3.11 builds [#99552](https://github.com/pytorch/pytorch/pull/99552)
- PyTorch cannot be installed at the same time as numpy in a conda env on osx-64 / Python 3.11 [#97031](https://github.com/pytorch/pytorch/issues/97031)
- Illegal instruction (core dumped) on Raspberry Pi 4.0 8gb - [pytorch/builder#1370](https://github.com/pytorch/builder/pull/1370)

### Torch.optim:

- Fix fused AdamW causes NaN loss [#95847](https://github.com/pytorch/pytorch/pull/95847)
- Fix Fused AdamW has worse loss than Apex and unfused AdamW for fp16/AMP [#98620](https://github.com/pytorch/pytorch/pull/98620)

The [release tracker](https://github.com/pytorch/pytorch/issues/97272) should contain all relevant pull requests related to this release as well as links to related issues

Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)87mwanjajoel, snario, vfdev-5, johnnynunez, anaximeno, YuCao16, yoshoku, houseme, mrverdant13, prat96, and 77 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)4GreenSlime1024, jwrh, Voyajer, and CaoHaiNam reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)26atalman, johnnv1, Forbu, YuCao16, mrverdant13, thevasudevgupta, heyaudace, zhiqwang, 9bow, PennyFranklin, and 16 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)16zhiqwang, 9bow, valeriocardoso, PennyFranklin, khushi-411, smbl64, GreenSlime1024, ingo-m, yjianzhu, kris-gzy, and 6 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)16zhiqwang, 9bow, PennyFranklin, semaphore-egg, lbluque, iamabhaytiwari343, GreenSlime1024, hal-314, superherointj, FNsi, and 6 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)4GreenSlime1024, Voyajer, davidboudinp, and tolgacangoz reacted with eyes emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)87 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)4 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)26 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)16 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)16 reactions
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)4 reactions

113 people reacted

## PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever

Mar 15, 2023
15 Mar 19:38


![@drisspg](https://avatars.githubusercontent.com/u/32754868?s=40&v=4)[drisspg](https://github.com/drisspg)

[v2.0.0](https://github.com/pytorch/pytorch/tree/v2.0.0)

[`c263bd4`](https://github.com/pytorch/pytorch/commit/c263bd43e8e8502d4726643bc6fd046f0130ac0e)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.
The key has expired.


GPG key ID: 4AEE18F83AFDEB23

Expired

Verified
on Nov 5, 2024, 08:54 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever](https://github.com/pytorch/pytorch/releases/tag/v2.0.0)

# PyTorch 2.0 Release notes

- Highlights
- Backwards Incompatible Changes
- Deprecations
- New Features
- Improvements
- Bug fixes
- Performance
- Documentation

# Highlights

We are excited to announce the release of PyTorch® 2.0 ( [release note](https://github.com/pytorch/pytorch/releases)) which we highlighted during the [PyTorch Conference](https://www.youtube.com/@PyTorch/playlists?view=50&sort=dd&shelf_id=2) on 12/2/22! PyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates at compiler level under the hood with faster performance and support for Dynamic Shapes and Distributed.

This next-generation release includes a Stable version of Accelerated Transformers (formerly called Better Transformers); Beta includes torch.compile as the main API for PyTorch 2.0, the scaled\_dot\_product\_attention function as part of torch.nn.functional, the MPS backend, functorch APIs in the torch.func module; and other Beta/Prototype improvements across various inferences, performance and training optimization features on GPUs and CPUs. For a comprehensive introduction and technical overview of torch.compile, please visit the 2.0 [Get Started page](https://pytorch.org/get-started/pytorch-2.0).

Along with 2.0, we are also releasing a series of beta updates to the PyTorch domain libraries, including those that are in-tree, and separate libraries including TorchAudio, TorchVision, and TorchText. An update for TorchX is also being released as it moves to community supported mode. More details can be found in this [library blog](https://pytorch.org/blog/new-library-updates-in-pytorch-2.0/).

This release is composed of over 4,541 commits and 428 contributors since 1.13.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.0 and the overall 2-series this year.

Summary:

- torch.compile is the main API for PyTorch 2.0, which wraps your model and returns a compiled model. It is a fully additive (and optional) feature and hence 2.0 is 100% backward compatible by definition.
- As an underpinning technology of torch.compile, TorchInductor with Nvidia and AMD GPUs will rely on OpenAI Triton deep learning compiler to generate performant code and hide low level hardware details. OpenAI Triton-generated kernels achieve performance that's on par with hand-written kernels and specialized cuda libraries such as cublas.
- Accelerated Transformers introduce high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA). The API is integrated with torch.compile() and model developers may also use the [scaled dot product attention](https://pytorch.org/docs/2.0/generated/torch.nn.functional.scaled_dot_product_attention.html) kernels directly by calling the new scaled\_dot\_product\_attention() operator.
- Metal Performance Shaders (MPS) backend provides GPU accelerated PyTorch training on Mac platforms with added support for Top 60 most used ops, bringing coverage to over 300 operators.
- Amazon AWS optimize the PyTorch CPU inference on AWS Graviton3 based [C7g instances](https://aws.amazon.com/blogs/aws/new-amazon-ec2-c7g-instances-powered-by-aws-graviton3-processors/). PyTorch 2.0 improves inference performance on Graviton compared to the previous releases, including improvements for Resnet50 and Bert.
- New prototype features and technologies across TensorParallel, DTensor, 2D parallel, TorchDynamo, AOTAutograd, PrimTorch and TorchInductor.

|     |     |     |     |
| --- | --- | --- | --- |
| **Stable** | **Beta** | **Prototype** | **Platform Changes** |
| Accelerated PT 2 Transformers | torch.compile | DTensor | CUDA support for 11.7 & 11.8 (deprecating CUDA 11.6) |
|  | PyTorch MPS Backend | TensorParallel | Python 3.8 (deprecating Python 3.7) |
|  | Scaled dot product attention | 2D Parallel | AWS Graviton3 |
|  | Functorch | Torch.compile (dynamic=True) |  |
|  | Dispatchable Collectives |  |
|  | torch.set\_default\_device and torch.device as context manager |  |  |
|  | X86 quantization backend |  |  |
|  | GNN inference and training performance |  |  |

\*To see a full list of public 2.0, 1.13 and 1.12 feature submissions click [here](https://docs.google.com/spreadsheets/d/1H3jazwO8BBCwK8JwLNYspLiHfUrzshEtyqjL-X93I9g/edit#gid=790902532)

# Backwards Incompatible Changes

### **Drop support for Python versions <= 3.7 ( [\#93155](https://github.com/pytorch/pytorch/pull/93155))**

Previously the minimum supported version of Python for PyTorch was 3.7. This PR updates the minimum version to require 3.8 in order to install PyTorch. See [Hardware / Software Support](https://github.com/pytorch/pytorch/blob/893aa5df3f2a475c91ea8eadb1353812e52fb227/RELEASE.md#python) for more information.

### **Drop support for CUDA 10 ( [\#89582](https://github.com/pytorch/pytorch/pull/89582))**

This PR updates the minimum CUDA version to 11.0. See the [getting-started](https://pytorch.org/get-started/locally/) for installation or [building from source](https://github.com/pytorch/pytorch#from-source) for more information.

### **Gradients are now set to `None` instead of zeros by default in `torch.optim.*.zero_grad()` and `torch.nn.Module.zero_grad()` ( [\#92731](https://github.com/pytorch/pytorch/pull/92731))**

This changes the default behavior of `zero_grad()` to zero out the grads by setting them to `None` instead of zero tensors. In other words, the `set_to_none` kwarg is now `True` by default instead of `False`. Setting grads to `None` reduces peak memory usage and increases performance. This will break code that directly accesses data or does computation on the grads after calling `zero_grad()` as they will now be `None`. To revert to the old behavior, pass in `zero_grad(set_to_none=False)`.

| 1.13 | 2.0 |
| --- | --- |
| ```<br>>>> import torch<br>>>> from torch import nn<br>>>> module = nn.Linear(2,22)<br>>>> i = torch.randn(2, 2, requires_grad=True)<br>>>> module(i).sum().backward()<br>>>> module.zero_grad()<br>>>> module.weight.grad == None<br>False<br>>>> module.weight.grad.data<br>tensor([[0., 0.],<br>        [0., 0.]])<br>>>> module.weight.grad + 1.0<br>tensor([[1., 1.],<br>        [1., 1.]])<br>``` | ```<br>>>> import torch<br>>>> from torch import nn<br>>>> module = nn.Linear(5, 5)<br>>>> i = torch.randn(2, 5, requires_grad=True)<br>>>> module(i).sum().backward()<br>>>> module.zero_grad()<br>>>> module.weight.grad == None<br>True<br>>>> module.weight.grad.data<br>AttributeError: 'NoneType' object has no attribute 'data'<br>>>> module.weight.grad + 1.0<br>TypeError: unsupported operand type(s) for +:<br>'NoneType' and 'float'<br>``` |

### **Update `torch.tensor` and `nn.Parameter` to serialize all their attributes ( [\#88913](https://github.com/pytorch/pytorch/pull/88913))**

Any attribute stored on `torch.tensor` and `torch.nn.Parameter` will now be serialized. This aligns the serialization behavior of `torch.nn.Parameter`, `torch.Tensor` and other tensor subclasses

| 1.13 | 2.0 |
| --- | --- |
| ```<br># torch.Tensor behavior<br>>>> a = torch.Tensor()<br>>>> a.foo = 'hey'<br>>>> buffer = io.BytesIO()<br>>>> torch.save(a, buffer)<br>>>> buffer.seek(0)<br>>>> b = torch.load(buffer)<br>>>> print(a.foo)<br>hey<br>>>> print(b.foo)<br>AttributeError: 'Tensor' object has no attribute 'foo'<br># torch.nn.Parameter behavior<br>>>> a = nn.Parameter()<br>>>> a.foo = 'hey'<br>>>> buffer = io.BytesIO()<br>>>> torch.save(a, buffer)<br>>>> buffer.seek(0)<br>>>> b = torch.load(buffer)<br>>>> print(a.foo)<br>hey<br>>>> print(b.foo)<br>AttributeError: 'Parameter' object has no attribute 'foo'<br># torch.Tensor subclass behavior<br>>>> class MyTensor(torch.Tensor):<br>...   pass<br>>>> a = MyTensor()<br>>>> a.foo = 'hey'<br>>>> print(a.foo)<br>hey<br>>>> buffer = io.BytesIO()<br>>>> torch.save(a, buffer)<br>>>> buffer.seek(0)<br>>>> b = torch.load(buffer)<br>>>>print(b.foo)<br>hey<br>``` | ```<br># torch.Tensor behavior<br>a = torch.Tensor()<br>a.foo = 'hey'<br>>>> buffer = io.BytesIO()<br>>>> torch.save(a, buffer)<br>>>> buffer.seek(0)<br>>>> b = torch.load(buffer)<br>>>> print(a.foo)<br>hey<br>>>> print(b.foo)<br>hey<br># torch.nn.Parameter behavior<br>>>> a = nn.Parameter()<br>>>> a.foo = 'hey'<br>>>> buffer = io.BytesIO()<br>>>> torch.save(a, buffer)<br>>>> buffer.seek(0)<br>>>> b = torch.load(buffer)<br>>>> print(a.foo)<br>hey<br>>>> print(b.foo)<br>hey<br># torch.Tensor subclass behavior<br>>>> class MyTensor(torch.Tensor):<br>...   pass<br>>>> a = MyTensor()<br>>>> a.foo = 'hey'<br>>>> print(a.foo)<br>hey<br>>>> buffer = io.BytesIO()<br>>>> torch.save(a, buffer)<br>>>> buffer.seek(0)<br>>>> b = torch.load(buffer)<br>>>>print(b.foo)<br>hey<br>``` |

If you have an attribute that you don't want to be serialized you should not store it as an attribute on tensor or Parameter but instead it is recommended to use `torch.utils.weak.WeakTensorKeyDictionary`

```
>>> foo_dict = weak.WeakTensorKeyDictionary()
>>> foo_dict[a] = 'hey'
>>> print(foo_dict[a])
hey
```

### **Algorithms `{Adadelta, Adagrad, Adam, Adamax, AdamW, ASGD, NAdam, RAdam, RMSProp, RProp, SGD}` default to faster `foreach` implementation when on CUDA + differentiable= `False`**

When applicable, this changes the default behavior of `step()` and anything that ca...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v2.0.0)

### Contributors

- [![@pytorch](https://avatars.githubusercontent.com/u/21003710?s=64&v=4)](https://github.com/pytorch)

pytorch


Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=2).

![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)56nps1ngh, cyyever, Enigmatisms, fredbjer, lazyoracle, james77777778, Looong01, zhiqwang, abhi-glitchhg, Zerohertz, and 46 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)1jwrh reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)118duanzhiihao, WoosukKwon, kashif, jisaacso, johnnv1, aikow, Syntax3rror404, cossio, baurst, ashvardanian, and 108 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)40stephenroller, Djdefrag, kshitij12345, igreat, orionr, morelen17, ffelten, atalman, Ludougan123234, akihironitta, and 30 more reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)50weiji14, johnnv1, Cam-B04, aikow, cossio, chester-tan, stephenroller, igreat, kevalmorabia97, ilya16, and 40 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)13mhasan502, abhi-glitchhg, bckim92, RyanHir, semaphore-egg, kaparoo, aredden, jihaohaaaa, qqwqqw689, Zhiy-Zhang, and 3 more reacted with eyes emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)56 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)1 reaction
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)118 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)40 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)50 reactions
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)13 reactions

185 people reacted

## PyTorch 1.13.1 Release, small bug fix release

Dec 15, 2022
16 Dec 00:17


![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)

[v1.13.1](https://github.com/pytorch/pytorch/tree/v1.13.1)

[`49444c3`](https://github.com/pytorch/pytorch/commit/49444c3e546bf240bed24a101e747422d1f8a0ee)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.
The key has expired.


GPG key ID: 4AEE18F83AFDEB23

Expired

Verified
on Jan 16, 2024, 02:59 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 1.13.1 Release, small bug fix release](https://github.com/pytorch/pytorch/releases/tag/v1.13.1)

This release is meant to fix the following issues (regressions / silent correctness):

- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch\_first=True [#88669](https://github.com/pytorch/pytorch/issues/88669)
- Installation via pip on Amazon Linux 2, regression [#88869](https://github.com/pytorch/pytorch/issues/88869)
- Installation using poetry on Mac M1, failure [#88049](https://github.com/pytorch/pytorch/issues/88049)
- Missing masked tensor documentation [#89734](https://github.com/pytorch/pytorch/issues/89734)
- torch.jit.annotations.parse\_type\_line is not safe (command injection) [#88868](https://github.com/pytorch/pytorch/issues/88868)
- Use the Python frame safely in \_pythonCallstack [#88993](https://github.com/pytorch/pytorch/pull/88993)
- Double-backward with full\_backward\_hook causes RuntimeError [#88312](https://github.com/pytorch/pytorch/issues/88312)
- Fix logical error in get\_default\_qat\_qconfig [#88876](https://github.com/pytorch/pytorch/pull/88876)
- Fix cuda/cpu check on NoneType and unit test [#88854](https://github.com/pytorch/pytorch/pull/88854) and [#88970](https://github.com/pytorch/pytorch/pull/88970)
- Onnx ATen Fallback for BUILD\_CAFFE2=0 for ONNX-only ops [#88504](https://github.com/pytorch/pytorch/pull/88504)
- Onnx operator\_export\_type on the new registry [#87735](https://github.com/pytorch/pytorch/pull/87735)
- torchrun AttributeError caused by file\_based\_local\_timer on Windows [#85427](https://github.com/pytorch/pytorch/issues/85427)

The [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues



![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)8creativeautomaton, MinhuiWan, Mitradeep2, Whaleer, lin72h, tolgacangoz, scarecrow-my, and angelseblani reacted with thumbs up emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)31akihironitta, ahmadmustafaanis, speedcell4, niklas-rittmann, TYH71, JulesGM, vict0rsch, tscholak, henrry179, hitenkoku, and 21 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)7chrislemke, leecs0503, MinhuiWan, Whaleer, lin72h, tolgacangoz, and FelipeFTN reacted with heart emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)3k9sret, tolgacangoz, and ren-pin reacted with eyes emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)8 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)31 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)7 reactions
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)3 reactions

39 people reacted

[Previous](https://github.com/pytorch/pytorch/releases?page=1) [1](https://github.com/pytorch/pytorch/releases?page=1) _2_ [3](https://github.com/pytorch/pytorch/releases?page=3) [4](https://github.com/pytorch/pytorch/releases?page=4) [5](https://github.com/pytorch/pytorch/releases?page=5) [6](https://github.com/pytorch/pytorch/releases?page=6) [7](https://github.com/pytorch/pytorch/releases?page=7) [Next](https://github.com/pytorch/pytorch/releases?page=3)

[Previous](https://github.com/pytorch/pytorch/releases?page=1) [Next](https://github.com/pytorch/pytorch/releases?page=3)

## Footer

[GitHub Homepage](https://github.com/)
© 2025 GitHub, Inc.



=== FILE: github.com_pytorch_pytorch_releases_page=3.2025-10-28T12_57_24.101Z.md ===

[Skip to content](https://github.com/pytorch/pytorch/releases?page=3#start-of-content)

## Navigation Menu

Toggle navigation

[Homepage](https://github.com/)

[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Freleases%3Fpage%3D3)

Appearance settings

- Platform









- [GitHub Copilot\\
\\
\\
\\
Write better code with AI](https://github.com/features/copilot)
- [GitHub Spark\\
\\
\\
New\\
\\
\\
Build and deploy intelligent apps](https://github.com/features/spark)
- [GitHub Models\\
\\
\\
New\\
\\
\\
Manage and compare prompts](https://github.com/features/models)
- [GitHub Advanced Security\\
\\
\\
\\
Find and fix vulnerabilities](https://github.com/security/advanced-security)
- [Actions\\
\\
\\
\\
Automate any workflow](https://github.com/features/actions)

- [Codespaces\\
\\
\\
\\
Instant dev environments](https://github.com/features/codespaces)
- [Issues\\
\\
\\
\\
Plan and track work](https://github.com/features/issues)
- [Code Review\\
\\
\\
\\
Manage code changes](https://github.com/features/code-review)
- [Discussions\\
\\
\\
\\
Collaborate outside of code](https://github.com/features/discussions)
- [Code Search\\
\\
\\
\\
Find more, search less](https://github.com/features/code-search)

Explore

- [Why GitHub](https://github.com/why-github)
- [Documentation](https://docs.github.com/)
- [GitHub Skills](https://skills.github.com/)
- [Blog](https://github.blog/)

Integrations

- [GitHub Marketplace](https://github.com/marketplace)
- [MCP Registry](https://github.com/mcp)

[View all features](https://github.com/features)

- Solutions







By company size

- [Enterprises](https://github.com/enterprise)
- [Small and medium teams](https://github.com/team)
- [Startups](https://github.com/enterprise/startups)
- [Nonprofits](https://github.com/solutions/industry/nonprofits)

By use case

- [App Modernization](https://github.com/solutions/use-case/app-modernization)
- [DevSecOps](https://github.com/solutions/use-case/devsecops)
- [DevOps](https://github.com/solutions/use-case/devops)
- [CI/CD](https://github.com/solutions/use-case/ci-cd)
- [View all use cases](https://github.com/solutions/use-case)

By industry

- [Healthcare](https://github.com/solutions/industry/healthcare)
- [Financial services](https://github.com/solutions/industry/financial-services)
- [Manufacturing](https://github.com/solutions/industry/manufacturing)
- [Government](https://github.com/solutions/industry/government)
- [View all industries](https://github.com/solutions/industry)

[View all solutions](https://github.com/solutions)

- Resources







Topics

- [AI](https://github.com/resources/articles?topic=ai)
- [DevOps](https://github.com/resources/articles?topic=devops)
- [Security](https://github.com/resources/articles?topic=security)
- [Software Development](https://github.com/resources/articles?topic=software-development)
- [View all](https://github.com/resources/articles)

Explore

- [Learning Pathways](https://resources.github.com/learn/pathways)
- [Events & Webinars](https://github.com/resources/events)
- [Ebooks & Whitepapers](https://github.com/resources/whitepapers)
- [Customer Stories](https://github.com/customer-stories)
- [Partners](https://github.com/partners)
- [Executive Insights](https://github.com/solutions/executive-insights)

- Open Source









- [GitHub Sponsors\\
\\
\\
\\
Fund open source developers](https://github.com/sponsors)

- [The ReadME Project\\
\\
\\
\\
GitHub community articles](https://github.com/readme)

Repositories

- [Topics](https://github.com/topics)
- [Trending](https://github.com/trending)
- [Collections](https://github.com/collections)

- Enterprise









- [Enterprise platform\\
\\
\\
\\
AI-powered developer platform](https://github.com/enterprise)

Available add-ons

- [GitHub Advanced Security\\
\\
\\
\\
Enterprise-grade security features](https://github.com/security/advanced-security)
- [Copilot for business\\
\\
\\
\\
Enterprise-grade AI features](https://github.com/features/copilot/copilot-business)
- [Premium Support\\
\\
\\
\\
Enterprise-grade 24/7 support](https://github.com/premium-support)

- [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search


Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

Include my email address so I can be contacted

Cancel
Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).


Cancel
Create saved search

[Sign in](https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Freleases%3Fpage%3D3)

[Sign up](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Freleases%2Findex&source=header-repo&source_repo=pytorch%2Fpytorch)

Appearance settings

Resetting focus

You signed in with another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=3) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=3) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/pytorch/pytorch/releases?page=3) to refresh your session.Dismiss alert

{{ message }}

[pytorch](https://github.com/pytorch)/ **[pytorch](https://github.com/pytorch/pytorch)** Public

- Couldn't load subscription status.
Retry

- [Fork\\
25.7k](https://github.com/login?return_to=%2Fpytorch%2Fpytorch)
- [Star\\
94.3k](https://github.com/login?return_to=%2Fpytorch%2Fpytorch)


- [Code](https://github.com/pytorch/pytorch)
- [Issues5k+](https://github.com/pytorch/pytorch/issues)
- [Pull requests1.5k](https://github.com/pytorch/pytorch/pulls)
- [Actions](https://github.com/pytorch/pytorch/actions)
- [Projects12](https://github.com/pytorch/pytorch/projects)
- [Wiki](https://github.com/pytorch/pytorch/wiki)
- [Security4](https://github.com/pytorch/pytorch/security)






[**Uh oh!**](https://github.com/pytorch/pytorch/security)

[There was an error while loading.](https://github.com/pytorch/pytorch/security) [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).

- [Insights](https://github.com/pytorch/pytorch/pulse)

Additional navigation options

- [Code](https://github.com/pytorch/pytorch)
- [Issues](https://github.com/pytorch/pytorch/issues)
- [Pull requests](https://github.com/pytorch/pytorch/pulls)
- [Actions](https://github.com/pytorch/pytorch/actions)
- [Projects](https://github.com/pytorch/pytorch/projects)
- [Wiki](https://github.com/pytorch/pytorch/wiki)
- [Security](https://github.com/pytorch/pytorch/security)
- [Insights](https://github.com/pytorch/pytorch/pulse)

# Releases: pytorch/pytorch

[Releases](https://github.com/pytorch/pytorch/releases) [Tags](https://github.com/pytorch/pytorch/tags)

Releases · pytorch/pytorch

## PyTorch 1.13: beta versions of functorch and improved support for Apple’s new M1 chips are now available

Oct 28, 2022
28 Oct 16:54


![@mikaylagawarecki](https://avatars.githubusercontent.com/u/35276741?s=40&v=4)[mikaylagawarecki](https://github.com/mikaylagawarecki)

[v1.13.0](https://github.com/pytorch/pytorch/tree/v1.13.0)

[`7c98e70`](https://github.com/pytorch/pytorch/commit/7c98e70d44abc7a1aead68b6ea6c8adc8c554db5)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.
The key has expired.


GPG key ID: 4AEE18F83AFDEB23

Expired

Verified
on Jan 16, 2024, 02:59 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 1.13: beta versions of functorch and improved support for Apple’s new M1 chips are now available](https://github.com/pytorch/pytorch/releases/tag/v1.13.0)

# Pytorch 1.13 Release Notes

- Highlights
- Backwards Incompatible Changes
- New Features
- Improvements
- Performance
- Documentation
- Developers

# Highlights

We are excited to announce the release of PyTorch 1.13! This includes stable versions of BetterTransformer. We deprecated CUDA 10.2 and 11.3 and completed migration of CUDA 11.6 and 11.7. Beta includes improved support for Apple M1 chips and functorch, a library that offers composable vmap (vectorization) and autodiff transforms, being included in-tree with the PyTorch release. This release is composed of over 3,749 commits and 467 contributors since 1.12.1. We want to sincerely thank our dedicated community for your contributions.

Summary:

- The BetterTransformer feature set supports fastpath execution for common Transformer models during Inference out-of-the-box, without the need to modify the model. Additional improvements include accelerated add+matmul linear algebra kernels for sizes commonly used in Transformer models and Nested Tensors is now enabled by default.

- Timely deprecating older CUDA versions allows us to proceed with introducing the latest CUDA version as they are introduced by Nvidia®, and hence allows support for C++17 in PyTorch and new NVIDIA Open GPU Kernel Modules.

- Previously, functorch was released out-of-tree in a separate package. After installing PyTorch, a user will be able to `import functorch` and use functorch without needing to install another package.

- PyTorch is offering native builds for Apple® silicon machines that use Apple's new M1 chip as a beta feature, providing improved support across PyTorch's APIs.


| Stable | Beta | Prototype |
| --- | --- | --- |
| - Better Transformer<br>- CUDA 10.2 and 11.3 CI/CD Deprecation | - Enable Intel® VTune™ Profiler's Instrumentation and Tracing Technology APIs<br>- Extend NNC to support channels last and bf16<br>- Functorch now in PyTorch Core Library<br>- Beta Support for M1 devices | - Arm® Compute Library backend support for AWS Graviton<br>- CUDA Sanitizer |

You can check the blogpost that shows the new features [here](https://pytorch.org/blog/PyTorch-1.13-release/).

# Backwards Incompatible changes

## Python API

### **uint8 and all integer dtype masks are no longer allowed in Transformer** **( [\#87106](https://github.com/pytorch/pytorch/pull/87106))**

Prior to 1.13, `key_padding_mask` could be set to uint8 or other integer dtypes in `TransformerEncoder` and `MultiheadAttention`, which might generate unexpected results. In this release, these dtypes are not allowed for the mask anymore. Please convert them to `torch.bool` before using.

1.12.1

```
>>> layer = nn.TransformerEncoderLayer(2, 4, 2)
>>> encoder = nn.TransformerEncoder(layer, 2)
>>> pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.uint8)
>>> inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1)
# works before 1.13
>>> outputs = encoder(inputs, src_key_padding_mask=pad_mask)
```

1.13

```
>>> layer = nn.TransformerEncoderLayer(2, 4, 2)
>>> encoder = nn.TransformerEncoder(layer, 2)
>>> pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.bool)
>>> inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1)
>>> outputs = encoder(inputs, src_key_padding_mask=pad_mask)
```

### **Updated `torch.floor_divide` to perform floor division** **( [\#78411](https://github.com/pytorch/pytorch/pull/78411))**

Prior to 1.13, `torch.floor_divide` erroneously performed truncation division (i.e. truncated the quotients). In this release, it has been fixed to perform floor division. To replicate the old behavior, use `torch.div` with `rounding_mode='trunc'`.

1.12.1

```
>>> a = torch.tensor([4.0, -3.0])
>>> b = torch.tensor([2.0, 2.0])
>>> torch.floor_divide(a, b)
tensor([ 2., -1.])
```

1.13

```
>>> a = torch.tensor([4.0, -3.0])
>>> b = torch.tensor([2.0, 2.0])
>>> torch.floor_divide(a, b)
tensor([ 2., -2.])
# Old behavior can be replicated using torch.div with rounding_mode='trunc'
>>> torch.div(a, b, rounding_mode='trunc')
tensor([ 2., -1.])
```

### **Fixed `torch.index_select` on CPU to error that index is out of bounds when the `source` tensor is empty ( [\#77881](https://github.com/pytorch/pytorch/pull/77881))**

Prior to 1.13, `torch.index_select` would return an appropriately sized tensor filled with random values on CPU if the source tensor was empty. In this release, we have fixed this bug so that it errors out. A consequence of this is that `torch.nn.Embedding` which utilizes `index_select` will error out rather than returning an empty tensor when `embedding_dim=0` and `input` contains indices which are out of bounds. The old behavior cannot be reproduced with `torch.nn.Embedding`, however since an Embedding layer with `embedding_dim=0` is a corner case this behavior is unlikely to be relied upon.

1.12.1

```
>>> t = torch.tensor([4], dtype=torch.long)
>>> embedding = torch.nn.Embedding(3, 0)
>>> embedding(t)
tensor([], size=(1, 0), grad_fn=<EmbeddingBackward0>)
```

1.13

```
>>> t = torch.tensor([4], dtype=torch.long)
>>> embedding = torch.nn.Embedding(3, 0)
>>> embedding(t)
RuntimeError: INDICES element is out of DATA bounds, id=4 axis_dim=3
```

### Disallow overflows when tensors are constructed from scalars ( [\#82329](https://github.com/pytorch/pytorch/pull/82329))

Prior to this PR, overflows during tensor construction from scalars would not throw an error. In 1.13, such cases will error.

1.12.1

```
>>> torch.tensor(1000, dtype=torch.int8)
tensor(-24, dtype=torch.int8)
```

1.13

```
>>> torch.tensor(1000, dtype=torch.int8)
RuntimeError: value cannnot be converted to type int8 without overflow
```

### **Error on indexing a cpu tensor with non-cpu indices ( [\#69607](https://github.com/pytorch/pytorch/pull/69607))**

Prior to 1.13, `cpu_tensor[cuda_indices]` was a valid program that would return a cpu tensor. The original use case for mixed device indexing was for `non_cpu_tensor[cpu_indices]`, and allowing the opposite was unintentional ( `cpu_tensor[non_cpu_indices]`). This behavior appears to be rarely used, and a refactor of our indexing kernels made it difficult to represent an op that takes in (cpu\_tensor, non\_cpu\_tensor) and returns another cpu\_tensor, so it is now an error.

To replicate the old behavior for `base[indices]`, you can ensure that either `indices` lives on the CPU device, or `base` and `indices` both live on the same device.

1.12.1

```
>>> a = torch.tensor([1.0, 2.0, 3.0])
>>> b = torch.tensor([0, 2], device='cuda')
>>> a[b]
tensor([1., 3.])
```

1.13

```
>>> a = torch.tensor([1.0, 2.0, 3.0])
>>> b = torch.tensor([0, 2], device='cuda')
>>> a[b]
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)
# Old behavior can be replicated by moving b to CPU, or a to CUDA
>>> a[b.cpu()]
tensor([1., 3.])
>>> a.cuda()[b]
tensor([1., 3.], device='cuda:0')
```

### Remove deprecated `torch.eig`, ` torch.matrix_rank`, `torch.lstsq` ( [\#70982](https://github.com/pytorch/pytorch/pull/70982), [\#70981](https://github.com/pytorch/pytorch/pull/70981), [\#70980](https://github.com/pytorch/pytorch/pull/70980))

The deprecation cycle for the above functions has been completed and they have been removed in the 1.13 release.

## torch.nn

### Enforce that the `bias` has the same dtype as `input` and `weight` for convolutions on CPU ( [\#83686](https://github.com/pytorch/pytorch/pull/83686))

To align with the implementation on other devices, the CPU implementation for convolutions was updated to enforce that the `dtype` of the `bias` matches the `dtype` of the `input` and `weight`.

1.12.1

```
# input and weight are dtype torch.int64
# bias is torch.float32
>>> out = torch.nn.functional.conv2d(input, weight, bias, ...)
```

1.13

```
# input and weight are dtype torch.int64
# bias is torch.float32
>>> with assertRaisesError():
>>>    out = torch.nn.functional.conv2d(input, weight, bias, ...)

# Updated code to avoid the error
>>> out = torch.nn.functional.conv2d(input, weight, bias.to(input.dtype), ...)
```

## Autograd

### Disallow setting the `.data` of a tensor that `requires_grad=True` with an integer tensor ( [\#78436](https://github.com/pytorch/pytorch/pull/78436))

Setting the `.data` of a tensor that `requires_grad` with an integer tensor now raises an error.

1.12.1

```
>>> x = torch.randn(2, requires_grad=True)
>>> x.data = torch.randint(1, (2,))
>>> x
tensor([0, 0], requires_grad=True)
```

1.13

```
>>> x = torch.randn(2, requires_grad=True)
>>> x.data = torch.randint(1, (2,))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: data set to a tensor that requires gradients must be floating point or complex dtype
```

### Added variable\_list support to ExtractVariables struct ( [\#84583](https://github.com/pytorch/pytorch/pull/84583))

Prior to this change, C++ custom autograd Function considers tensors passed in TensorList to not be tensors for the purposes of recording the backward graph. After this change, custom Functions that receive TensorList must modify their backward functions to also compute gradients for these additional tensor inputs. Note that this behavior now differs from that of custom autograd Functions in Python.

1.12.1

```
struct MyFunction : public Function<MyFunction> {
    static Variable forward(AutogradContext* ctx, at::Tensor t, at::TensorList tensors) {
      return 2 * tensors[0] + 3 * t;
    }

    static variable_list backward(
        AutogradContext* ctx,
        variable_list grad_output) {
      return {3 * grad_output[0]};
    }
};
```

1.13

```
struct MyFunction : public Function<MyFunction> {
    static Variable forward(AutogradContext* ctx, at::Tensor t, at::TensorList tensors) {
      return 2 * tensors[0] + 3 * t;
    }

    static variable_list backward(
        AutogradContext* ctx,
        variable_list grad_output) {
      return {3 * grad_output[0], 2 * grad_output[0]};
    }
};
```

### Don't detach when making views; force kernel to detach ( [\#84893](https://github.com/pytorch/pytorch/pull/84893))

View operations registered as CompositeExplicitAutograd kernels are no longer allowed to return input tensors as-is. You must explic...

[Read more](https://github.com/pytorch/pytorch/releases/tag/v1.13.0)

### Contributors

- [![@hfwen0502](https://avatars.githubusercontent.com/u/14792895?s=64&v=4)](https://github.com/hfwen0502)
- [![@lchu6](https://avatars.githubusercontent.com/u/20955448?s=64&v=4)](https://github.com/lchu6)
- [![@beartype](https://avatars.githubusercontent.com/u/63089855?s=64&v=4)](https://github.com/beartype)

hfwen0502, lchu6, and beartype


Assets3

Loading

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).

yzhangcs, AH-dark, akihironitta, thanhpcc96, aymenkhs, khushi-411, lehoangHUST, samwaterbury, MengShen0709, Cardroid, and 25 more reacted with thumbs up emojiyzhangcs, akihironitta, and TheFanatr reacted with laugh emojiyzhangcs, akihironitta, khushi-411, JohT, zachcoleman, ericspod, sunway513, TheFanatr, Icy-Thought, innocentmunai, and 6 more reacted with hooray emojiyzhangcs, akihironitta, aymenkhs, khushi-411, zachcoleman, TheFanatr, dannis999, albanD, erickTornero, LaRiffle, and 2 more reacted with heart emojiyzhangcs, akihironitta, ilya16, rohitgr7, aymenkhs, samwaterbury, zachcoleman, ke1337, bencevans, antoinebrl, and 8 more reacted with rocket emojiyzhangcs, akihironitta, and tolgacangoz reacted with eyes emoji

All reactions

- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)35 reactions
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)3 reactions
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)16 reactions
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)12 reactions
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)18 reactions
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)3 reactions

58 people reacted

## PyTorch 1.12.1 Release, small bug fix release

Aug 5, 2022
05 Aug 19:35


![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)

[v1.12.1](https://github.com/pytorch/pytorch/tree/v1.12.1)

[`664058f`](https://github.com/pytorch/pytorch/commit/664058fa83f1d8eede5d66418abff6e20bd76ca8)

This commit was created on GitHub.com and signed with GitHub’s **verified signature**.
The key has expired.


GPG key ID: 4AEE18F83AFDEB23

Expired

Verified
on Jan 16, 2024, 02:59 PM

[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).


Compare

# Choose a tag to compare

## Sorry, something went wrong.

Filter

Loading

## Sorry, something went wrong.

### Uh oh!

There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).

## No results found

[View all tags](https://github.com/pytorch/pytorch/tags)

[PyTorch 1.12.1 Release, small bug fix release](https://github.com/pytorch/pytorch/releases/tag/v1.12.1)

This release is meant to fix the following issues (regressions / silent correctness):

## Optim

- Remove overly restrictive assert in adam [#80222](https://github.com/pytorch/pytorch/pull/80222)

## Autograd

- Convolution forward over reverse internal asserts in specific case [#81111](https://github.com/pytorch/pytorch/issues/81111)
- 25% Performance regression from v0.1.1 to 0.2.0 when calculating hessian [#82504](https://github.com/pytorch/pytorch/pull/82504)

## Distributed

- Fix distributed store to use add for the counter of DL shared seed [#80348](https://github.com/pytorch/pytorch/pull/80348)
- Raise proper timeout when sharing the distributed shared seed [#81666](https://github.com/pytorch/pytorch/pull/81666)

## NN

- Allow register float16 weight\_norm on cpu and speed up test [#80600](https://github.com/pytorch/pytorch/pull/80600)
- Fix weight norm backward bug on CPU when OMP\_NUM\_THREADS <= 2 [#80930](https://github.com/pytorch/pytorch/pull/80930)
- Weight\_norm is not working with float16 [#80599](https://github.com/pytorch/pytorch/issues/80599)
- New release breaks torch.nn.weight\_norm backwards pass and breaks all Wav2Vec2 implementations [#80569](https://github.com/pytorch/pytorch/issues/80569)
- Disable src mask for transformer and multiheadattention fastpath [#81277](https://github.com/pytorch/pytorch/pull/81277)
- Make nn.stateless correctly reset parameters if the forward pass fails [#81262](https://github.com/pytorch/pytorch/pull/81262)
- torchvision.transforms.functional.rgb\_to\_grayscale() + torch.nn.Conv2d() don\`t work on 1080 GPU [#81106](https://github.com/pytorch/pytorch/issues/81106)
- Transformer and CPU path with src\_mask raises error with torch 1.12 [#81129](https://github.com/pytorch/pytorch/issues/81129)

## Data Loader

- \[Locking lower ranks seed recepients https://github.com/ [/pull/81071](https://github.com/pytorch/pytorch/pull/81071)\
\
## CUDA\
\
- os.environ\["CUDA\_VISIBLE\_DEVICES"\] has no effect [#80876](https://github.com/pytorch/pytorch/issues/80876)\
- share\_memory() on CUDA tensors no longer no-ops and instead crashes [#80733](https://github.com/pytorch/pytorch/issues/80733)\
- \[Prims\] Unbreak CUDA lazy init [#80899](https://github.com/pytorch/pytorch/pull/80899)\
- PyTorch 1.12 cu113 wheels cudnn discoverability issue [#80637](https://github.com/pytorch/pytorch/issues/80637)\
- Remove overly restrictive checks for cudagraph [#80881](https://github.com/pytorch/pytorch/pull/80881)\
\
## ONNX\
\
- ONNX cherry picks [#82435](https://github.com/pytorch/pytorch/pull/82435)\
\
## MPS\
\
- MPS cherry picks [#80898](https://github.com/pytorch/pytorch/issues/80898)\
\
## Other\
\
- Don't error if \_warned\_capturable\_if\_run\_uncaptured not set [#80345](https://github.com/pytorch/pytorch/pull/80345)\
- Initializing libiomp5.dylib, but found libomp.dylib already initialized. [#78490](https://github.com/pytorch/pytorch/issues/78490)\
- Assertion error - \_dl\_shared\_seed\_recv\_cnt - pt 1.12 - multi node [#80845](https://github.com/pytorch/pytorch/issues/80845)\
- Add 3.10 stdlib to torch.package [#81261](https://github.com/pytorch/pytorch/pull/81261)\
- CPU-only c++ extension libraries (functorch, torchtext) built against PyTorch wheels are not fully compatible with PyTorch wheels [#80489](https://github.com/pytorch/pytorch/issues/80489)\
\
Assets3\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
robertocarlosmedina, gallegogt, AhmedAliStats, ecstayalive, TheFanatr, hoangtnm, mbilalai, Rishit-dagli, junjihashimoto, Red-Eyed, and 18 more reacted with thumbs up emojiRed-Eyed, AYUSHMIT, edwardnguyen1705, VladPetruMarius, lucadiliello, leecs0503, and tolgacangoz reacted with hooray emojiRed-Eyed, atalman, VladPetruMarius, pi-null-mezon, aymenkhs, raghukiran1224, nickjeliopoulos, leecs0503, and tolgacangoz reacted with heart emojiRed-Eyed, jsz4n, VladPetruMarius, nickjeliopoulos, and tolgacangoz reacted with eyes emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)28 reactions\
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)7 reactions\
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)9 reactions\
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)5 reactions\
\
36 people reacted\
\
## PyTorch 1.12: TorchArrow, Functional API for Modules and nvFuser, are now available\
\
Jun 28, 2022\
28 Jun 16:48\
\
\
![@soulitzer](https://avatars.githubusercontent.com/u/13428986?s=40&v=4)[soulitzer](https://github.com/soulitzer)\
\
[v1.12.0](https://github.com/pytorch/pytorch/tree/v1.12.0)\
\
[`67ece03`](https://github.com/pytorch/pytorch/commit/67ece03c8cd632cce9523cd96efde6f2d1cc8121)\
\
This commit was created on GitHub.com and signed with GitHub’s **verified signature**.\
The key has expired.\
\
\
GPG key ID: 4AEE18F83AFDEB23\
\
Expired\
\
Verified\
on Jan 16, 2024, 02:59 PM\
\
[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).\
\
\
Compare\
\
# Choose a tag to compare\
\
## Sorry, something went wrong.\
\
Filter\
\
Loading\
\
## Sorry, something went wrong.\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
## No results found\
\
[View all tags](https://github.com/pytorch/pytorch/tags)\
\
[PyTorch 1.12: TorchArrow, Functional API for Modules and nvFuser, are now available](https://github.com/pytorch/pytorch/releases/tag/v1.12.0)\
\
# PyTorch 1.12 Release Notes\
\
- Highlights\
- Backwards Incompatible Change\
- New Features\
- Improvements\
- Performance\
- Documentation\
\
# Highlights\
\
We are excited to announce the release of PyTorch 1.12! This release is composed of over 3124 commits, 433 contributors. Along with 1.12, we are releasing beta versions of AWS S3 Integration, PyTorch Vision Models on Channels Last on CPU, Empowering PyTorch on Intel® Xeon® Scalable processors with Bfloat16 and FSDP API. We want to sincerely thank our dedicated community for your contributions.\
\
Summary:\
\
- Functional Module API to functionally apply module computation with a given set of parameters\
- Complex32 and Complex Convolutions in PyTorch\
- DataPipes from TorchData fully backward compatible with DataLoader\
- Functorch with improved coverage for APIs\
- nvFuser a deep learning compiler for PyTorch\
- Changes to float32 matrix multiplication precision on Ampere and later CUDA hardware\
- TorchArrow, a new beta library for machine learning preprocessing over batch data\
\
# Backwards Incompatible changes\
\
## Python API\
\
**Updated type promotion for `torch.clamp`** ( [#77035](https://github.com/pytorch/pytorch/pull/77035))\
\
In 1.11, the ‘min’ and ‘max’ arguments in `torch.clamp` did not participate in type promotion, which made it inconsistent with `minimum` and `maximum` operations. In 1.12, the ‘min’ and ‘max’ arguments participate in type promotion.\
\
1.11\
\
```\
>>> import torch\
>>> a = torch.tensor([1., 2., 3., 4.], dtype=torch.float32)\
>>> b = torch.tensor([2., 2., 2., 2.], dtype=torch.float64)\
>>> c = torch.tensor([3., 3., 3., 3.], dtype=torch.float64)\
>>> torch.clamp(a, b, c).dtype\
torch.float32\
```\
\
1.12\
\
```\
>>> import torch\
>>> a = torch.tensor([1., 2., 3., 4.], dtype=torch.float32)\
>>> b = torch.tensor([2., 2., 2., 2.], dtype=torch.float64)\
>>> c = torch.tensor([3., 3., 3., 3.], dtype=torch.float64)\
>>> torch.clamp(a, b, c).dtype\
torch.float64\
```\
\
## Complex Numbers\
\
### Fix complex type promotion ( [\#77524](https://github.com/pytorch/pytorch/pull/77524))\
\
Updates the type promotion rule such that given a complex scalar and real tensor, the value type of real tensor is preserved\
\
1.11\
\
```\
>>> a = torch.randn((2, 2), dtype=torch.float)\
>>> b = torch.tensor(1, dtype=torch.cdouble)\
>>> (a + b).dtype\
torch.complex128\
```\
\
1.12\
\
```\
>>> a = torch.randn((2, 2), dtype=torch.float)\
>>> b = torch.tensor(1, dtype=torch.cdouble)\
>>> (a + b).dtype\
torch.complex64\
```\
\
## LinAlg\
\
### Disable TF32 for matmul by default and add high-level control of fp32 matmul precision ( [\#76509](https://github.com/pytorch/pytorch/pull/76509))\
\
PyTorch 1.12 makes the default math mode for fp32 matrix multiplications more precise and consistent across hardware. This may affect users on Ampere or later CUDA devices and TPUs. See the PyTorch [blog](https://dev-discuss.pytorch.org/t/pytorch-and-tensorfloat32/504) for more details.\
\
## Sparse\
\
### Use ScatterGatherKernel for scatter\_reduce (CPU-only) ( [\#74226](https://github.com/pytorch/pytorch/pull/74226), [\#74608](https://github.com/pytorch/pytorch/pull/74608))\
\
In 1.11.0, unlike `scatter` which takes a `reduce` kwarg or `scatter_add`, `scatter_reduce` was not an in-place function. That is, it did not allow the user to pass an output tensor which contains data that is reduced together with the scattered data. Instead, the scatter reduction took place on an output tensor initialized under the hood. Indices of the output that were not scattered to were filled with reduction inits (or 0 for options ‘amin’ and ‘amax’).\
\
In 1.12.0, `scatter_reduce` (which is in beta) is in-place to align with the API of the related existing functions `scatter`/ `scatter_add`. For this reason, the argument `input` in 1.11.0 has been renamed `src` in 1.12.0 and the new `self` argument now takes a destination tensor to be scattered onto. Since the destination tensor is no longer initialized under the hood, the `output_size` kwarg in 1.11.0 that allowed users to specify the size of the output at dimension `dim` has been removed. Further, in 1.12.0 we introduce an `include_self` kwarg which determines whether values in the `self` (destination) tensor are included in the reduction. Setting `include_self=True` could, for example, allow users to provide special reduction inits for the scatter\_reduction operation. Otherwise, if `include_self=False,` indices scattered to are treated as if they were filled with reduction inits.\
\
In the snippet below, we illustrate how the behavior of `scatter_reduce` in 1.11.0 can be achieved with the function released in 1.12.0.\
\
Example:\
\
```\
>>> src = torch.arange(6, dtype=torch.float).reshape(3, 2)\
>>> index = torch.tensor([[0, 2], [1, 1], [0, 0]])\
>>> dim = 1\
>>> output_size = 4\
>>> reduce = "prod"\
```\
\
1.11\
\
```\
>>> torch.scatter_reduce(src, dim, index, reduce, output_size=output_size)\
`tensor([[ 0., 1., 1., 1.],\
        [ 1., 6., 1., 1.],\
        [20., 1., 1., 1.]])`\
```\
\
1.12\
\
```\
>>> output_shape = list(src.shape)\
>>> output_shape[dim] = output_size\
# reduction init for prod is 1\
# filling the output with 1 is only necessary if the user wants to preserve the behavior in 1.11\
# where indices not scattered to are filled with reduction inits\
>>> output = src.new_empty(output_shape).fill_(1)\
>>> output.scatter_reduce_(dim, index, src, reduce)\
`tensor([[ 0., 1., 1., 1.],\
        [ 1., 6., 1., 1.],\
        [20., 1., 1., 1.]])`\
```\
\
## torch.nn\
\
### `nn.GroupNorm`: Report an error if `num_channels` is not divisible by `num_groups` ( [\#74293](https://github.com/pytorch/pytorch/pull/74293))\
\
Previously, `nn.GroupNorm` would error out during the forward pass if `num_channels` is not divisible by `num_groups`. Now, the error is thrown for this case during module construction instead.\
\
1.11\
\
```\
m = torch.nn.GroupNorm(3, 7)\
m(...)  # errors during forward pass\
```\
\
1.12\
\
```\
m = torch.nn.GroupNorm(3, 7)  # errors during construction\
```\
\
### `nn.Dropout2d`: Return to 1.10 behavior: perform 1D channel-wise dropout for 3D inputs\
\
In PyTorch 1.10 and older, passing a 3D input to `nn.Dropout2D` resulted in 1D channel-wise dropout behavior; i.e. such inputs were interpreted as having shape `(N, C, L)` with N = batch size and C = # channels and channel-wise dropout was performed along the second dimension.\
\
1.10\
\
```\
x = torch.randn(2, 3, 4)\
m = nn.Dropout2d(p=0.5)\
out = m(x)  # input is assumed to be shape (N, C, L); dropout along the second dim.\
```\
\
With the introduction of no-batch-dim input support in 1.11, 3D inputs were reinterpreted as having shape `(C, H, W)`; i.e. an input without a batch dimension, and dropout behavior was changed to drop along the first dimension. This was a silent breaking change.\
\
1.11\
\
```\
x = torch.randn(2, 3, 4)\
m = nn.Dropout2d(p=0.5)\
out = m(x)  # input is assumed to be shape (C, H, W); dropout along the first dim.\
```\
\
The breaking change in 1.11 resulted in a lack of support for 1D channel-wise dropout behavior, so `Dropout2d` in PyTorch 1.12 returns to 1.10 behavior with a warning to give some time to adapt before the no-batch-dim interpretation goes back into effect.\
\
1.12\
\
```\
x = torch.randn(2, 3, 4)\
m = nn.Dropout2d(p=0.5)\
out = m(x)  # input is assumed to be shape (N, C, L); dropout along the second dim.\
            # throws a warning suggesting nn.Dropout1d for 1D channel-wise dropout.\
```\
\
If you want 1D channel-wise dropout behavior, please switch to use of the newly-added `nn.Dropout1d` module instead of `nn.Dropout2d`. If you want no-batch-dim input behavior, please note that while this is not supported in 1.12, a future release will reinstate the interpretation of 3D inputs to `nn.Dropout2d` as those without a batch dimension.\
\
### **`F.cosine_similarity`: Improve numerical stability ( [\#31378](https://github.com/pytorch/pytorch/pull/31378))**\
\
Previously, we first compute the inner product, then normalize. After this change, we first normalize, then compute inner product. This should be more numerically stable because it avoids losing precision in inner product for inputs with large norms. Because of this change, outputs may be different in some cases.\
\
## Composability\
\
**Functions in torch.ops.aten.{foo} no longer accept `self` as a kwarg**\
\
`torch.ops.aten.{foo}` objects are now instances of `OpOverloadPacket` (instead of a function) that have their `__call__` method in Python, which means that you cannot pass `self` as a kwarg. You can pass it normally as a positional argument instead.\
\
1.11\
\
```\
>>> torch.ops.aten.sin(self=torch.ones(2))\
    tensor([0.8415, 0.8415])\
```\
\
1.12\
\
```\
# this now fails\
>>> torch.ops.aten.sin(self=torch.ones(2))\
Traceback (most recent call last):\
  File "<stdin>", line 1, in <module>\
TypeError: __call__() got multiple values for argument 'self'\
# this works\
>>> torch.ops.aten.sin(torch.ones(2))\
tensor([0.8415, 0.8415])\
```\
\
**torch\_dispatch now traces individual op overloads instead of op overload packets (** [**#72673**](https://github.com/pytorch/pytorch/pull/72673) **)**\
\
`torch.ops.aten.add` actually corresponds to a bundle of functions from C++, corresponding to all over the overloads of add operator (specifically, `add.Tensor`, `add.Scalar` and `add.out`). Now, `__torch_dispatch__` will directly take in an overload corresponding to a single aten function.\
\
1.11\
\
```\
class MyTensor(torch.Tensor):\
    ....\
    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\
        # Before, func refers to a "packet" of all overloads\
        # for a given operator, e.g. "add"\
        assert func == torch.ops.aten.add\
```\
\
1.12\
\
```\
class MyTensor(torch.Tensor):\
    ....\
    def __torch_dispatch__(cls, func, types, args=(), kwargs=No...\
```\
\
[Read more](https://github.com/pytorch/pytorch/releases/tag/v1.12.0)\
\
Assets3\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
frgfm, oke-aditya, iceychris, akihironitta, tchaton, forgi86, sanchitintel, jameswong3388, JackCaoG, Yura52, and 45 more reacted with thumbs up emojiWYGNG, ekolodin, J3698, zhiqwang, zhuyyx, and gmendozah reacted with laugh emojikhushi-411, Miezhiko, oke-aditya, iceychris, akihironitta, tchaton, samuelrince, sanchitintel, aidyai, jaggbow, and 20 more reacted with hooray emojikhushi-411, oke-aditya, iceychris, akihironitta, tchaton, sanchitintel, SigureMo, zhang0557kui, neonsecret, Guillem96, and 13 more reacted with heart emojikhushi-411, oke-aditya, iceychris, akihironitta, tchaton, ellisbrown, sanchitintel, lirundong, SigureMo, zhang0557kui, and 13 more reacted with rocket emojibjmeo8, WYGNG, zhiqwang, LiweiPeng, and tolgacangoz reacted with eyes emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)55 reactions\
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)6 reactions\
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)30 reactions\
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)23 reactions\
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)23 reactions\
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)5 reactions\
\
85 people reacted\
\
## PyTorch 1.11, TorchData, and functorch are now available\
\
Mar 10, 2022\
10 Mar 16:59\
\
\
![@bdhirsh](https://avatars.githubusercontent.com/u/16311747?s=40&v=4)[bdhirsh](https://github.com/bdhirsh)\
\
[v1.11.0](https://github.com/pytorch/pytorch/tree/v1.11.0)\
\
[`bc2c6ed`](https://github.com/pytorch/pytorch/commit/bc2c6edaf163b1a1330e37a6e34caf8c553e4755)\
\
This commit was created on GitHub.com and signed with GitHub’s **verified signature**.\
The key has expired.\
\
\
GPG key ID: 4AEE18F83AFDEB23\
\
Expired\
\
Verified\
on Nov 7, 2024, 10:56 AM\
\
[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).\
\
\
Compare\
\
# Choose a tag to compare\
\
## Sorry, something went wrong.\
\
Filter\
\
Loading\
\
## Sorry, something went wrong.\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
## No results found\
\
[View all tags](https://github.com/pytorch/pytorch/tags)\
\
[PyTorch 1.11, TorchData, and functorch are now available](https://github.com/pytorch/pytorch/releases/tag/v1.11.0)\
\
# PyTorch 1.11 Release Notes\
\
- Highlights\
- Backwards Incompatible Change\
- New Features\
- Improvements\
- Performance\
- Documentation\
\
# Highlights\
\
We are excited to announce the release of PyTorch 1.11. This release is composed of over 3,300 commits since 1.10, made by 434 contributors. Along with 1.11, we are releasing beta versions of TorchData and functorch. We want to sincerely thank our community for continuously improving PyTorch.\
\
- TorchData is a new library for common modular data loading primitives for easily constructing flexible and performant data pipelines. [_View it on GitHub_](https://github.com/pytorch/data).\
- functorch, a library that adds composable function transforms to PyTorch, is now available in beta. [_View it on GitHub_](https://github.com/pytorch/functorch).\
- Distributed Data Parallel (DDP) static graph optimizations available in stable.\
\
You can check the blogpost that shows the new features [here](https://pytorch.org/blog/pytorch-1.11-released/).\
\
# Backwards Incompatible changes\
\
## Python API\
\
### Fixed python `deepcopy` to correctly copy all attributes on `Tensor` objects ( [\#65584](https://github.com/pytorch/pytorch/pull/65584))\
\
This change ensures that the `deepcopy` operation on Tensor properly copies all the attributes (and not just the plain Tensor properties).\
\
| 1.10.2 | 1.11.0 |\
| --- | --- |\
| ```<br>a = torch.rand(2)<br>a.foo = 3<br>torch.save(a, "bar")<br>b = torch.load("bar")<br>print(b.foo)<br># Raise AttributeError: "Tensor" object has no attribute "foo"<br>      <br>``` | ```<br>a = torch.rand(2)<br>a.foo = 3<br>torch.save(a, "bar")<br>b = torch.load("bar")<br>print(b.foo)<br># 3<br>      <br>``` |\
\
### **`steps` argument is no longer optional in `torch.linspace` and `torch.logspace`**\
\
This argument used to default to 100 in PyTorch 1.10.2, but was deprecated (previously you would see a deprecation warning if you didn’t explicitly pass in `steps`). In PyTorch 1.11, it is not longer optional.\
\
| 1.10.2 | 1.11.0 |\
| --- | --- |\
| ```<br># Works, but raises a deprecation warning<br># Steps defaults to 100<br>a = torch.linspace(1, 10)<br># UserWarning: Not providing a value for linspace's steps is deprecated<br># and will throw a runtime error in a future release.<br># This warning will appear only once per process.<br># (Triggered internally at  ../aten/src/ATen/native/RangeFactories.cpp:19<br>      <br>``` | ```<br># In 1.11, you must specify steps<br>a = torch.linspace(1, 10, steps=100)<br>      <br>``` |\
\
### Remove `torch.hub.import_module` function that was mistakenly public ( [\#67990](https://github.com/pytorch/pytorch/pull/67990))\
\
This function is not intended for public use.\
\
If you have existing code that relies on it, you can find an equivalent function at `torch.hub._import_module`.\
\
## C++ API\
\
### **We’ve cleaned up many of the headers in the C++ frontend to only include the subset of `aten` operators that they actually used ( [\#68247](https://github.com/pytorch/pytorch/pull/68247), [\#68687](https://github.com/pytorch/pytorch/pull/68687), [\#68688](https://github.com/pytorch/pytorch/pull/68688), [\#68714](https://github.com/pytorch/pytorch/pull/68714), [\#68689](https://github.com/pytorch/pytorch/pull/68689), [\#68690](https://github.com/pytorch/pytorch/pull/68690), [\#68697](https://github.com/pytorch/pytorch/pull/68697), [\#68691](https://github.com/pytorch/pytorch/pull/68691), [\#68692](https://github.com/pytorch/pytorch/pull/68692), [\#68693](https://github.com/pytorch/pytorch/pull/68693), [\#69840](https://github.com/pytorch/pytorch/pull/69840))**\
\
When you `#include` a header from the C++ frontend, you can no longer assume that every `aten` operators are transitively included. You can work around this by directly adding `#include <ATen/ATen.h>` in your file, which will maintain the old behavior of including every `aten` operators.\
\
### **Custom implementation for `c10::List` and `c10::Dict` move constructors have been removed (** [**\#69370**](https://github.com/pytorch/pytorch/pull/69370) **)**\
\
The semantics have changed from "make the moved-from List/Dict empty" to "keep the moved-from List/Dict unchanged"\
\
| 1.10.2 | 1.11.0 |\
| --- | --- |\
| ```<br>c10::List list1({"3", "4"});<br>c10::List list2(std::move(list1));<br>std::cout << list1.size() // 0<br>      <br>``` | ```<br>c10::List list1({"3", "4"});<br>c10::List list2(std::move(list1)); // calls copy ctr<br>std::cout << list1.size() // 2<br>      <br>``` |\
\
## CUDA\
\
### **Removed `THCeilDiv` function and corresponding `THC/THCDeviceUtils.cuh` header ( [\#65472](https://github.com/pytorch/pytorch/pull/65472))**\
\
As part of cleaning up `TH` from the codebase, the `THCeilDiv` function has been removed. Instead, please use `at::ceil_div`, and include the corresponding `ATen/ceil_div.h` header\
\
### **Removed `THCudaCheck` (** [**\#66391**](https://github.com/pytorch/pytorch/pull/66391) **)**\
\
You can replace it with `C10_CUDA_CHECK`, which has been available since at least PyTorch 1.4, so just replacing is enough even if you support older versions\
\
### **Removed `THCudaMalloc()`, `THCudaFree()`, `THCThrustAllocator.cuh` (** [**\#65492**](https://github.com/pytorch/pytorch/pull/65492) **)**\
\
If your extension is using `THCThrustAllocator.cuh`, please replace it with `ATen/cuda/ThrustAllocator.h` and corresponding APIs (see examples in this PR).\
\
This PR also removes `THCudaMalloc/THCudaFree` calls. Please use `c10::cuda::CUDACachingAllocator::raw_alloc(size)/raw_delete(ptr)`, or, preferably, switch to `c10:cuda::CUDaCachingAllocator::allocate` which manages deallocation. Caching allocator APIs are available since PyTorch 1.2, so just replacing it is enough even if you support older versions of PyTorch.\
\
## Build\
\
### Stopped building shared library for AOT Compiler, `libaot_compiler.so` ( [\#66227](https://github.com/pytorch/pytorch/pull/66227))\
\
Building `aot_compiler.cpp` as a separate library is not necessary, as it’s already included in `libtorch.so`.\
\
You can update your build system to only dynamically link `libtorch.so`.\
\
## Mobile\
\
### Make `typing.Union` type unsupported for mobile builds ( [\#65556](https://github.com/pytorch/pytorch/pull/65556))\
\
`typing.Union` support was added for TorchScript in 1.10. It was removed specifically for mobile due to its lack of use and increase in binary size of PyTorch for Mobile builds.\
\
## Distributed\
\
### `torch.distributed.rpc`: Final Removal of ProcessGroup RPC backend ( [\#67363](https://github.com/pytorch/pytorch/pull/67363))\
\
ProcessGroup RPC backend is deprecated. In 1.10, it threw an error to help users update their code, and, in 1.11, it is removed completely.\
\
The backend type “PROCESS\_GROUP” is now deprecated, e.g.\
\
`torch.distributed.rpc.init_rpc("worker0", backend="PROCESS_GROUP", rank=0, world_size=1)`\
\
and should be replaced with:\
\
`torch.distributed.rpc.init_rpc("worker0", backend="TENSORPIPE", rank=0, world_size=1)`\
\
## Quantization\
\
### Disabled the support for `getitem` in FX Graph Mode Quantization ( [\#66647](https://github.com/pytorch/pytorch/pull/66647))\
\
`getitem` used to be quantized in `FX Graph Mode Quantization`, and it is no longer quantized. This won’t break any models but could result in a slight difference in numerics.\
\
| 1.10.2 | 1.11.0 |\
| --- | --- |\
| ```<br>from torch.ao.quantization.quantize_fx import convert_fx, prepare_fx<br>class M(torch.nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.linear = torch.nn.Linear(5, 5)<br>    def forward(self, x):<br>        x = self.linear(x)<br>        y = torch.stack([x], 0)<br>        return y[0]<br>m = M().eval()<br>m = prepare_fx(m, {"": torch.ao.quantization.default_qconfig})<br>m = convert_fx(m)<br>print(m)<br># prints<br># GraphModule(<br>#   (linear): QuantizedLinear(in_features=5, out_features=5,<br>#      scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)<br># )<br># def forward(self, x):<br>#     linear_input_scale_0 = self.linear_input_scale_0<br>#     linear_input_zero_point_0 = self.linear_input_zero_point_0<br>#     quantize_per_tensor = torch.quantize_per_tensor(x,<br>#         linear_input_scale_0, linear_input_zero_point_0, torch.quint8)<br>#     x = linear_input_scale_0 = linear_input_zero_point_0 = None<br>#     linear = self.linear(quantize_per_tensor)<br>#     quantize_per_tensor = None<br>#     stack = torch.stack([linear], 0);  linear = None<br>#     getitem = stack[0]; stack = None<br>#     dequantize_2 = getitem.dequantize();  getitem = None<br>#     return getitem<br>      <br>``` | ```<br>from torch.ao.quantization.quantize_fx import convert_fx, prepare_fx<br>class M(torch.nn.Module):<br>    def __init__(self):<br>        super().__init__()<br>        self.linear = torch.nn.Linear(5, 5)<br>    def forward(self, x):<br>        x = self.linear(x)<br>        y = torch.stack([x], 0)<br>        return y[0]<br>m = M().eval()<br>m = prepare_fx(m, {"": torch.ao.quantization.default_qconfig})<br>m = convert_fx(m)<br>print(m)<br># prints<br># GraphModule(<br>#   (linear): QuantizedLinear(in_features=5, out_features=5, scale=1.0,<br>                    zero_point=0, qscheme=torch.per_tensor_affine)<br># )<br># def forward(self, x):<br>#     linear_input_scale_0 = self.linear_input_scale_0<br>#     linear_input_zero_point_0 = self.linear_input_zero_point_0<br>#     quantize_per_tensor = tor...<br>``` |\
\
[Read more](https://github.com/pytorch/pytorch/releases/tag/v1.11.0)\
\
Assets3\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
dhruvbird, Dengda98, zhiqwang, voldemortX, strint, pzelasko, steib, ashim-mahara, wuwenjie1992, sadra-barikbin, and 22 more reacted with thumbs up emojizhiqwang reacted with laugh emojivfdev-5, akihironitta, semaphore-egg, mberr, zhiqwang, amorehead, ronghanghu, toshiks, cchadj, xin-hao-2025, and tolgacangoz reacted with hooray emojizhiqwang and tolgacangoz reacted with heart emojikrshrimali, akihironitta, cthoyt, sanchitintel, zhiqwang, m3at, Mathux, willsq, aheuillet, cchadj, and 3 more reacted with rocket emojigugarosa, odellus, cchadj, xin-hao-2025, zhiqwang, and tolgacangoz reacted with eyes emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)32 reactions\
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)1 reaction\
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)11 reactions\
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)2 reactions\
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)13 reactions\
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)6 reactions\
\
48 people reacted\
\
## PyTorch 1.10.2 Release, small bug fix release\
\
Jan 27, 2022\
27 Jan 21:51\
\
\
![@atalman](https://avatars.githubusercontent.com/u/7563158?s=40&v=4)[atalman](https://github.com/atalman)\
\
[v1.10.2](https://github.com/pytorch/pytorch/tree/v1.10.2)\
\
[`71f889c`](https://github.com/pytorch/pytorch/commit/71f889c7d265b9636b93ede9d651c0a9c4bee191)\
\
This commit was created on GitHub.com and signed with GitHub’s **verified signature**.\
The key has expired.\
\
\
GPG key ID: 4AEE18F83AFDEB23\
\
Expired\
\
Verified\
on Jan 16, 2024, 02:59 PM\
\
[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).\
\
\
Compare\
\
# Choose a tag to compare\
\
## Sorry, something went wrong.\
\
Filter\
\
Loading\
\
## Sorry, something went wrong.\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
## No results found\
\
[View all tags](https://github.com/pytorch/pytorch/tags)\
\
[PyTorch 1.10.2 Release, small bug fix release](https://github.com/pytorch/pytorch/releases/tag/v1.10.2)\
\
This release is meant to deploy additional fixes not included in 1.10.1 release:\
\
- fix pybind issue for get\_autocast\_cpu\_dtype and get\_autocast\_gpu\_dtype [#66396](https://github.com/pytorch/pytorch/pull/66396)\
- Remove fgrad\_input from slow\_conv2d [#64280](https://github.com/pytorch/pytorch/pull/64280)\
- fix formatting CIRCLE\_TAG when building docs [#67026](https://github.com/pytorch/pytorch/pull/67026)\
\
Assets3\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
seemethere, atalman, mzr1996, Chachay, Miezhiko, shagunsodhani, lucadiliello, durandtibo, eeezio, 0T34, and 6 more reacted with thumbs up emojiseemethere, atalman, shagunsodhani, durandtibo, 0T34, AB-MT, GuoZhiBin2014, edrozenberg, and tolgacangoz reacted with rocket emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)16 reactions\
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)9 reactions\
\
16 people reacted\
\
## PyTorch 1.10.1 Release, small bug fix release\
\
Dec 15, 2021\
15 Dec 22:27\
\
\
![@seemethere](https://avatars.githubusercontent.com/u/1700823?s=40&v=4)[seemethere](https://github.com/seemethere)\
\
[v1.10.1](https://github.com/pytorch/pytorch/tree/v1.10.1)\
\
[`302ee7b`](https://github.com/pytorch/pytorch/commit/302ee7bfb604ebef384602c56e3853efed262030)\
\
This commit was created on GitHub.com and signed with GitHub’s **verified signature**.\
The key has expired.\
\
\
GPG key ID: 4AEE18F83AFDEB23\
\
Expired\
\
Verified\
on Jan 16, 2024, 02:59 PM\
\
[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).\
\
\
Compare\
\
# Choose a tag to compare\
\
## Sorry, something went wrong.\
\
Filter\
\
Loading\
\
## Sorry, something went wrong.\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
## No results found\
\
[View all tags](https://github.com/pytorch/pytorch/tags)\
\
[PyTorch 1.10.1 Release, small bug fix release](https://github.com/pytorch/pytorch/releases/tag/v1.10.1)\
\
This release is meant to fix the following issues (regressions / silent correctness):\
\
- torch.nn.cross\_entropy silently incorrect in PyTorch 1.10 on CUDA on non-contiguous inputs [#67167](https://github.com/pytorch/pytorch/issues/67167)\
- channels\_last significantly degrades accuracy [#67239](https://github.com/pytorch/pytorch/issues/67239)\
- Potential strict aliasing rule violation in bitwise\_binary\_op (on ARM/NEON) [#66119](https://github.com/pytorch/pytorch/issues/66119)\
- torch.get\_autocast\_cpu\_dtype() returns a new dtype [#65786](https://github.com/pytorch/pytorch/issues/65786)\
- Conv2d grad bias gets wrong value for bfloat16 case [#68048](https://github.com/pytorch/pytorch/issues/68048)\
\
The [release tracker](https://github.com/pytorch/pytorch/issues/69100) should contain all relevant pull requests related to this release as well as links to related issues\
\
Assets3\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
rishikksh20, kikirizki, hoangtnm, weiweiWYW, enverfakhan, tolgacangoz, jxaflxb, and trip036 reacted with thumbs up emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)8 reactions\
\
8 people reacted\
\
## PyTorch 1.10 Release, including CUDA Graphs APIs, Frontend and compiler improvements\
\
Oct 21, 2021\
21 Oct 15:49\
\
\
![@albanD](https://avatars.githubusercontent.com/u/6359743?s=40&v=4)[albanD](https://github.com/albanD)\
\
[v1.10.0](https://github.com/pytorch/pytorch/tree/v1.10.0)\
\
[`36449ea`](https://github.com/pytorch/pytorch/commit/36449ea93134574c2a22b87baad3de0bf8d64d42)\
\
This commit was created on GitHub.com and signed with GitHub’s **verified signature**.\
The key has expired.\
\
\
GPG key ID: 4AEE18F83AFDEB23\
\
Expired\
\
Verified\
on Jan 16, 2024, 02:59 PM\
\
[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).\
\
\
Compare\
\
# Choose a tag to compare\
\
## Sorry, something went wrong.\
\
Filter\
\
Loading\
\
## Sorry, something went wrong.\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
## No results found\
\
[View all tags](https://github.com/pytorch/pytorch/tags)\
\
[PyTorch 1.10 Release, including CUDA Graphs APIs, Frontend and compiler improvements](https://github.com/pytorch/pytorch/releases/tag/v1.10.0)\
\
# 1.10.0 Release Notes\
\
- Highlights\
- Backwards Incompatible Change\
- New Features\
- Improvements\
- Performance\
- Documentation\
\
# Highlights\
\
We are excited to announce the release of PyTorch 1.10. This release is composed of over 3,400 commits since 1.9, made by 426 contributors. We want to sincerely thank our community for continuously improving PyTorch.\
\
PyTorch 1.10 updates are focused on improving training and performance of PyTorch, and developer usability. Highlights include:\
\
- CUDA Graphs APIs are integrated to reduce CPU overheads for CUDA workloads.\
- Several frontend APIs such as FX, `torch.special`, and `nn.Module` Parametrization, have moved from beta to stable.\
- Support for automatic fusion in JIT Compiler expands to CPUs in addition to GPUs.\
- Android NNAPI support is now available in beta.\
\
You can check the blogpost that shows the new features [here](https://pytorch.org/blog/pytorch-1.10-released/).\
\
# Backwards Incompatible changes\
\
## Python API\
\
### `torch.any`/ `torch.all` behavior changed slightly to be more consistent for zero-dimension, `uint8` tensors. ( [\#64642](https://github.com/pytorch/pytorch/pull/64642))\
\
These two functions match the behavior of NumPy, returning an output dtype of bool for all support dtypes, except for `uint8` (in which case they return a 1 or a 0, but with `uint8` dtype). In some cases with 0-dim tensor inputs, the returned `uint8` value could mistakenly take on a value > 1. This has now been fixed.\
\
| 1.9.1 | 1.10.0 |\
| --- | --- |\
| ```<br>>>> torch.all(torch.tensor(42, dtype=torch.uint8))<br>tensor(1, dtype=torch.uint8)<br>>>> torch.all(torch.tensor(42, dtype=torch.uint8), dim=0)<br>tensor(42, dtype=torch.uint8) # wrong, old behavior<br>      <br>``` | ```<br>>>> torch.all(torch.tensor(42, dtype=torch.uint8))<br>tensor(1, dtype=torch.uint8)<br>>>> torch.all(torch.tensor(42, dtype=torch.uint8), dim=0)<br>tensor(1, dtype=torch.uint8) # new, corrected and consistent behavior<br>      <br>``` |\
\
### Remove deprecated `torch.{is,set}_deterministic` ( [\#62158](https://github.com/pytorch/pytorch/pull/62158))\
\
This is the end of the deprecation cycle for both of these functions. You should be using `torch.use_deterministic_algorithms` and `torch.are_deterministic_algorithms_enabled` instead.\
\
## Complex Numbers\
\
### **Conjugate View: [`tensor.conj()`](https://pytorch.org/docs/1.10./generated/torch.conj.html) now returns a view tensor that aliases the same memory and has conjugate bit set ( [\#54987](https://github.com/pytorch/pytorch/pull/54987), [\#60522](https://github.com/pytorch/pytorch/pull/60522), [\#66082](https://github.com/pytorch/pytorch/pull/66082), [\#63602](https://github.com/pytorch/pytorch/pull/63602)).**\
\
This means that `.conj()` is now an O(1) operation and returns a tensor that views the same memory as `tensor` and has conjugate bit set. This notion of conjugate bit enables fusion of operations with conjugation which gives a lot of performance benefit for operations like matrix multiplication. All out-of-place operations will have the same behavior as before, but an in-place operation on a conjugated tensor will additionally modify the input tensor.\
\
| 1.9.1 | 1.10.0 |\
| --- | --- |\
| ```<br>>>> import torch<br>>>> x = torch.tensor([1+2j])<br>>>> y = x.conj()<br>>>> y.add_(2)<br>>>> print(x)<br>tensor([1.+2.j])<br>      <br>``` | ```<br>>>> import torch<br>>>> x = torch.tensor([1+2j])<br>>>> y = x.conj()<br>>>> y.add_(2)<br>>>> print(x)<br>tensor([3.+2.j])<br>      <br>``` |\
\
Note: You can verify if the conj bit is set by calling `tensor.is_conj()`. The conjugation can be resolved, i.e., you can obtain a new tensor that doesn’t share storage with the input tensor at any time by calling `conjugated_tensor.clone()` or `conjugated_tensor.resolve_conj()` .\
\
Note that these conjugated tensors behave differently from the corresponding numpy arrays obtained from `np.conj()` when an in-place operation is performed on them (similar to the example shown above).\
\
### **Negative View: `tensor.conj().neg()` returns a view tensor that aliases the same memory as both tensor and `tensor.conj()` and has a negative bit set ( [\#56058](https://github.com/pytorch/pytorch/pull/56058)).**\
\
`conjugated_tensor.neg()` continues to be an O(1) operation, but the returned tensor shares memory with both `tensor` and `conjugated_tensor`.\
\
| 1.9.1 | 1.10.0 |\
| --- | --- |\
| ```<br>>>> x = torch.tensor([1+2j])<br>>>> y = x.conj()<br>>>> z = y.imag<br>>>> z.add_(2)<br>>>> print(x)<br>tensor([1.+2.j])<br>      <br>``` | ```<br>>>> x = torch.tensor([1+2j])<br>>>> y = x.conj()<br>>>> z = y.imag<br>>>> print(z.is_neg())<br>True<br>>>> z.add_(2)<br>>>> print(x)<br>tensor([1.-0.j])<br>      <br>``` |\
\
### `tensor.numpy()` now throws `RuntimeError` when called on a tensor with conjugate or negative bit set ( [\#61925](https://github.com/pytorch/pytorch/pull/61925)).\
\
Because the notion of conjugate bit and negative bit doesn’t exist outside of PyTorch, calling operations that return a Python object viewing the same memory as input like `.numpy()` would no longer work for tensors with conjugate or negative bit set.\
\
| 1.9.1 | 1.10.0 |\
| --- | --- |\
| ```<br>>>> x = torch.tensor([1+2j])<br>>>> y = x.conj().imag<br>>>> print(y.numpy())<br>[2.]<br>      <br>``` | ```<br>>>> x = torch.tensor([1+2j])<br>>>> y = x.conj().imag<br>>>> print(y.numpy())<br>RuntimeError: Can't call numpy() on Tensor that has negative<br>bit set. Use tensor.resolve_neg().numpy() instead.<br>      <br>``` |\
\
## Autograd\
\
### Raise `TypeError` instead of `RuntimeError` when assigning to a Tensor’s grad field with wrong type ( [\#64876](https://github.com/pytorch/pytorch/pull/64876))\
\
Setting the `.grad` field with a non-None and non-Tensor object used to return a `RuntimeError` but it now properly returns a `TypeError`. If your code was catching this error, you should simply update it to catch a `TypeError` instead of a `RuntimeError`.\
\
| 1.9.1 | 1.10.0 |\
| --- | --- |\
| ```<br>try:<br>    # Assigning an int to a Tensor's grad field<br>    a.grad = 0<br>except RuntimeError as e:<br>    pass<br>      <br>``` | ```<br>try:<br>   a.grad = 0<br>except TypeError as e:<br>    pass<br>      <br>``` |\
\
### Raise error when inputs to `autograd.grad` are empty ( [\#52016](https://github.com/pytorch/pytorch/pull/52016))\
\
Calling `autograd.grad` with an empty list of inputs used to do the same as backward. To reduce confusion, it now raises the expected error. If you were relying on this, you can simply update your code as follows:\
\
| 1.9.1 | 1.10.0 |\
| --- | --- |\
| ```<br>grad = autograd.grad(out, tuple())<br>assert grad == tuple()<br>      <br>``` | ```<br>out.backward()<br>      <br>``` |\
\
### Optional arguments to `autograd.gradcheck` and `autograd.gradgradcheck` are now kwarg-only ( [\#65290](https://github.com/pytorch/pytorch/pull/65290))\
\
These two functions now have a significant number of optional arguments controlling what they do (i.e., `eps`, `atol`, `rtol`, `raise_exception`, etc.). To improve readability, we made these arguments kwarg-only. If you are passing these arguments to `autograd.gradcheck` or `autograd.gradgradcheck` as positional arguments, you can update your code as follows:\
\
| 1.9.1 | 1.10.0 |\
| --- | --- |\
| ```<br>torch.autograd.gradcheck(fn, x, 1e-6)<br>      <br>``` | ```<br>torch.autograd.gradcheck(fn, x, eps=1e-6)<br>      <br>``` |\
\
### In-place detach ( `detach_`) now errors for views that return multiple outputs ( [\#58285](https://github.com/pytorch/pytorch/pull/58285))\
\
This change is finishing the deprecation cycle for the inplace-over-view logic. In particular, a few things that were warning are updated:\
\
```\
* `detach_` will now raise an error when invoked on any view created by `split`, `split_with_sizes`, or `chunk`. You should use the non-inplace `detach` instead.\
* The error message for when an in-place operation (that is not detach) is performed on a view created by `split`, `split_with_size`, and `chunk` has been changed from "This view is an output of a function..." to "This view is the output of a function...".\
\
```\
\
| 1.9.1 | 1.10.0 |\
| --- | --- |\
| ```<br>b = a.split(1)[0]<br>b.detach_()<br>      <br>``` | ```<br>b = a.split(1)[0]<br>c = b.detach()<br>      <br>``` |\
\
### Fix saved variable unpacking version counter ( [\#60195](https://github.com/pytorch/pytorch/pull/60195))\
\
In-place on the unpacked SavedVariables used to be ignored. They are now properly detected which can lead to errors saying that a variable needed for backward was modified in-place.\
\
This is a valid error and the ...\
\
[Read more](https://github.com/pytorch/pytorch/releases/tag/v1.10.0)\
\
Assets3\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
yzhangcs, zasdfgbnm, oke-aditya, wolegechu, zhiqwang, edgarriba, Kyeongpil, seemethere, krshrimali, EdisonLeeeee, and 40 more reacted with thumbs up emojiyzhangcs, zasdfgbnm, oke-aditya, zhiqwang, edgarriba, Kyeongpil, arcesoftware, Varal7, iomrla, mfaccin, and 3 more reacted with laugh emojiyzhangcs, zhiqwang, edgarriba, PingchuanMa, dfalbel, Kyeongpil, tmke8, Nikronic, alexeygolyshev, WangDeyao, and 8 more reacted with hooray emojiyzhangcs, zasdfgbnm, oke-aditya, gorodnitskiy, zhiqwang, edgarriba, AdilZouitine, Kyeongpil, FranklinAurelio, alexander-soare, and 17 more reacted with heart emojiaraffin, yzhangcs, zasdfgbnm, zhiqwang, edgarriba, dfalbel, Kyeongpil, anjali411, Nikronic, alexeygolyshev, and 9 more reacted with rocket emojiyzhangcs, zasdfgbnm, zhiqwang, edgarriba, krshrimali, SomeoneSerge, arcesoftware, Varal7, iomrla, mfaccin, and tolgacangoz reacted with eyes emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)50 reactions\
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)13 reactions\
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)18 reactions\
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)27 reactions\
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)19 reactions\
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)11 reactions\
\
73 people reacted\
\
## Small bug fix release\
\
Sep 22, 2021\
22 Sep 12:58\
\
\
![@malfet](https://avatars.githubusercontent.com/u/2453524?s=40&v=4)[malfet](https://github.com/malfet)\
\
[v1.9.1](https://github.com/pytorch/pytorch/tree/v1.9.1)\
\
[`dfbd030`](https://github.com/pytorch/pytorch/commit/dfbd030854359207cb3040b864614affeace11ce)\
\
This commit was created on GitHub.com and signed with GitHub’s **verified signature**.\
The key has expired.\
\
\
GPG key ID: 4AEE18F83AFDEB23\
\
Expired\
\
Verified\
on Nov 7, 2024, 11:05 AM\
\
[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).\
\
\
Compare\
\
# Choose a tag to compare\
\
## Sorry, something went wrong.\
\
Filter\
\
Loading\
\
## Sorry, something went wrong.\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
## No results found\
\
[View all tags](https://github.com/pytorch/pytorch/tags)\
\
[Small bug fix release](https://github.com/pytorch/pytorch/releases/tag/v1.9.1)\
\
# PyTorch 1.9.1 Release Notes\
\
- Improvements\
- Bug Fixes\
- Documentation\
\
# Improvements\
\
- Stop warning on `.names()` access in `max_pool2d` [#60059](https://github.com/pytorch/pytorch/pull/60059)\
- Remove Caffe2 thread-pool leak warning [#60318](https://github.com/pytorch/pytorch/pull/60318)\
- Add option to skip GitHub tag validation for `torch.hub.load` [#62139](https://github.com/pytorch/pytorch/pull/62139)\
- Use `log.warning` in `torch.distributed.run` to print OMP\_NUM\_THREADS warning [#63953](https://github.com/pytorch/pytorch/pull/63953)\
- TorchElastic: Pretty print the failure message captured by [@record](https://github.com/record) [#64036](https://github.com/pytorch/pytorch/pull/64036)\
- `torch.distribtued.run` to set `nproc_per_node` to 1 by default [#61552](https://github.com/pytorch/pytorch/pull/61552)\
- Remove experimental API warning from `torch.distributed.elastic.utils.store` [#60807](https://github.com/pytorch/pytorch/pull/60807)\
- Deprecate `use_env` in `torch.distributed.run` [#59409](https://github.com/pytorch/pytorch/pull/59409)\
- Better engineering changes for torch.distributed launcher [#59152](https://github.com/pytorch/pytorch/pull/59152)\
\
# Bug fixes\
\
## Distributed / TorchElastic\
\
- Make init\_method=tcp:// compatible with `torch.distributed.run` [#63910](https://github.com/pytorch/pytorch/pull/63910)\
- Fix default parameters (number of restarts, log level, number of processes per node) that regressed with the transition from `torch.distributed.launch` and `torch.distributed.run` and clarify the documentation accordingly [#61294](https://github.com/pytorch/pytorch/pull/61294)\
\
## Hub\
\
- Fix HTTP/403 error when calling `torch.hub.load` for TorchVision models [#62072](https://github.com/pytorch/pytorch/pull/62072)\
\
## Misc\
\
- `torch.mm` to check input matrix sizes shapes [#61394](https://github.com/pytorch/pytorch/pull/61394)\
\
# Documentation\
\
- Fix broken link in elastic launch doc [#62378](https://github.com/pytorch/pytorch/pull/62378)\
- Fix typo in `torch.distribtued.run` warning message [#61127](https://github.com/pytorch/pytorch/pull/61127)\
\
### Contributors\
\
- [![@record](https://avatars.githubusercontent.com/u/468913?s=64&v=4)](https://github.com/record)\
\
record\
\
\
Assets2\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)17pit-ray, semaphore-egg, moskomule, Supermaxman, snakers4, alexeygolyshev, jasperzhong, sudoflex, voldemortX, Rishit-dagli, and 7 more reacted with thumbs up emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)3www516717402, tolgacangoz, and ryumh99 reacted with eyes emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)17 reactions\
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)3 reactions\
\
19 people reacted\
\
## LTS 1.8.2, Wrap cub in its own namespace\
\
Aug 17, 2021\
17 Aug 18:33\
\
\
![@seemethere](https://avatars.githubusercontent.com/u/1700823?s=40&v=4)[seemethere](https://github.com/seemethere)\
\
[v1.8.2](https://github.com/pytorch/pytorch/tree/v1.8.2)\
\
[`e0495a7`](https://github.com/pytorch/pytorch/commit/e0495a7aa104471d95dc85a1b8f6473fbcc427a8)\
\
This commit was created on GitHub.com and signed with GitHub’s **verified signature**.\
The key has expired.\
\
\
GPG key ID: 4AEE18F83AFDEB23\
\
Expired\
\
Verified\
on Nov 6, 2024, 05:34 PM\
\
[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).\
\
\
Compare\
\
# Choose a tag to compare\
\
## Sorry, something went wrong.\
\
Filter\
\
Loading\
\
## Sorry, something went wrong.\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
## No results found\
\
[View all tags](https://github.com/pytorch/pytorch/tags)\
\
[LTS 1.8.2, Wrap cub in its own namespace](https://github.com/pytorch/pytorch/releases/tag/v1.8.2)\
\
# **PyTorch 1.8.2 Release Notes**\
\
- Highlights\
- Bug Fixes\
\
# Highlights\
\
We are excited to announce the release of PyTorch 1.8.2. This is the first release we are making as part of the [Pytorch Enterprise Support Program](https://pytorch.org/enterprise-support-program). This release includes a bug fix requested by a customer in an LTS branch.\
\
We'd like to thank Microsoft for their support and work on this release.\
\
# Bug Fixes\
\
- Wrap cub in its own namespace ( [#55292](https://github.com/pytorch/pytorch/pull/55292)) ( [#61605](https://github.com/pytorch/pytorch/pull/61605))\
\
Assets2\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)7hoangtnm, Dangzheng, zhouzhuojie, toandaominh1997, xiaoyu-work, victorgabr, and tolgacangoz reacted with thumbs up emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)4hoangtnm, kami93, chrisdahms-embark, and tolgacangoz reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)2hoangtnm and tolgacangoz reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)3hoangtnm, victorgabr, and tolgacangoz reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)3RuleNHao, chrisdahms-embark, and tolgacangoz reacted with eyes emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)7 reactions\
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)4 reactions\
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)2 reactions\
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)3 reactions\
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)3 reactions\
\
10 people reacted\
\
## PyTorch 1.9 Release, including Torch.Linalg and Mobile Interpreter\
\
Jun 15, 2021\
15 Jun 16:06\
\
\
![@anjali411](https://avatars.githubusercontent.com/u/20081078?s=40&v=4)[anjali411](https://github.com/anjali411)\
\
[v1.9.0](https://github.com/pytorch/pytorch/tree/v1.9.0)\
\
[`d69c22d`](https://github.com/pytorch/pytorch/commit/d69c22dd61a2f006dcfe1e3ea8468a3ecaf931aa)\
\
This commit was created on GitHub.com and signed with GitHub’s **verified signature**.\
The key has expired.\
\
\
GPG key ID: 4AEE18F83AFDEB23\
\
Expired\
\
Verified\
on Nov 5, 2024, 09:28 AM\
\
[Learn about vigilant mode](https://docs.github.com/github/authenticating-to-github/displaying-verification-statuses-for-all-of-your-commits).\
\
\
Compare\
\
# Choose a tag to compare\
\
## Sorry, something went wrong.\
\
Filter\
\
Loading\
\
## Sorry, something went wrong.\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
## No results found\
\
[View all tags](https://github.com/pytorch/pytorch/tags)\
\
[PyTorch 1.9 Release, including Torch.Linalg and Mobile Interpreter](https://github.com/pytorch/pytorch/releases/tag/v1.9.0)\
\
# **PyTorch 1.9 Release Notes**\
\
- Highlights\
- Backwards Incompatible Change\
- Deprecations\
- New Features\
- Improvements\
- Bug Fixes\
- Performance\
- Documentation\
\
# Highlights\
\
We are excited to announce the release of PyTorch 1.9. The release is composed of more than 3,400 commits since 1.8, made by 398 contributors. Highlights include:\
\
- Major improvements to support scientific computing, including torch.linalg, torch.special, and Complex Autograd\
- Major improvements in on-device binary size with Mobile Interpreter\
- Native support for elastic-fault tolerance training through the upstreaming of TorchElastic into PyTorch Core\
- Major updates to the PyTorch RPC framework to support large scale distributed training with GPU support\
- New APIs to optimize performance and packaging for model inference deployment\
- Support for Distributed training, GPU utilization and SM efficiency in the PyTorch Profiler\
\
We’d like to thank the community for their support and work on this latest release. We’d especially like to thank Quansight and Microsoft for their contributions.\
\
You can find more details on all the highlighted features in the [_PyTorch 1.9 Release blogpost_](https://pytorch.org/blog/pytorch-1.9-released/).\
\
# Backwards Incompatible changes\
\
## Python API\
\
- **`torch.divide` with `rounding_mode='floor'` now returns infinity when a non-zero number is divided by zero (** [**#56893**](https://github.com/pytorch/pytorch/pull/56893) **).**\
\
\
This fixes the `rounding_mode='floor'` behavior to return the same non-finite values as other rounding modes when there is a division by zero. Previously it would always result in a NaN value, but a non-zero number divided by zero should return +/- infinity in IEEE floating point arithmetic. Note this does not effect `torch.floor_divide` or the floor division operator, which currently use `rounding_mode='trunc'` (and are also deprecated for that reason).\
\
| 1.8.1 | 1.9.0 |\
| --- | --- |\
| ```<br>>>> a = torch.tensor([-1.0, 0.0, 1.0])<br>>>> b = torch.tensor([0.0])<br>>>> torch.divide(a, b, rounding_mode='floor')<br>tensor([nan, nan, nan])<br>      <br>``` | ```<br>>>> a = torch.tensor([-1.0, 0.0, 1.0])<br>>>> b = torch.tensor([0.0])<br>>>> torch.divide(a, b, rounding_mode='floor')<br>tensor([-inf, nan, inf])<br>      <br>``` |\
\
- **Legacy tensor constructors and `Tensor.new` no longer support passing both `Tensor` and `device` as inputs ( [#58108](https://github.com/pytorch/pytorch/pull/58108)).**\
\
\
This fixes a bug in which 1-element integer tensors were misinterpreted as specifying tensor size, yielding an uninitialized tensor. As noted in the error message, use the new-style `torch.tensor(...)` or `torch.as_tensor(...)` to copy or alias an existing tensor. If you want to create an uninitialized tensor, use `torch.empty(...)`.\
\
| 1.8.1 | 1.9.0 |\
| --- | --- |\
| ```<br>>>> a = torch.tensor([1])<br>>>> torch.LongTensor(a, device='cpu') # uninitialized<br>tensor([7022349217739848992])<br>>>> a.new(a, device='cpu')<br>tensor([4294967295]) # uninitialized<br>      <br>``` | ```<br>>>> a = torch.tensor([1])<br>>>> torch.LongTensor(a, device='cpu')<br>RuntimeError: Legacy tensor constructor of the form torch.Tensor(tensor, device=device) is<br>not supported. Use torch.tensor(...) or torch.as_tensor(...) instead.<br>>>> a.new(a, device='cpu')<br>RuntimeError: Legacy tensor new of the form tensor.new(tensor, device=device) is not<br>supported. Use torch.as_tensor(...) instead.<br>      <br>``` |\
\
- **`torch.divide` with `rounding_mode='true'` is replaced with `rounding_mode=None` ( [#51988](https://github.com/pytorch/pytorch/pull/51988)).**\
\
`torch.divide`'s undocumented `rounding_mode='true'` option has been removed, and instead `rounding_mode=None` should be passed to indicate no rounding should take place. This is equivalent to omitting the argument entirely.\
\
| 1.8.1 | 1.9.0 |\
| --- | --- |\
| ```<br>>>> a, b = torch.full((2,), 4.2), torch.full((2,), 2)<br>>>> torch.divide(a, b, rounding_mode='true')<br>tensor([2.1000, 2.1000])<br>      <br>``` | ```<br>>>> a, b = torch.full((2,), 4.2), torch.full((2,), 2)<br>>>> torch.divide(a, b, rounding_mode=None) # equivalent to  torch.divide(a, b, rounding_mode='true') from the prior release<br>tensor([2.1000, 2.1000])<br>      <br>``` |\
\
- **`import torch.tensor as tensor` is no longer supported ( [#53424](https://github.com/pytorch/pytorch/pull/53424)).**\
\
\
Instead, use `from torch import tensor`\
\
| 1.8.1 | 1.9.0 |\
| --- | --- |\
| ```<br>>>> import torch.tensor as tensor<br>>>> torch.tensor(1.)<br>tensor(1.)<br>      <br>``` | ```<br>>>> import torch.tensor as tensor<br>ModuleNotFoundError: No module named 'torch.tensor'<br>>>> from torch import tensor<br>>>> tensor(1.)<br>tensor(1.)<br>      <br>``` |\
\
- **binary release: `numpy` is no longer a required dependency**\
\
\
If you require `numpy` (and don't already have it installed) you will need to install it separately.\
\
## Autograd\
\
- **`torch.autograd.gradcheck.get_numerical_jacobian` and `torch.autograd.gradcheck.get_analytical_jacobian` no longer support functions that return complex valued output as well as any other values of `grad_out` not equal to 1** ( [#55692](https://github.com/pytorch/pytorch/pull/55692)).\
\
\
This change is a part of a refactor of `gradcheck`’s internals. Note that `gradcheck` itself still supports functions with complex output. This new restriction only applies to calls to the two internal helper functions. As a workaround, you can wrap your functions to return either the real or imaginary component of its output before calling these functions. Additionally these internal helpers no longer accept any other value except 1 for `grad_out` for any input function. Note that these helper functions are also being deprecated in this release.\
\
1.8.1:\
\
```\
get_numerical_jacobian(torch.complex, (a, b), grad_out=2.0)\
```\
\
1.9.0:\
\
```\
      def wrapped(fn):\
            def wrapper(*input):\
                return torch.real(fn(*input))\
            return wrapper\
\
        get_numerical_jacobian(wrapped(torch.complex), (a, b), grad_out=1.0)\
```\
\
- **`torch.autograd.gradcheck` now throws `GradcheckError`** ( [#55656](https://github.com/pytorch/pytorch/pull/55656)).\
\
\
This change is a part of a refactor of `gradcheck`’s internals. All errors that are able to be silenced by `raise_exception=False` now raise `GradcheckError` (which inherits from `RuntimeError`). If you explicitly check that the type of the error is `RuntimeError` you'll need to update your code to check for `GradcheckError` instead. Otherwise if you use something like `except` or `isinstance`, no changes are necessary.\
\
1.8.1:\
\
```\
# An example of a situation that will now return GradcheckError instead of\
# RuntimeError is when there is a jacobian mismatch, which can happen\
# for example when you forget to specify float64 for your inputs.\
try:\
    torch.autograd.gradcheck(torch.sin, (torch.ones(1, requires_grad=True),))\
except RuntimeError as e:\
    assert type(e) is RuntimeError # explicitly check type -> NEEDS UPDATE\
```\
\
1.9.0:\
\
```\
try:\
    torch.autograd.gradcheck(torch.sin, (torch.ones(1, requires_grad=True),)\
except RuntimeError as e:\
   # GradcheckError inherits from RuntimeError so you can still catch this\
   # with RuntimeError (No change necessary!)\
\
   # BUT, if you explicitly check type...\
   assert type(e) is torch.autograd.GradcheckError\
```\
\
- **Finished deprecation cycle for in-place view error checks** ( [#56093](https://github.com/pytorch/pytorch/pull/56093)).\
\
\
In-place modification of views will now raise an error if that view was created by a custom function or a function that returns multiple views, or if the view was created in no-grad mode. Modifying in-place a view created in the situations above are error-prone and have been deprecated since v1.5.0. Doing these in-place modifications are now forbidden. For more information on how to work around this, see the related sections the release notes linked below:\
\
  - [v1.5.0](https://github.com/pytorch/pytorch/releases?after=v1.5.1) (view created in custom autograd function, view created in no-grad block)\
  - [v1.7.0](https://github.com/pytorch/pytorch/releases?after=v1.8.0-rc3) (section on `split` and `chunk`, i.e., functions that return multiple views).\
\
## torch.nn\
\
- **Fixed regression for `nn.MultiheadAttention` to now apply bias flag to both in and out projection layers** ( [#52537](https://github.com/pytorch/pytorch/pull/52537)).\
\
\
In PyTorch 1.6, a regression was introduced that caused the `bias` flag of `nn.MultiheadAttention` only to apply to the input projection layer. This caused the output projection layer to always include a `bias` parameter, even with `bias=False` specified. The regression is now fixed in PyTorch 1.9, making the `bias` flag correctly apply to both the input and output projection layers. This fix is BC-breaking for the `bias=False` case as it will now result in no `bias` parameter for the output projection layer.\
\
| v1.6 - v1.8.1: | pre 1.6 & 1.9.0 |\
| --- | --- |\
| ```<br>>>> mha = torch.nn.MultiheadAttenti...<br>``` |\
\
[Read more](https://github.com/pytorch/pytorch/releases/tag/v1.9.0)\
\
Assets2\
\
Loading\
\
### Uh oh!\
\
There was an error while loading. [Please reload this page](https://github.com/pytorch/pytorch/releases?page=3).\
\
![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)31titaneric, imaginary-person, aavbsouza, scaomath, szmigacz, erogol, krshrimali, isgursoy, toandaominh1997, lamhoangtung, and 21 more reacted with thumbs up emoji![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)3jordan-carson, tolgacangoz, and lastthyme reacted with laugh emoji![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)18erogol, krshrimali, lamhoangtung, gemfield, ashim-mahara, Adnios, voldemortX, alexeygolyshev, Xreki, hoangtnm, and 8 more reacted with hooray emoji![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)7Issam28, hoangtnm, Nikronic, satishjasthi, jordan-carson, tolgacangoz, and lastthyme reacted with heart emoji![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)20imaginary-person, aavbsouza, tirthasheshpatel, erogol, krshrimali, imirzadeh, lamhoangtung, gemfield, ashim-mahara, Adnios, and 10 more reacted with rocket emoji![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)4zemu-unile, jordan-carson, tolgacangoz, and lastthyme reacted with eyes emoji\
\
All reactions\
\
- ![+1](https://github.githubassets.com/assets/1f44d-41cb66fe1e22.png)31 reactions\
- ![smile](https://github.githubassets.com/assets/1f604-7528822fb4c5.png)3 reactions\
- ![tada](https://github.githubassets.com/assets/1f389-36899a2cb781.png)18 reactions\
- ![heart](https://github.githubassets.com/assets/2764-982dc91ea48a.png)7 reactions\
- ![rocket](https://github.githubassets.com/assets/1f680-d0ef47fdb515.png)20 reactions\
- ![eyes](https://github.githubassets.com/assets/1f440-ee44e91e92a7.png)4 reactions\
\
43 people reacted\
\
[Previous](https://github.com/pytorch/pytorch/releases?page=2) [1](https://github.com/pytorch/pytorch/releases?page=1) [2](https://github.com/pytorch/pytorch/releases?page=2) _3_ [4](https://github.com/pytorch/pytorch/releases?page=4) [5](https://github.com/pytorch/pytorch/releases?page=5) [6](https://github.com/pytorch/pytorch/releases?page=6) [7](https://github.com/pytorch/pytorch/releases?page=7) [Next](https://github.com/pytorch/pytorch/releases?page=4)\
\
[Previous](https://github.com/pytorch/pytorch/releases?page=2) [Next](https://github.com/pytorch/pytorch/releases?page=4)\
\

