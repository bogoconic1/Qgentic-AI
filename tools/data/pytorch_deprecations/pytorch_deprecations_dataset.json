[
  {
    "deprecated_api": "torch.onnx.dynamo_export",
    "context": "in torch.onnx",
    "version": "2.9.0",
    "reason": "The function was removed; use torch.onnx.export instead.",
    "replacement": "torch.onnx.export",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.nn as nn\nimport torch.onnx\n\nclass SimpleModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = SimpleModel()\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.dynamo_export(model, dummy_input, 'simple_model.onnx')",
    "corrected_code": "import torch\nimport torch.nn as nn\nimport torch.onnx\n\nclass SimpleModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = SimpleModel()\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(model, dummy_input, 'simple_model.onnx')",
    "change_description": "The deprecated torch.onnx.dynamo_export function was replaced with torch.onnx.export, which is the supported way to export a model to ONNX format in PyTorch 2.9.0 and later."
  },
  {
    "deprecated_api": "torch.onnx.enable_fake_mode",
    "context": "in torch.onnx",
    "version": "2.9.0",
    "reason": "The function was removed; dynamo=True mode uses FakeTensors by default.",
    "replacement": "No replacement needed; FakeTensor is used automatically.",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.onnx\n\nmodel = torch.nn.Linear(10, 5)\nexample_input = torch.randn(1, 10)\n\ntorch.onnx.enable_fake_mode()\ntorch.onnx.export(model, example_input, 'model.onnx', dynamo=True)",
    "corrected_code": "import torch\nimport torch.onnx\n\nmodel = torch.nn.Linear(10, 5)\nexample_input = torch.randn(1, 10)\n\n# No need to call enable_fake_mode; FakeTensor is used automatically with dynamo=True\ntorch.onnx.export(model, example_input, 'model.onnx', dynamo=True)",
    "change_description": "The call to torch.onnx.enable_fake_mode() was removed because it is no longer needed or available. When using torch.onnx.export with dynamo=True, FakeTensor is used automatically."
  },
  {
    "deprecated_api": "torch.onnx.symbolic_caffe2",
    "context": "in torch.onnx",
    "version": "2.9.0",
    "reason": "Support for caffe2 in the ONNX exporter has ended and is removed.",
    "replacement": "No replacement; caffe2 support is removed.",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.onnx\n\n# Example model\ndef simple_model(x):\n    return x * 2\n\n# Using the deprecated symbolic_caffe2 API\ntorch.onnx.symbolic_caffe2.register_ops_in_version(9)\n",
    "corrected_code": "import torch\nimport torch.onnx\n\n# Example model\ndef simple_model(x):\n    return x * 2\n\n# caffe2 support has been removed; symbolic_caffe2 is no longer available.\n# There is no replacement for torch.onnx.symbolic_caffe2.\n# Remove any usage of symbolic_caffe2-related APIs.\n",
    "change_description": "The torch.onnx.symbolic_caffe2 API has been removed as caffe2 support is no longer available in the ONNX exporter. Any code using torch.onnx.symbolic_caffe2 should be deleted, as there is no replacement."
  },
  {
    "deprecated_api": "draft_export",
    "context": "as argument or function in torch.export",
    "version": "2.9.0",
    "reason": "Removed implicit draft tracing from the default exporter path.",
    "replacement": "Set TORCH_ONNX_ENABLE_DRAFT_EXPORT=True to trigger draft export if needed.",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.nn as nn\n\ndef foo(x):\n    return x + 1\n\n# Deprecated usage: using draft_export argument\nexported = torch.export.export(foo, (torch.randn(2, 3),), draft_export=True)\n",
    "corrected_code": "import torch\nimport torch.nn as nn\nimport os\n\ndef foo(x):\n    return x + 1\n\n# Corrected usage: set environment variable to enable draft export\nos.environ[\"TORCH_ONNX_ENABLE_DRAFT_EXPORT\"] = \"True\"\nexported = torch.export.export(foo, (torch.randn(2, 3),))\n",
    "change_description": "The 'draft_export' argument to torch.export.export has been removed. To enable draft export, set the environment variable 'TORCH_ONNX_ENABLE_DRAFT_EXPORT=True' before calling the export function."
  },
  {
    "deprecated_api": "allow_complex_guards_as_runtime_asserts",
    "context": "as argument to export",
    "version": "2.9.0",
    "reason": "Merged into prefer_deferred_runtime_asserts_over_guards.",
    "replacement": "prefer_deferred_runtime_asserts_over_guards",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "import torch\nfrom torch.export import export\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x * 2\n\nm = MyModule()\nexample_input = (torch.randn(2, 3),)\n# Deprecated usage:\ngm = export(m, example_input, allow_complex_guards_as_runtime_asserts=True)\n",
    "corrected_code": "import torch\nfrom torch.export import export\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x * 2\n\nm = MyModule()\nexample_input = (torch.randn(2, 3),)\n# Corrected usage:\ngm = export(m, example_input, prefer_deferred_runtime_asserts_over_guards=True)\n",
    "change_description": "The argument 'allow_complex_guards_as_runtime_asserts' to torch.export.export is deprecated and has been replaced by 'prefer_deferred_runtime_asserts_over_guards'. Update your code to use the new parameter name."
  },
  {
    "deprecated_api": "torch.onnx.verification",
    "context": "in torch.onnx",
    "version": "2.9.0",
    "reason": "Deprecated members in torch.onnx.verification are removed.",
    "replacement": "No replacement.",
    "category": "import",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.onnx.verification\n\n# Suppose we have an ONNX model file\nonnx_model_path = 'model.onnx'\n\n# Use the deprecated verification API to check the model\nresult = torch.onnx.verification.verify(onnx_model_path)\nprint('Verification result:', result)",
    "corrected_code": "import torch\n\n# The torch.onnx.verification module has been removed and there is no replacement.\n# You may need to use external ONNX tools for model verification, such as onnx.checker.\n\n# Example using onnx (external package):\n# import onnx\n# onnx_model = onnx.load('model.onnx')\n# onnx.checker.check_model(onnx_model)\n# print('ONNX model is valid.')",
    "change_description": "The torch.onnx.verification module and its members have been removed in PyTorch 2.9.0 with no direct replacement. Any code importing or using torch.onnx.verification must be removed or replaced with external ONNX tools (such as onnx.checker) for model verification."
  },
  {
    "deprecated_api": "torch.onnx.symbolic_opsets",
    "context": "in torch.onnx",
    "version": "2.9.0",
    "reason": "Previously private torch.onnx.symbolic_opsets* functions will no longer be accessible.",
    "replacement": "Copy the source code if needed.",
    "category": "import",
    "severity": "error",
    "deprecated_code": "import torch\nfrom torch.onnx import symbolic_opsets\n\ndef custom_add(g, self, other):\n    return symbolic_opsets.opset9.add(g, self, other)\n\n# Example usage in a custom export\n# ...",
    "corrected_code": "import torch\n# symbolic_opsets is no longer accessible from torch.onnx\n# If you need the implementation, copy it from the PyTorch source code:\n# https://github.com/pytorch/pytorch/tree/main/torch/onnx/symbolic_opsets\n\n# Example: Copy the relevant function directly into your codebase\n\ndef custom_add(g, self, other):\n    # Paste the implementation of 'add' from symbolic_opsets/opset9.py here\n    # For illustration, a placeholder:\n    return g.op(\"Add\", self, other)\n\n# Example usage in a custom export\n# ...",
    "change_description": "The import of 'symbolic_opsets' from 'torch.onnx' is no longer allowed as of PyTorch 2.9.0. To use symbolic ops, you must copy the relevant source code from the PyTorch repository into your own project instead of importing 'torch.onnx.symbolic_opsets'."
  },
  {
    "deprecated_api": "dynamo_export",
    "context": "in torch.onnx",
    "version": "2.7.0",
    "reason": "torch.onnx.dynamo_export is deprecated in favor of torch.onnx.export with dynamo=True.",
    "replacement": "torch.onnx.export with dynamo=True",
    "category": "function",
    "severity": "warning",
    "deprecated_code": "import torch\nimport torch.nn as nn\nimport torch.onnx\n\nclass MyModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = MyModel()\nexample_input = torch.randn(1, 3, 224, 224)\n\ntorch.onnx.dynamo_export(model, example_input, 'model.onnx')",
    "corrected_code": "import torch\nimport torch.nn as nn\nimport torch.onnx\n\nclass MyModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = MyModel()\nexample_input = torch.randn(1, 3, 224, 224)\n\ntorch.onnx.export(model, example_input, 'model.onnx', dynamo=True)",
    "change_description": "The deprecated torch.onnx.dynamo_export function is replaced by torch.onnx.export with the argument dynamo=True. This change ensures future compatibility and leverages the updated ONNX export API."
  },
  {
    "deprecated_api": "XNNPACKQuantizer",
    "context": "in torch.ao.quantization.quantizer.xnnpack_quantizer",
    "version": "2.7.0",
    "reason": "XNNPACKQuantizer is moved to ExecuTorch.",
    "replacement": "executorch.backends.xnnpack.quantizer.xnnpack_quantizer.XNNPACKQuantizer",
    "category": "class",
    "severity": "warning",
    "deprecated_code": "from torch.ao.quantization.quantizer.xnnpack_quantizer import XNNPACKQuantizer\n\n# Assume 'model' is a pre-defined PyTorch model\ndef quantize_model(model):\n    quantizer = XNNPACKQuantizer()\n    quantizer.prepare(model)\n    quantizer.convert(model)\n    return model\n",
    "corrected_code": "from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import XNNPACKQuantizer\n\n# Assume 'model' is a pre-defined PyTorch model\ndef quantize_model(model):\n    quantizer = XNNPACKQuantizer()\n    quantizer.prepare(model)\n    quantizer.convert(model)\n    return model\n",
    "change_description": "The import path for XNNPACKQuantizer has changed. Instead of importing from torch.ao.quantization.quantizer.xnnpack_quantizer, you should now import XNNPACKQuantizer from executorch.backends.xnnpack.quantizer.xnnpack_quantizer. The usage of the class remains the same."
  },
  {
    "deprecated_api": "print_lr",
    "context": "in LRScheduler",
    "version": "2.7.0",
    "reason": "print_lr method is removed; use get_last_lr instead.",
    "replacement": "get_last_lr",
    "category": "method",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.optim as optim\n\nmodel = torch.nn.Linear(10, 2)\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n\nfor epoch in range(3):\n    # Training code here\n    scheduler.step()\n    scheduler.print_lr()  # Deprecated usage",
    "corrected_code": "import torch\nimport torch.optim as optim\n\nmodel = torch.nn.Linear(10, 2)\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n\nfor epoch in range(3):\n    # Training code here\n    scheduler.step()\n    print(scheduler.get_last_lr())  # Correct replacement",
    "change_description": "The deprecated print_lr() method was removed from LRScheduler in PyTorch 2.7.0. Instead of calling scheduler.print_lr(), use scheduler.get_last_lr() to retrieve the current learning rates and print them."
  },
  {
    "deprecated_api": "verbose",
    "context": "as parameter to LRScheduler constructors",
    "version": "2.7.0",
    "reason": "verbose kwarg is removed; use get_last_lr for logging.",
    "replacement": "get_last_lr",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "",
    "corrected_code": "",
    "change_description": ""
  },
  {
    "deprecated_api": "capture_pre_autograd_graph",
    "context": "in torch._export",
    "version": "2.7.0",
    "reason": "capture_pre_autograd_graph was a temporary API; use export instead.",
    "replacement": "export",
    "category": "function",
    "severity": "warning",
    "deprecated_code": "import torch\nfrom torch import nn\nfrom torch._export import capture_pre_autograd_graph\n\ndef f(x):\n    return x * 2\n\nm = nn.Linear(4, 4)\nexample_input = (torch.randn(2, 4),)\n# Deprecated usage\nexported = capture_pre_autograd_graph(m, example_input)\n",
    "corrected_code": "import torch\nfrom torch import nn\nfrom torch._export import export\n\ndef f(x):\n    return x * 2\n\nm = nn.Linear(4, 4)\nexample_input = (torch.randn(2, 4),)\n# Correct usage\nexported = export(m, example_input)\n",
    "change_description": "The deprecated function 'capture_pre_autograd_graph' from torch._export was replaced with 'export'. The new 'export' API should be used to export models instead of the temporary 'capture_pre_autograd_graph' function."
  },
  {
    "deprecated_api": "get_control_flow_submodules",
    "context": "in torch.ao.quantization.pt2e.graph_utils",
    "version": "2.7.0",
    "reason": "get_control_flow_submodules is no longer public; use _get_control_flow_submodules for private access.",
    "replacement": "_get_control_flow_submodules",
    "category": "import",
    "severity": "error",
    "deprecated_code": "from torch.ao.quantization.pt2e.graph_utils import get_control_flow_submodules\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seq = nn.Sequential(nn.Linear(10, 10), nn.ReLU())\n\nmodel = MyModel()\n# Deprecated usage\nsubmodules = get_control_flow_submodules(model)\nprint(submodules)",
    "corrected_code": "from torch.ao.quantization.pt2e.graph_utils import _get_control_flow_submodules\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seq = nn.Sequential(nn.Linear(10, 10), nn.ReLU())\n\nmodel = MyModel()\n# Corrected usage\nsubmodules = _get_control_flow_submodules(model)\nprint(submodules)",
    "change_description": "The import and usage of get_control_flow_submodules (now deprecated) was replaced with _get_control_flow_submodules, which is now the correct (private) API to use."
  },
  {
    "deprecated_api": "export_for_inference",
    "context": "in torch.export",
    "version": "2.8.0",
    "reason": "export_for_inference has been removed in favor of export_for_training().run_decompositions().",
    "replacement": "export_for_training().run_decompositions()",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\nfrom torch import nn\nfrom torch.export import export_for_inference\n\nclass MyModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = MyModel()\nexample_input = (torch.randn(1, 3, 224, 224),)\nexported = export_for_inference(model, example_input)\n",
    "corrected_code": "import torch\nfrom torch import nn\nfrom torch.export import export_for_training\n\nclass MyModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = MyModel()\nexample_input = (torch.randn(1, 3, 224, 224),)\nexported = export_for_training(model, example_input).run_decompositions()\n",
    "change_description": "The deprecated 'export_for_inference' function has been removed. The correct approach is to use 'export_for_training(model, example_input).run_decompositions()' instead. This change involves replacing the import and function call to use the new API."
  },
  {
    "deprecated_api": "strict",
    "context": "as parameter to torch.export.export and export_for_training",
    "version": "2.8.0",
    "reason": "Default changed to strict=False; must now be passed explicitly for old behavior.",
    "replacement": "Pass strict=True explicitly if needed.",
    "category": "parameter",
    "severity": "info",
    "deprecated_code": "import torch\nimport torch.nn as nn\n\ndef my_module(x):\n    return x * 2\n\n# Deprecated: 'strict' parameter used, but default has changed in 2.8.0\nexported = torch.export.export(my_module, (torch.randn(2, 3),), strict=True)\n",
    "corrected_code": "import torch\nimport torch.nn as nn\n\ndef my_module(x):\n    return x * 2\n\n# Correct: Pass strict=True explicitly if you want old behavior\nexported = torch.export.export(my_module, (torch.randn(2, 3),), strict=True)\n",
    "change_description": "The 'strict' parameter in torch.export.export is now False by default as of PyTorch 2.8.0. To retain the previous strict behavior, you must now pass strict=True explicitly. The code is unchanged if you want strict behavior, but you must be aware of the new default."
  },
  {
    "deprecated_api": "opset_version",
    "context": "as parameter to torch.onnx.export",
    "version": "2.8.0",
    "reason": "Default opset_version changed from 18 to 20 in 2.9.0.",
    "replacement": "Explicitly set opset_version if needed.",
    "category": "parameter",
    "severity": "info",
    "deprecated_code": "import torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = SimpleModel()\ndummy_input = torch.randn(1, 3)\ntorch.onnx.export(model, dummy_input, 'simple_model.onnx', opset_version=18)",
    "corrected_code": "import torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = SimpleModel()\ndummy_input = torch.randn(1, 3)\ntorch.onnx.export(model, dummy_input, 'simple_model.onnx', opset_version=20)",
    "change_description": "The default value of the 'opset_version' parameter in torch.onnx.export changed from 18 to 20 in PyTorch 2.9.0. To ensure consistent and future-proof behavior, explicitly set 'opset_version' to the desired value (e.g., 20) instead of relying on the previous default."
  },
  {
    "deprecated_api": "weights_only",
    "context": "as parameter to torch.load",
    "version": "2.6.0",
    "reason": "Default value for weights_only changed to True; warning if not specified.",
    "replacement": "Explicitly set weights_only parameter.",
    "category": "parameter",
    "severity": "warning",
    "deprecated_code": "import torch\n\n# Loading a checkpoint with weights_only explicitly set to False (deprecated usage)\nmodel = MyModel()\ncheckpoint = torch.load('checkpoint.pth', weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])",
    "corrected_code": "import torch\n\n# Explicitly set weights_only parameter to avoid deprecation warning\nmodel = MyModel()\ncheckpoint = torch.load('checkpoint.pth', weights_only=False)  # Explicitly set, as recommended\nmodel.load_state_dict(checkpoint['model_state_dict'])",
    "change_description": "In PyTorch 2.6.0, the default value of the weights_only parameter in torch.load changed to True. To avoid deprecation warnings and ensure future compatibility, you should now always explicitly set the weights_only parameter when calling torch.load."
  },
  {
    "deprecated_api": "torch.{device}.amp",
    "context": "autocast API for device-specific autocast",
    "version": "2.4.0",
    "reason": "All autocast APIs are unified under torch.amp.",
    "replacement": "torch.amp",
    "category": "import",
    "severity": "warning",
    "deprecated_code": "import torch.cuda.amp\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters())\n\nwith torch.cuda.amp.autocast():\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()",
    "corrected_code": "import torch.amp\n\nmodel = MyModel().cuda()\noptimizer = torch.optim.Adam(model.parameters())\n\nwith torch.amp.autocast(device_type='cuda'):\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()",
    "change_description": "The code previously imported and used torch.cuda.amp.autocast for device-specific autocasting. In PyTorch 2.4.0, all autocast APIs are unified under torch.amp. The corrected code imports torch.amp and uses torch.amp.autocast with the device_type argument to specify the device."
  },
  {
    "deprecated_api": "data_ptr",
    "context": "as method on FakeTensor in eager-mode",
    "version": "2.4.0",
    "reason": "FakeTensors do not have a valid data pointer; accessing data_ptr is not safe.",
    "replacement": "Do not access data_ptr on FakeTensor.",
    "category": "method",
    "severity": "warning",
    "deprecated_code": "",
    "corrected_code": "",
    "change_description": ""
  },
  {
    "deprecated_api": "no_dist",
    "context": "as parameter to torch.distributed.checkpoint",
    "version": "2.3.0",
    "reason": "no_dist parameter is removed; now inferred from process group state.",
    "replacement": "No replacement; behavior is automatic.",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "import torch\nfrom torch.distributed import checkpoint\n\nmodel = ...  # some nn.Module\nstate_dict = model.state_dict()\n\n# Deprecated usage: passing 'no_dist' parameter\ncheckpoint.save_state_dict(state_dict, 'model.pt', no_dist=True)",
    "corrected_code": "import torch\nfrom torch.distributed import checkpoint\n\nmodel = ...  # some nn.Module\nstate_dict = model.state_dict()\n\n# Correct usage: simply omit 'no_dist', behavior is inferred automatically\ncheckpoint.save_state_dict(state_dict, 'model.pt')",
    "change_description": "The 'no_dist' parameter has been removed from torch.distributed.checkpoint.save_state_dict. You should no longer pass 'no_dist'; the function now automatically infers whether to use distributed mode based on the process group state."
  },
  {
    "deprecated_api": "coordinator_rank",
    "context": "as parameter to torch.distributed.checkpoint",
    "version": "2.3.0",
    "reason": "coordinator_rank parameter is removed; rank 0 is always coordinator.",
    "replacement": "No replacement; rank 0 is coordinator.",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "import torch\nfrom torch.distributed import checkpoint\n\ndef save_model(model, path):\n    # Deprecated usage: coordinator_rank parameter is specified\n    checkpoint.save_state_dict(\n        state_dict=model.state_dict(),\n        storage_writer=checkpoint.FileSystemWriter(path),\n        coordinator_rank=2  # Deprecated: coordinator_rank is no longer supported\n    )",
    "corrected_code": "import torch\nfrom torch.distributed import checkpoint\n\ndef save_model(model, path):\n    # Correct usage: coordinator_rank parameter removed; rank 0 is always coordinator\n    checkpoint.save_state_dict(\n        state_dict=model.state_dict(),\n        storage_writer=checkpoint.FileSystemWriter(path)\n    )",
    "change_description": "The coordinator_rank parameter was removed from torch.distributed.checkpoint.save_state_dict. In the corrected code, coordinator_rank is omitted, as rank 0 is always used as the coordinator."
  },
  {
    "deprecated_api": "tp_mesh_dim",
    "context": "as parameter to parallelize_module",
    "version": "2.3.0",
    "reason": "tp_mesh_dim argument is removed; only DeviceMesh is accepted.",
    "replacement": "Use DeviceMesh directly.",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "import torch\nfrom torch.distributed.tensor.parallel import parallelize_module\nfrom torch.distributed.device_mesh import DeviceMesh\n\nmesh = DeviceMesh(\"cuda\", [0, 1])\nmodel = torch.nn.Linear(10, 10)\n# Deprecated usage: passing tp_mesh_dim\nparallel_model = parallelize_module(model, mesh, tp_mesh_dim=0)\n",
    "corrected_code": "import torch\nfrom torch.distributed.tensor.parallel import parallelize_module\nfrom torch.distributed.device_mesh import DeviceMesh\n\nmesh = DeviceMesh(\"cuda\", [0, 1])\nmodel = torch.nn.Linear(10, 10)\n# Correct usage: do not pass tp_mesh_dim, just pass DeviceMesh\nparallel_model = parallelize_module(model, mesh)\n",
    "change_description": "The tp_mesh_dim argument has been removed from parallelize_module. Now, only the DeviceMesh should be passed; do not specify tp_mesh_dim."
  },
  {
    "deprecated_api": "torch.jit.quantized.quantize_rnn_cell_modules",
    "context": "in torch.jit.quantized",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.nn.quantized.dynamic.RNNCell.",
    "replacement": "torch.ao.nn.quantized.dynamic.RNNCell",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.rnn = nn.RNNCell(10, 20)\n    def forward(self, x, hx):\n        return self.rnn(x, hx)\n\nmodel = MyModel()\nquantized_model = torch.jit.quantized.quantize_rnn_cell_modules(model, dtype=torch.qint8)\n",
    "corrected_code": "import torch\nimport torch.nn as nn\nfrom torch.ao.nn.quantized.dynamic import RNNCell as QuantizedRNNCell\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.rnn = QuantizedRNNCell(10, 20)\n    def forward(self, x, hx):\n        return self.rnn(x, hx)\n\nmodel = MyModel()\n# No need to quantize; RNNCell is already quantized\nquantized_model = model\n",
    "change_description": "The deprecated torch.jit.quantized.quantize_rnn_cell_modules function is removed. Instead, directly use torch.ao.nn.quantized.dynamic.RNNCell in your model definition to create quantized RNN cells. There is no need to call a separate quantization function."
  },
  {
    "deprecated_api": "torch.jit.quantized.quantize_rnn_modules",
    "context": "in torch.jit.quantized",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.quantization.quantize_dynamic.",
    "replacement": "torch.ao.quantization.quantize_dynamic",
    "category": "function",
    "severity": "error",
    "deprecated_code": "",
    "corrected_code": "",
    "change_description": ""
  },
  {
    "deprecated_api": "torch.jit.quantized.quantize_linear_modules",
    "context": "in torch.jit.quantized",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.nn.quantized.dynamic.Linear.",
    "replacement": "torch.ao.nn.quantized.dynamic.Linear",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(10, 5)\n    def forward(self, x):\n        return self.fc(x)\n\nmodel = MyModel()\n# Deprecated quantization API\nquantized_model = torch.jit.quantized.quantize_linear_modules(model)",
    "corrected_code": "import torch\nimport torch.nn as nn\nimport torch.ao.nn.quantized.dynamic as nnqd\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(10, 5)\n    def forward(self, x):\n        return self.fc(x)\n\nmodel = MyModel()\n# Replace nn.Linear with quantized dynamic Linear\nmodel.fc = nnqd.Linear(10, 5)\n# Optionally, you can copy weights from the original Linear\nmodel.fc.set_weight(model.fc._weight_bias()[0])",
    "change_description": "The deprecated torch.jit.quantized.quantize_linear_modules API was removed. Instead, directly replace nn.Linear modules with torch.ao.nn.quantized.dynamic.Linear for quantized inference. This involves manually swapping the layers and optionally copying weights."
  },
  {
    "deprecated_api": "torch.jit.quantized.QuantizedLinear",
    "context": "in torch.jit.quantized",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.nn.quantized.dynamic.Linear.",
    "replacement": "torch.ao.nn.quantized.dynamic.Linear",
    "category": "class",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.jit.quantized\n\n# Define a quantized linear layer using the deprecated API\ndeprecated_linear = torch.jit.quantized.QuantizedLinear(8, 4)\n\n# Example input\ndata = torch.quantize_per_tensor(torch.randn(2, 8), scale=0.1, zero_point=10, dtype=torch.qint8)\n\n# Forward pass\noutput = deprecated_linear(data)\nprint(output)",
    "corrected_code": "import torch\nimport torch.ao.nn.quantized.dynamic as nnqd\n\n# Define a quantized linear layer using the recommended API\ncorrected_linear = nnqd.Linear(8, 4)\n\n# Example input (float, as dynamic quantized layers expect float input)\ndata = torch.randn(2, 8)\n\n# Forward pass\noutput = corrected_linear(data)\nprint(output)",
    "change_description": "The deprecated torch.jit.quantized.QuantizedLinear class has been replaced with torch.ao.nn.quantized.dynamic.Linear. The new class is used directly, and expects float input (dynamic quantization), rather than pre-quantized tensors."
  },
  {
    "deprecated_api": "torch.jit.QuantizedLinearFP16",
    "context": "in torch.jit",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.nn.quantized.dynamic.Linear.",
    "replacement": "torch.ao.nn.quantized.dynamic.Linear",
    "category": "class",
    "severity": "error",
    "deprecated_code": "import torch\n\n# Define a quantized linear layer using the deprecated API\ndeprecated_linear = torch.jit.QuantizedLinearFP16(8, 4)\n\n# Example input tensor\nx = torch.randn(2, 8, dtype=torch.float16)\n\n# Forward pass\noutput = deprecated_linear(x)\nprint(output)",
    "corrected_code": "import torch\nfrom torch.ao.nn.quantized.dynamic import Linear\n\n# Define a quantized linear layer using the recommended API\ncorrected_linear = Linear(8, 4, dtype=torch.qint8)\n\n# Example input tensor\nx = torch.randn(2, 8, dtype=torch.float32)\n\n# Forward pass\noutput = corrected_linear(x)\nprint(output)",
    "change_description": "The deprecated torch.jit.QuantizedLinearFP16 class was replaced with torch.ao.nn.quantized.dynamic.Linear. The new class is used for dynamic quantized linear layers. Note that the input tensor should be float32 for dynamic quantized layers, and the dtype argument specifies the quantized weight type."
  },
  {
    "deprecated_api": "torch.jit.quantized.QuantizedGRU",
    "context": "in torch.jit.quantized",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.nn.quantized.dynamic.GRU.",
    "replacement": "torch.ao.nn.quantized.dynamic.GRU",
    "category": "class",
    "severity": "error",
    "deprecated_code": "import torch\n\n# Deprecated usage of QuantizedGRU from torch.jit.quantized\ngru = torch.jit.quantized.QuantizedGRU(input_size=10, hidden_size=20, num_layers=2)\n\n# Example input\ndata = torch.randn(5, 3, 10)  # (seq_len, batch, input_size)\nh0 = torch.randn(2, 3, 20)    # (num_layers, batch, hidden_size)\n\noutput, hn = gru(data, h0)\nprint(output.shape, hn.shape)",
    "corrected_code": "import torch\nfrom torch.ao.nn.quantized.dynamic import GRU\n\n# Correct usage: use GRU from torch.ao.nn.quantized.dynamic\ngru = GRU(input_size=10, hidden_size=20, num_layers=2)\n\ndata = torch.randn(5, 3, 10)  # (seq_len, batch, input_size)\nh0 = torch.randn(2, 3, 20)    # (num_layers, batch, hidden_size)\n\noutput, hn = gru(data, h0)\nprint(output.shape, hn.shape)",
    "change_description": "The deprecated torch.jit.quantized.QuantizedGRU class has been removed. The correct approach is to use torch.ao.nn.quantized.dynamic.GRU instead. Update the import and class instantiation accordingly."
  },
  {
    "deprecated_api": "torch.jit.quantized.QuantizedGRUCell",
    "context": "in torch.jit.quantized",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.nn.quantized.dynamic.GRUCell.",
    "replacement": "torch.ao.nn.quantized.dynamic.GRUCell",
    "category": "class",
    "severity": "error",
    "deprecated_code": "import torch\n\n# Deprecated usage of QuantizedGRUCell\nqgru_cell = torch.jit.quantized.QuantizedGRUCell(input_size=10, hidden_size=20)\n\n# Example input tensors\nx = torch.randn(1, 10)\nh = torch.randn(1, 20)\n\n# Forward pass\noutput = qgru_cell(x, h)\nprint(output)",
    "corrected_code": "import torch\nfrom torch.ao.nn.quantized.dynamic import GRUCell\n\n# Correct usage of quantized GRUCell\nqgru_cell = GRUCell(input_size=10, hidden_size=20)\n\n# Example input tensors\nx = torch.randn(1, 10)\nh = torch.randn(1, 20)\n\n# Forward pass\noutput = qgru_cell(x, h)\nprint(output)",
    "change_description": "The deprecated torch.jit.quantized.QuantizedGRUCell class has been replaced with torch.ao.nn.quantized.dynamic.GRUCell. Update the import and class instantiation to use the new location."
  },
  {
    "deprecated_api": "torch.jit.quantized.QuantizedLSTM",
    "context": "in torch.jit.quantized",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.nn.quantized.dynamic.LSTM.",
    "replacement": "torch.ao.nn.quantized.dynamic.LSTM",
    "category": "class",
    "severity": "error",
    "deprecated_code": "import torch\n\n# Deprecated: Using torch.jit.quantized.QuantizedLSTM\nlstm = torch.jit.quantized.QuantizedLSTM(input_size=8, hidden_size=16, num_layers=1)\n\n# Dummy input: (seq_len, batch, input_size)\nx = torch.randn(5, 3, 8)\nh0 = torch.randn(1, 3, 16)\nc0 = torch.randn(1, 3, 16)\n\noutput, (hn, cn) = lstm(x, (h0, c0))\nprint(output.shape)",
    "corrected_code": "import torch\nimport torch.ao.nn.quantized.dynamic as nnqd\n\n# Correct: Using torch.ao.nn.quantized.dynamic.LSTM\nlstm = nnqd.LSTM(input_size=8, hidden_size=16, num_layers=1)\n\n# Dummy input: (seq_len, batch, input_size)\nx = torch.randn(5, 3, 8)\nh0 = torch.randn(1, 3, 16)\nc0 = torch.randn(1, 3, 16)\n\noutput, (hn, cn) = lstm(x, (h0, c0))\nprint(output.shape)",
    "change_description": "The code replaces the deprecated torch.jit.quantized.QuantizedLSTM class with the recommended torch.ao.nn.quantized.dynamic.LSTM class. The import and instantiation are updated to use the new API, while the rest of the code remains unchanged."
  },
  {
    "deprecated_api": "torch.jit.quantized.QuantizedLSTMCell",
    "context": "in torch.jit.quantized",
    "version": "2.3.0",
    "reason": "torch.jit.quantized APIs are removed; use torch.ao.nn.quantized.dynamic.LSTM.",
    "replacement": "torch.ao.nn.quantized.dynamic.LSTM",
    "category": "class",
    "severity": "error",
    "deprecated_code": "import torch\n\n# Deprecated: Using torch.jit.quantized.QuantizedLSTMCell\ndtype = torch.qint8\ncell = torch.jit.quantized.QuantizedLSTMCell(input_size=10, hidden_size=20, dtype=dtype)\n\nx = torch.randn(3, 10)\nhx = torch.randn(3, 20)\ncx = torch.randn(3, 20)\noutput, (hn, cn) = cell(x, (hx, cx))\nprint(output.shape)",
    "corrected_code": "import torch\nfrom torch.ao.nn.quantized.dynamic import LSTM\n\n# Correct: Use torch.ao.nn.quantized.dynamic.LSTM\nlstm = LSTM(input_size=10, hidden_size=20, num_layers=1)\n\nx = torch.randn(3, 1, 10)  # (seq_len, batch, input_size)\noutput, (hn, cn) = lstm(x)\nprint(output.shape)",
    "change_description": "The deprecated torch.jit.quantized.QuantizedLSTMCell class is removed. The correct approach is to use torch.ao.nn.quantized.dynamic.LSTM, which provides quantized LSTM functionality. The new API works at the module level (entire LSTM), not just a single cell, and expects input in (seq_len, batch, input_size) format."
  },
  {
    "deprecated_api": "torch._export.export",
    "context": "in torch._export",
    "version": "2.3.0",
    "reason": "Removed in favor of torch.export.export.",
    "replacement": "torch.export.export",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\nimport torch.nn as nn\n\ndef get_args():\n    return (torch.randn(1, 3),)\n\nclass MyModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = MyModel()\nexported = torch._export.export(model, get_args)\nprint(exported)\n",
    "corrected_code": "import torch\nimport torch.nn as nn\n\ndef get_args():\n    return (torch.randn(1, 3),)\n\nclass MyModel(nn.Module):\n    def forward(self, x):\n        return x * 2\n\nmodel = MyModel()\nexported = torch.export.export(model, get_args)\nprint(exported)\n",
    "change_description": "The code replaces the deprecated torch._export.export function with the new torch.export.export function, as torch._export.export has been removed in favor of torch.export.export in PyTorch 2.3.0."
  },
  {
    "deprecated_api": "CallSpec",
    "context": "in torch.export",
    "version": "2.3.0",
    "reason": "CallSpec is removed.",
    "replacement": "No replacement.",
    "category": "class",
    "severity": "error",
    "deprecated_code": "",
    "corrected_code": "",
    "change_description": ""
  },
  {
    "deprecated_api": "constraints",
    "context": "as parameter to torch.export.export",
    "version": "2.3.0",
    "reason": "Removed in favor of dynamic_shapes.",
    "replacement": "dynamic_shapes",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "",
    "corrected_code": "",
    "change_description": ""
  },
  {
    "deprecated_api": "equality_constraints",
    "context": "as parameter to ExportedProgram",
    "version": "2.3.0",
    "reason": "Removed; dimensions with equal constraints now have the same symbol.",
    "replacement": "No replacement.",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "import torch\nfrom torch.export import export, ExportedProgram\n\ndef my_module(x, y):\n    return x + y\n\n# Deprecated usage of equality_constraints\nexp_prog = export(\n    my_module,\n    (torch.randn(2, 3), torch.randn(2, 3)),\n    equality_constraints=[(0, 1)]  # Deprecated parameter\n)\n",
    "corrected_code": "import torch\nfrom torch.export import export, ExportedProgram\n\ndef my_module(x, y):\n    return x + y\n\n# Corrected: simply remove equality_constraints\nexp_prog = export(\n    my_module,\n    (torch.randn(2, 3), torch.randn(2, 3))\n)\n",
    "change_description": "The equality_constraints parameter has been removed from torch.export.export and ExportedProgram in PyTorch 2.3.0. To correct the code, simply omit the equality_constraints argument; dimensions with equal constraints now automatically share the same symbol."
  },
  {
    "deprecated_api": "definitely_true",
    "context": "in torch.fx.experimental.symbolic_shapes",
    "version": "2.8.0",
    "reason": "Removed in favor of guard_or_true.",
    "replacement": "guard_or_true",
    "category": "function",
    "severity": "error",
    "deprecated_code": "from torch.fx.experimental.symbolic_shapes import definitely_true\n\ndef check_shape(shape_env, expr):\n    # Deprecated usage of definitely_true\n    if definitely_true(expr, shape_env):\n        print(\"Expression is definitely true in the given shape environment.\")\n    else:\n        print(\"Expression is not definitely true.\")\n\n# Example usage\n# check_shape(shape_env, expr)",
    "corrected_code": "from torch.fx.experimental.symbolic_shapes import guard_or_true\n\ndef check_shape(shape_env, expr):\n    # Updated usage: use guard_or_true instead of definitely_true\n    if guard_or_true(expr, shape_env):\n        print(\"Expression is definitely true in the given shape environment.\")\n    else:\n        print(\"Expression is not definitely true.\")\n\n# Example usage\n# check_shape(shape_env, expr)",
    "change_description": "The function definitely_true from torch.fx.experimental.symbolic_shapes has been removed in favor of guard_or_true. Replace all usages of definitely_true with guard_or_true to ensure compatibility with PyTorch 2.8.0 and later."
  },
  {
    "deprecated_api": "definitely_false",
    "context": "in torch.fx.experimental.symbolic_shapes",
    "version": "2.8.0",
    "reason": "Removed in favor of guard_or_false.",
    "replacement": "guard_or_false",
    "category": "function",
    "severity": "error",
    "deprecated_code": "from torch.fx.experimental.symbolic_shapes import definitely_false\n\ndef check_shape_equality(a, b):\n    if definitely_false(a == b):\n        print(\"Shapes are definitely not equal.\")\n    else:\n        print(\"Shapes might be equal.\")\n\n# Example usage\ncheck_shape_equality(3, 4)",
    "corrected_code": "from torch.fx.experimental.symbolic_shapes import guard_or_false\n\ndef check_shape_equality(a, b):\n    if guard_or_false(a == b):\n        print(\"Shapes are definitely not equal.\")\n    else:\n        print(\"Shapes might be equal.\")\n\n# Example usage\ncheck_shape_equality(3, 4)",
    "change_description": "The function 'definitely_false' has been removed in favor of 'guard_or_false'. Replace all usages and imports of 'definitely_false' with 'guard_or_false' from torch.fx.experimental.symbolic_shapes."
  },
  {
    "deprecated_api": "enable_cpp_framelocals_guard_eval",
    "context": "as Dynamo config variable",
    "version": "2.8.0",
    "reason": "No longer has any effect.",
    "replacement": "No replacement.",
    "category": "parameter",
    "severity": "info",
    "deprecated_code": "import torch\n\ndef my_function(x):\n    return x * 2\n\nwith torch._dynamo.config.patch(enable_cpp_framelocals_guard_eval=True):\n    compiled_fn = torch.compile(my_function)\n    result = compiled_fn(torch.tensor(3))\n    print(result)",
    "corrected_code": "import torch\n\ndef my_function(x):\n    return x * 2\n\nwith torch._dynamo.config.patch():  # Removed enable_cpp_framelocals_guard_eval\n    compiled_fn = torch.compile(my_function)\n    result = compiled_fn(torch.tensor(3))\n    print(result)",
    "change_description": "The 'enable_cpp_framelocals_guard_eval' parameter was removed from the torch._dynamo.config.patch() call, as it is deprecated and no longer has any effect in PyTorch 2.8.0 and later."
  },
  {
    "deprecated_api": "rocm.n_max_profiling_configs",
    "context": "as Inductor config variable",
    "version": "2.8.0",
    "reason": "Deprecated in favor of rocm.ck_max_profiling_configs and rocm.ck_tile_max_profiling_configs.",
    "replacement": "rocm.ck_max_profiling_configs, rocm.ck_tile_max_profiling_configs",
    "category": "parameter",
    "severity": "warning",
    "deprecated_code": "import torch\nfrom torch._inductor import config\n\n# Set the maximum number of profiling configs for ROCm (DEPRECATED)\nconfig.rocm.n_max_profiling_configs = 32\n\n# ... rest of your model/inductor code ...",
    "corrected_code": "import torch\nfrom torch._inductor import config\n\n# Set the maximum number of profiling configs for ROCm (RECOMMENDED)\nconfig.rocm.ck_max_profiling_configs = 32\n# Optionally, set tile-specific profiling configs\nconfig.rocm.ck_tile_max_profiling_configs = 16\n\n# ... rest of your model/inductor code ...",
    "change_description": "The deprecated 'config.rocm.n_max_profiling_configs' parameter has been replaced by 'config.rocm.ck_max_profiling_configs' and 'config.rocm.ck_tile_max_profiling_configs'. Update your code to use the new configuration variables for controlling ROCm profiling configs in Inductor."
  },
  {
    "deprecated_api": "autotune_fallback_to_aten",
    "context": "as Inductor config variable",
    "version": "2.8.0",
    "reason": "Inductor will no longer silently fall back to ATen.",
    "replacement": "Add 'ATEN' to max_autotune_gemm_backends for old behavior.",
    "category": "parameter",
    "severity": "warning",
    "deprecated_code": "\nimport torch\nfrom torch._inductor import config\n\n# Deprecated: Using 'autotune_fallback_to_aten' in Inductor config\nconfig.autotune_fallback_to_aten = True\n\n# Example model and compilation\nmodel = torch.nn.Linear(10, 5)\ncompiled_model = torch.compile(model)\n",
    "corrected_code": "\nimport torch\nfrom torch._inductor import config\n\n# Correct: Add 'ATEN' to 'max_autotune_gemm_backends' for old fallback behavior\nconfig.max_autotune_gemm_backends.append('ATEN')\n\n# Example model and compilation\nmodel = torch.nn.Linear(10, 5)\ncompiled_model = torch.compile(model)\n",
    "change_description": "The deprecated 'autotune_fallback_to_aten' Inductor config variable is removed. To restore the old fallback-to-ATen behavior, add 'ATEN' to the 'max_autotune_gemm_backends' list instead."
  },
  {
    "deprecated_api": "use_mixed_mm",
    "context": "as Inductor config variable",
    "version": "2.8.0",
    "reason": "Inductor now supports prologue fusion; no need for special cases.",
    "replacement": "No replacement.",
    "category": "parameter",
    "severity": "warning",
    "deprecated_code": "import torch\nfrom torch._inductor import config\n\ndef run_model():\n    # Enable mixed mm for Inductor (DEPRECATED)\n    config.use_mixed_mm = True\n    x = torch.randn(16, 32, device='cuda')\n    y = torch.randn(32, 64, device='cuda')\n    result = torch.mm(x, y)\n    return result\n",
    "corrected_code": "import torch\n\ndef run_model():\n    # No need to set use_mixed_mm; Inductor handles prologue fusion automatically\n    x = torch.randn(16, 32, device='cuda')\n    y = torch.randn(32, 64, device='cuda')\n    result = torch.mm(x, y)\n    return result\n",
    "change_description": "The deprecated config variable 'use_mixed_mm' was removed. Inductor now supports prologue fusion by default, so there is no need to set this parameter."
  },
  {
    "deprecated_api": "mixed_mm_choice",
    "context": "as Inductor config variable",
    "version": "2.8.0",
    "reason": "Inductor now supports prologue fusion; no need for special cases.",
    "replacement": "No replacement.",
    "category": "parameter",
    "severity": "warning",
    "deprecated_code": "import torch\nfrom torch._inductor import config\n\ndef enable_mixed_mm():\n    # Enable mixed_mm_choice for special matmul fusion\n    config.mixed_mm_choice = True\n    # ... rest of the code ...\n    x = torch.randn(16, 16)\n    y = torch.randn(16, 16)\n    z = torch.mm(x, y)\n    return z",
    "corrected_code": "import torch\nfrom torch._inductor import config\n\ndef enable_mixed_mm():\n    # mixed_mm_choice is deprecated and no longer needed\n    # Inductor now supports prologue fusion by default\n    # ... rest of the code ...\n    x = torch.randn(16, 16)\n    y = torch.randn(16, 16)\n    z = torch.mm(x, y)\n    return z",
    "change_description": "The 'mixed_mm_choice' configuration variable is removed from the code, as Inductor now supports prologue fusion by default and no special configuration is needed."
  },
  {
    "deprecated_api": "descriptive_names",
    "context": "as Inductor config setting",
    "version": "2.8.0",
    "reason": "Deprecated; use 'torch', 'original_aten', or 'inductor_node' instead.",
    "replacement": "'torch', 'original_aten', or 'inductor_node'",
    "category": "parameter",
    "severity": "warning",
    "deprecated_code": "import torch\nfrom torch._inductor import config\n\n# Enable descriptive names for generated code (DEPRECATED)\nconfig.descriptive_names = True\n\ndef my_model(x):\n    return x * 2\n\ncompiled_model = torch.compile(my_model)\nresult = compiled_model(torch.tensor([1, 2, 3]))\nprint(result)",
    "corrected_code": "import torch\nfrom torch._inductor import config\n\n# Use one of the supported name styles: 'torch', 'original_aten', or 'inductor_node'\nconfig.name_style = 'torch'  # or 'original_aten', or 'inductor_node'\n\ndef my_model(x):\n    return x * 2\n\ncompiled_model = torch.compile(my_model)\nresult = compiled_model(torch.tensor([1, 2, 3]))\nprint(result)",
    "change_description": "The deprecated 'config.descriptive_names' setting was removed. Instead, use 'config.name_style' and set it to one of the supported values: 'torch', 'original_aten', or 'inductor_node'."
  },
  {
    "deprecated_api": "custom_op_default_layout_constraint",
    "context": "in torch._inductor.config",
    "version": "2.8.0",
    "reason": "Moved to functorch config.",
    "replacement": "torch._functorch.config.custom_op_default_layout_constraint",
    "category": "parameter",
    "severity": "info",
    "deprecated_code": "import torch\n\n# Set the default layout constraint for custom ops using the deprecated API\nold_value = torch._inductor.config.custom_op_default_layout_constraint\ntorch._inductor.config.custom_op_default_layout_constraint = 'channels_last'\n\n# ... run some code ...\n\n# Restore the old value\ntorch._inductor.config.custom_op_default_layout_constraint = old_value",
    "corrected_code": "import torch\n\n# Set the default layout constraint for custom ops using the new API\nold_value = torch._functorch.config.custom_op_default_layout_constraint\ntorch._functorch.config.custom_op_default_layout_constraint = 'channels_last'\n\n# ... run some code ...\n\n# Restore the old value\ntorch._functorch.config.custom_op_default_layout_constraint = old_value",
    "change_description": "The configuration parameter 'custom_op_default_layout_constraint' has moved from 'torch._inductor.config' to 'torch._functorch.config'. Update your code to use 'torch._functorch.config.custom_op_default_layout_constraint' instead of the deprecated location."
  },
  {
    "deprecated_api": "emit_current_arch_binary",
    "context": "as AOTI config variable",
    "version": "2.8.0",
    "reason": "Deprecated.",
    "replacement": "No replacement.",
    "category": "parameter",
    "severity": "warning",
    "deprecated_code": "import torch\nfrom torch._inductor.aoti import AOTIConfig\n\naoti_config = AOTIConfig(\n    emit_current_arch_binary=True,  # Deprecated parameter\n    some_other_option=False\n)\nprint(aoti_config)",
    "corrected_code": "import torch\nfrom torch._inductor.aoti import AOTIConfig\n\naoti_config = AOTIConfig(\n    some_other_option=False\n)\nprint(aoti_config)",
    "change_description": "The 'emit_current_arch_binary' parameter has been removed from the AOTIConfig constructor, as it is deprecated and no longer supported. There is no replacement for this parameter."
  },
  {
    "deprecated_api": "aot_inductor.embed_cubin",
    "context": "as AOTI config variable",
    "version": "2.8.0",
    "reason": "Renamed to aot_inductor.embed_kernel_binary.",
    "replacement": "aot_inductor.embed_kernel_binary",
    "category": "parameter",
    "severity": "info",
    "deprecated_code": "import torch\nfrom torch._inductor import config as inductor_config\n\n# Enable embedding of CUDA binary (deprecated way)\ninductor_config.aot_inductor.embed_cubin = True\n\ndef my_function(x):\n    return x * x\n\nscripted_fn = torch.compile(my_function)\nresult = scripted_fn(torch.randn(3, 3))",
    "corrected_code": "import torch\nfrom torch._inductor import config as inductor_config\n\n# Enable embedding of kernel binary (new way)\ninductor_config.aot_inductor.embed_kernel_binary = True\n\ndef my_function(x):\n    return x * x\n\nscripted_fn = torch.compile(my_function)\nresult = scripted_fn(torch.randn(3, 3))",
    "change_description": "The configuration variable 'aot_inductor.embed_cubin' has been renamed to 'aot_inductor.embed_kernel_binary'. Update your code to use 'embed_kernel_binary' instead of 'embed_cubin' when configuring AOTInductor options."
  },
  {
    "deprecated_api": "aot_inductor.compile_wrapper_with_O0",
    "context": "as AOTI config variable",
    "version": "2.8.0",
    "reason": "Renamed to compile_wrapper_opt_level.",
    "replacement": "compile_wrapper_opt_level",
    "category": "parameter",
    "severity": "info",
    "deprecated_code": "import torch\nfrom torch._dynamo.backends import aot_inductor\n\ndef my_backend(gm, example_inputs):\n    return aot_inductor.compile(gm, example_inputs, compile_wrapper_with_O0=True)\n\n# Example usage\ncompiled_fn = torch.compile(lambda x: x + 1, backend=my_backend)\nresult = compiled_fn(torch.tensor(1.0))",
    "corrected_code": "import torch\nfrom torch._dynamo.backends import aot_inductor\n\ndef my_backend(gm, example_inputs):\n    return aot_inductor.compile(gm, example_inputs, compile_wrapper_opt_level=0)\n\n# Example usage\ncompiled_fn = torch.compile(lambda x: x + 1, backend=my_backend)\nresult = compiled_fn(torch.tensor(1.0))",
    "change_description": "The parameter 'compile_wrapper_with_O0' has been renamed to 'compile_wrapper_opt_level'. Update your backend configuration to use 'compile_wrapper_opt_level=0' instead of 'compile_wrapper_with_O0=True'."
  },
  {
    "deprecated_api": "torch.autograd.function.traceable",
    "context": "in torch.autograd.function",
    "version": "2.4.0",
    "reason": "API deleted; was only for internal purposes.",
    "replacement": "No replacement.",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\n\n@torch.autograd.function.traceable\nclass MyFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        return input * 2\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output * 2\n",
    "corrected_code": "import torch\n\nclass MyFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        return input * 2\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output * 2\n",
    "change_description": "The @torch.autograd.function.traceable decorator has been removed because it was deleted in PyTorch 2.4.0 and was only for internal use. Simply remove the decorator; there is no replacement needed."
  },
  {
    "deprecated_api": "torch.distributed.pipeline",
    "context": "import path",
    "version": "2.4.0",
    "reason": "Module removed; use torch.distributed.pipelining instead.",
    "replacement": "torch.distributed.pipelining",
    "category": "import",
    "severity": "error",
    "deprecated_code": "import torch.distributed.pipeline as pipeline\n\n# Example: Creating a simple pipeline model\nmodel = pipeline.Pipeline(\n    modules=[layer1, layer2, layer3],\n    chunks=4\n)\noutput = model(input_tensor)",
    "corrected_code": "import torch.distributed.pipelining as pipelining\n\n# Example: Creating a simple pipeline model\nmodel = pipelining.Pipeline(\n    modules=[layer1, layer2, layer3],\n    chunks=4\n)\noutput = model(input_tensor)",
    "change_description": "The import path torch.distributed.pipeline has been removed in PyTorch 2.4.0. You should import from torch.distributed.pipelining instead. All usages of pipeline should be updated to pipelining."
  },
  {
    "deprecated_api": "get_group",
    "context": "in DeviceMesh",
    "version": "2.4.0",
    "reason": "Calling get_group without passing in the dim now errors; use get_all_groups.",
    "replacement": "get_all_groups",
    "category": "method",
    "severity": "error",
    "deprecated_code": "import torch\nfrom torch.distributed.device_mesh import DeviceMesh\n\ndevice_mesh = DeviceMesh('cuda', [[0, 1], [2, 3]])\n# Deprecated: get_group() without dim argument\nmesh_group = device_mesh.get_group()\nprint(mesh_group)",
    "corrected_code": "import torch\nfrom torch.distributed.device_mesh import DeviceMesh\n\ndevice_mesh = DeviceMesh('cuda', [[0, 1], [2, 3]])\n# Correct: use get_all_groups() to get all groups\nmesh_groups = device_mesh.get_all_groups()\nprint(mesh_groups)",
    "change_description": "The deprecated code calls device_mesh.get_group() without specifying the 'dim' argument, which now errors in PyTorch 2.4.0. The corrected code replaces get_group() with get_all_groups(), which returns all groups associated with the device mesh."
  },
  {
    "deprecated_api": "PROCESS_GROUP",
    "context": "as backend argument to torch.distributed.rpc.init_rpc",
    "version": "1.11.0",
    "reason": "ProcessGroup RPC backend is removed.",
    "replacement": "TENSORPIPE",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "",
    "corrected_code": "",
    "change_description": ""
  },
  {
    "deprecated_api": "steps",
    "context": "as parameter to torch.linspace and torch.logspace",
    "version": "1.11.0",
    "reason": "steps argument is no longer optional; must be specified.",
    "replacement": "Specify steps parameter explicitly.",
    "category": "parameter",
    "severity": "error",
    "deprecated_code": "import torch\n\n# Deprecated usage: omitting 'steps' parameter (was optional before 1.11.0)\nstart = 0\nend = 10\nx = torch.linspace(start, end)\ny = torch.logspace(start, end)\nprint(x)\nprint(y)",
    "corrected_code": "import torch\n\n# Corrected usage: specify 'steps' parameter explicitly\nstart = 0\nend = 10\nx = torch.linspace(start, end, steps=100)\ny = torch.logspace(start, end, steps=100)\nprint(x)\nprint(y)",
    "change_description": "The 'steps' parameter for torch.linspace and torch.logspace is no longer optional as of PyTorch 1.11.0. You must now explicitly specify the 'steps' argument when calling these functions."
  },
  {
    "deprecated_api": "torch.hub.import_module",
    "context": "in torch.hub",
    "version": "1.11.0",
    "reason": "Function was mistakenly public; use torch.hub._import_module.",
    "replacement": "torch.hub._import_module",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch.hub\n\n# Load a module from a given file path using the deprecated API\ndef load_custom_module(module_path, module_name):\n    module = torch.hub.import_module(module_path, module_name)\n    return module\n\ncustom_module = load_custom_module('my_models/custom_model.py', 'custom_model')\n",
    "corrected_code": "import torch.hub\n\n# Load a module from a given file path using the correct API\ndef load_custom_module(module_path, module_name):\n    module = torch.hub._import_module(module_path, module_name)\n    return module\n\ncustom_module = load_custom_module('my_models/custom_model.py', 'custom_model')\n",
    "change_description": "The code replaces the deprecated torch.hub.import_module function with the correct torch.hub._import_module function, as import_module was mistakenly made public and is now deprecated."
  },
  {
    "deprecated_api": "torch.eig",
    "context": "in torch",
    "version": "1.13.0",
    "reason": "Function removed after deprecation cycle.",
    "replacement": "torch.linalg.eig",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\n\n# Create a random square matrix\nA = torch.randn(4, 4)\n\n# Compute eigenvalues and eigenvectors using the deprecated torch.eig\n# Note: torch.eig returns a tuple (eigenvalues, eigenvectors)\neigenvalues, eigenvectors = torch.eig(A, eigenvectors=True)\n\nprint('Eigenvalues:', eigenvalues)\nprint('Eigenvectors:', eigenvectors)",
    "corrected_code": "import torch\n\n# Create a random square matrix\nA = torch.randn(4, 4)\n\n# Compute eigenvalues and eigenvectors using the recommended torch.linalg.eig\n# torch.linalg.eig returns (eigenvalues, eigenvectors)\neigenvalues, eigenvectors = torch.linalg.eig(A)\n\nprint('Eigenvalues:', eigenvalues)\nprint('Eigenvectors:', eigenvectors)",
    "change_description": "The deprecated torch.eig function has been replaced with torch.linalg.eig. The new function returns eigenvalues as a 1D complex tensor and eigenvectors as a 2D complex tensor, and does not require the eigenvectors argument. The output format is also slightly different: torch.eig returned real and imaginary parts separately in a 2D tensor, while torch.linalg.eig returns complex numbers."
  },
  {
    "deprecated_api": "torch.matrix_rank",
    "context": "in torch",
    "version": "1.13.0",
    "reason": "Function removed after deprecation cycle.",
    "replacement": "torch.linalg.matrix_rank",
    "category": "function",
    "severity": "error",
    "deprecated_code": "",
    "corrected_code": "",
    "change_description": ""
  },
  {
    "deprecated_api": "torch.lstsq",
    "context": "in torch",
    "version": "1.13.0",
    "reason": "Function removed after deprecation cycle.",
    "replacement": "torch.linalg.lstsq",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\n\nA = torch.randn(5, 3)\nB = torch.randn(5, 2)\n# Solve the least squares problem Ax = B\nresult = torch.lstsq(B, A)\nX = result.solution  # X contains the solution\nprint(X)",
    "corrected_code": "import torch\n\nA = torch.randn(5, 3)\nB = torch.randn(5, 2)\n# Solve the least squares problem Ax = B\nresult = torch.linalg.lstsq(A, B)\nX = result.solution  # X contains the solution\nprint(X)",
    "change_description": "The deprecated torch.lstsq function has been replaced by torch.linalg.lstsq. The argument order has changed: torch.lstsq(B, A) becomes torch.linalg.lstsq(A, B). The result is now a named tuple with a .solution attribute."
  },
  {
    "deprecated_api": "torch.{is,set}_deterministic",
    "context": "in torch",
    "version": "1.10.0",
    "reason": "End of deprecation cycle; use torch.use_deterministic_algorithms and torch.are_deterministic_algorithms_enabled.",
    "replacement": "torch.use_deterministic_algorithms, torch.are_deterministic_algorithms_enabled",
    "category": "function",
    "severity": "error",
    "deprecated_code": "import torch\n\ndef train_model():\n    # Ensure deterministic algorithms for reproducibility\n    torch.set_deterministic(True)\n    print('Deterministic mode:', torch.is_deterministic())\n    # ... training code ...\n\ntrain_model()",
    "corrected_code": "import torch\n\ndef train_model():\n    # Ensure deterministic algorithms for reproducibility\n    torch.use_deterministic_algorithms(True)\n    print('Deterministic mode:', torch.are_deterministic_algorithms_enabled())\n    # ... training code ...\n\ntrain_model()",
    "change_description": "The deprecated torch.set_deterministic and torch.is_deterministic functions have been replaced by torch.use_deterministic_algorithms and torch.are_deterministic_algorithms_enabled, respectively. The corrected code uses the new API to enable deterministic algorithms and check their status."
  }
]