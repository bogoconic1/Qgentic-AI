2025-09-21 05:36:16,010 [INFO] Initialized logging and created output directories for v6 pipeline.
2025-09-21 05:36:16,010 [INFO] Setting all random seeds to 42.
2025-09-21 05:36:16,011 [INFO] Reading train data from task/us-patent-phrase-to-phrase-matching/train.csv.
2025-09-21 05:36:16,047 [INFO] Reading test data from task/us-patent-phrase-to-phrase-matching/test.csv.
2025-09-21 05:36:16,053 [INFO] Train shape: (32825, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 05:36:16,053 [INFO] Test shape:  (3648, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 05:36:16,059 [INFO] Train missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 05:36:16,059 [INFO] Test missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 05:36:16,059 [INFO] Building vocabulary from segmented corpus.
2025-09-21 05:36:16,395 [INFO] Built vocabulary of size 9042.
2025-09-21 05:36:16,395 [INFO] Tokenizing and numericalizing dataframe with 32825 rows (with token types).
2025-09-21 05:36:16,513 [INFO] Processed 5000 rows.
2025-09-21 05:36:16,631 [INFO] Processed 10000 rows.
2025-09-21 05:36:16,749 [INFO] Processed 15000 rows.
2025-09-21 05:36:16,867 [INFO] Processed 20000 rows.
2025-09-21 05:36:16,985 [INFO] Processed 25000 rows.
2025-09-21 05:36:17,103 [INFO] Processed 30000 rows.
2025-09-21 05:36:17,170 [INFO] Tokenizing and numericalizing dataframe with 3648 rows (with token types).
2025-09-21 05:36:17,257 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 32825 rows (anchor<->target).
2025-09-21 05:36:17,375 [INFO] Processed 5000 swapped rows.
2025-09-21 05:36:17,495 [INFO] Processed 10000 swapped rows.
2025-09-21 05:36:17,613 [INFO] Processed 15000 swapped rows.
2025-09-21 05:36:17,730 [INFO] Processed 20000 swapped rows.
2025-09-21 05:36:17,848 [INFO] Processed 25000 swapped rows.
2025-09-21 05:36:17,965 [INFO] Processed 30000 swapped rows.
2025-09-21 05:36:18,032 [INFO] Prepared swapped augmentation arrays for training.
2025-09-21 05:36:18,033 [INFO] Computing handcrafted similarity features.
2025-09-21 05:36:18,463 [INFO] Computed handcrafted features for 5000 rows.
2025-09-21 05:36:18,886 [INFO] Computed handcrafted features for 10000 rows.
2025-09-21 05:36:19,315 [INFO] Computed handcrafted features for 15000 rows.
2025-09-21 05:36:19,745 [INFO] Computed handcrafted features for 20000 rows.
2025-09-21 05:36:20,171 [INFO] Computed handcrafted features for 25000 rows.
2025-09-21 05:36:20,596 [INFO] Computed handcrafted features for 30000 rows.
2025-09-21 05:36:20,842 [INFO] Finished computing handcrafted features.
2025-09-21 05:36:20,842 [INFO] Computing handcrafted similarity features.
2025-09-21 05:36:21,154 [INFO] Finished computing handcrafted features.
2025-09-21 05:36:21,154 [INFO] Handcrafted feature dimension: 8
2025-09-21 05:36:21,154 [INFO] Labels statistics: min=0.0, max=1.0, mean=0.3619, std=0.2588
2025-09-21 05:36:21,159 [INFO] Found 106 unique context codes across train+test.
2025-09-21 05:36:21,412 [INFO] Class counts: [6774.0, 10306.0, 11068.0, 3634.0, 1043.0] | CE weights: [0.15397107601165771, 0.10120318084955215, 0.0942356288433075, 0.2870115637779236, 1.0]
2025-09-21 05:36:21,414 [INFO] Creating stratified folds on 5-class bins.
2025-09-21 05:36:21,429 [INFO] Fold 0: 6567 samples.
2025-09-21 05:36:21,429 [INFO] Fold 1: 6566 samples.
2025-09-21 05:36:21,429 [INFO] Fold 2: 6566 samples.
2025-09-21 05:36:21,429 [INFO] Fold 3: 6564 samples.
2025-09-21 05:36:21,429 [INFO] Fold 4: 6562 samples.
2025-09-21 05:36:21,430 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 05:36:21,430 [INFO] ========== Fold 1/5 ==========
2025-09-21 05:36:22,343 [INFO] Applied swap augmentation. New train size: 52516
2025-09-21 05:36:22,348 [INFO] Train split: 52516 samples; Val split: 6567 samples.
2025-09-21 05:36:22,348 [INFO] Initializing PatentDataset with 52516 samples.
2025-09-21 05:36:22,348 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 05:36:22,348 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 05:36:23,379 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 05:36:23,379 [INFO] Initializing EMA state from current model weights.
2025-09-21 05:36:23,526 [INFO] Fold 0 - Epoch 1/6 started.
2025-09-21 05:36:36,375 [INFO] Train step 50/274 - Loss: 1.047944 (MSE 0.086246, CE 2.506636, Corr 0.976133, Cons 0.173863, CSim 0.137291, Rank 0.695072) | LR: 6.097561e-05
2025-09-21 05:36:48,002 [INFO] Train step 100/274 - Loss: 0.932436 (MSE 0.080951, CE 2.199225, Corr 0.939496, Cons 0.149609, CSim 0.108968, Rank 0.691670) | LR: 1.219512e-04
2025-09-21 05:36:59,717 [INFO] Train step 150/274 - Loss: 0.879059 (MSE 0.077908, CE 2.062932, Corr 0.902452, Cons 0.138324, CSim 0.096084, Rank 0.688325) | LR: 1.829268e-04
2025-09-21 05:37:11,316 [INFO] Train step 200/274 - Loss: 0.844519 (MSE 0.074895, CE 1.977765, Corr 0.874338, Cons 0.122627, CSim 0.088240, Rank 0.685952) | LR: 1.997082e-04
2025-09-21 05:37:22,945 [INFO] Train step 250/274 - Loss: 0.818548 (MSE 0.073299, CE 1.913421, Corr 0.848804, Cons 0.112752, CSim 0.083927, Rank 0.683612) | LR: 1.983384e-04
2025-09-21 05:37:28,570 [INFO] Epoch training completed in 65.04s with average loss 0.806270
2025-09-21 05:37:28,571 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:37:30,089 [INFO] Fold 0 - Epoch 1 - Train Loss: 0.806270 | EMA Val MSE: 0.068648 | EMA Val CE: 1.785402 | EMA Val Pearson: 0.162608
2025-09-21 05:37:30,093 [INFO] Fold 0 - Epoch 1 - New best EMA model with Pearson 0.162608.
2025-09-21 05:37:30,093 [INFO] Fold 0 - Epoch 2/6 started.
2025-09-21 05:37:41,976 [INFO] Train step 50/274 - Loss: 0.671213 (MSE 0.060186, CE 1.567532, Corr 0.648072, Cons 0.062213, CSim 0.062698, Rank 0.664997) | LR: 1.942877e-04
2025-09-21 05:37:53,549 [INFO] Train step 100/274 - Loss: 0.654996 (MSE 0.059647, CE 1.529731, Corr 0.618018, Cons 0.059706, CSim 0.063263, Rank 0.659719) | LR: 1.902280e-04
2025-09-21 05:38:05,176 [INFO] Train step 150/274 - Loss: 0.647348 (MSE 0.058296, CE 1.515022, Corr 0.599146, Cons 0.057236, CSim 0.062634, Rank 0.656395) | LR: 1.851529e-04
2025-09-21 05:38:16,863 [INFO] Train step 200/274 - Loss: 0.639170 (MSE 0.057171, CE 1.498252, Corr 0.580828, Cons 0.054716, CSim 0.062149, Rank 0.653404) | LR: 1.791195e-04
2025-09-21 05:38:28,453 [INFO] Train step 250/274 - Loss: 0.631361 (MSE 0.056200, CE 1.481942, Corr 0.564286, Cons 0.051875, CSim 0.061553, Rank 0.650278) | LR: 1.721956e-04
2025-09-21 05:38:34,085 [INFO] Epoch training completed in 63.99s with average loss 0.626394
2025-09-21 05:38:34,086 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:38:35,614 [INFO] Fold 0 - Epoch 2 - Train Loss: 0.626394 | EMA Val MSE: 0.059119 | EMA Val CE: 1.533489 | EMA Val Pearson: 0.380282
2025-09-21 05:38:35,618 [INFO] Fold 0 - Epoch 2 - New best EMA model with Pearson 0.380282.
2025-09-21 05:38:35,618 [INFO] Fold 0 - Epoch 3/6 started.
2025-09-21 05:38:47,541 [INFO] Train step 50/274 - Loss: 0.548123 (MSE 0.045687, CE 1.300350, Corr 0.415373, Cons 0.036090, CSim 0.056575, Rank 0.621849) | LR: 1.604825e-04
2025-09-21 05:38:59,138 [INFO] Train step 100/274 - Loss: 0.542668 (MSE 0.044925, CE 1.288353, Corr 0.406539, Cons 0.033831, CSim 0.055910, Rank 0.620818) | LR: 1.517058e-04
2025-09-21 05:39:10,754 [INFO] Train step 150/274 - Loss: 0.542471 (MSE 0.044760, CE 1.288728, Corr 0.404625, Cons 0.033000, CSim 0.055407, Rank 0.619964) | LR: 1.423473e-04
2025-09-21 05:39:22,403 [INFO] Train step 200/274 - Loss: 0.537517 (MSE 0.044540, CE 1.276253, Corr 0.399560, Cons 0.031928, CSim 0.055278, Rank 0.619279) | LR: 1.325122e-04
2025-09-21 05:39:34,049 [INFO] Train step 250/274 - Loss: 0.534013 (MSE 0.043842, CE 1.269149, Corr 0.392708, Cons 0.031381, CSim 0.054782, Rank 0.618289) | LR: 1.223112e-04
2025-09-21 05:39:39,664 [INFO] Epoch training completed in 64.05s with average loss 0.532016
2025-09-21 05:39:39,665 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:39:41,134 [INFO] Fold 0 - Epoch 3 - Train Loss: 0.532016 | EMA Val MSE: 0.054660 | EMA Val CE: 1.422763 | EMA Val Pearson: 0.487734
2025-09-21 05:39:41,139 [INFO] Fold 0 - Epoch 3 - New best EMA model with Pearson 0.487734.
2025-09-21 05:39:41,139 [INFO] Fold 0 - Epoch 4/6 started.
2025-09-21 05:39:53,217 [INFO] Train step 50/274 - Loss: 0.479563 (MSE 0.035503, CE 1.148476, Corr 0.312142, Cons 0.026339, CSim 0.049797, Rank 0.601632) | LR: 1.067874e-04
2025-09-21 05:40:04,874 [INFO] Train step 100/274 - Loss: 0.474817 (MSE 0.035501, CE 1.136076, Corr 0.308264, Cons 0.025988, CSim 0.049573, Rank 0.600101) | LR: 9.618007e-05
2025-09-21 05:40:16,494 [INFO] Train step 150/274 - Loss: 0.474472 (MSE 0.035522, CE 1.135770, Corr 0.305705, Cons 0.025105, CSim 0.049776, Rank 0.599364) | LR: 8.561573e-05
2025-09-21 05:40:28,095 [INFO] Train step 200/274 - Loss: 0.471722 (MSE 0.035166, CE 1.129312, Corr 0.302277, Cons 0.024660, CSim 0.049780, Rank 0.599151) | LR: 7.521326e-05
2025-09-21 05:40:39,715 [INFO] Train step 250/274 - Loss: 0.469766 (MSE 0.034883, CE 1.124789, Corr 0.300122, Cons 0.024405, CSim 0.049614, Rank 0.598520) | LR: 6.508975e-05
2025-09-21 05:40:45,391 [INFO] Epoch training completed in 64.25s with average loss 0.469698
2025-09-21 05:40:45,392 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:40:46,921 [INFO] Fold 0 - Epoch 4 - Train Loss: 0.469698 | EMA Val MSE: 0.052810 | EMA Val CE: 1.353007 | EMA Val Pearson: 0.544141
2025-09-21 05:40:46,926 [INFO] Fold 0 - Epoch 4 - New best EMA model with Pearson 0.544141.
2025-09-21 05:40:46,926 [INFO] Fold 0 - Epoch 5/6 started.
2025-09-21 05:40:58,867 [INFO] Train step 50/274 - Loss: 0.434070 (MSE 0.030531, CE 1.040922, Corr 0.258423, Cons 0.022630, CSim 0.047544, Rank 0.589033) | LR: 5.086032e-05
2025-09-21 05:41:10,482 [INFO] Train step 100/274 - Loss: 0.429747 (MSE 0.030652, CE 1.028635, Corr 0.256876, Cons 0.022821, CSim 0.048115, Rank 0.589077) | LR: 4.191050e-05
2025-09-21 05:41:22,201 [INFO] Train step 150/274 - Loss: 0.432290 (MSE 0.030600, CE 1.035685, Corr 0.258012, Cons 0.022754, CSim 0.047637, Rank 0.589851) | LR: 3.361443e-05
2025-09-21 05:41:33,935 [INFO] Train step 200/274 - Loss: 0.434723 (MSE 0.030737, CE 1.041907, Corr 0.260227, Cons 0.022329, CSim 0.047436, Rank 0.590260) | LR: 2.606547e-05
2025-09-21 05:41:45,493 [INFO] Train step 250/274 - Loss: 0.433236 (MSE 0.030528, CE 1.038608, Corr 0.258063, Cons 0.022197, CSim 0.047187, Rank 0.589687) | LR: 1.934857e-05
2025-09-21 05:41:51,164 [INFO] Epoch training completed in 64.24s with average loss 0.433003
2025-09-21 05:41:51,165 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:41:52,679 [INFO] Fold 0 - Epoch 5 - Train Loss: 0.433003 | EMA Val MSE: 0.052600 | EMA Val CE: 1.330014 | EMA Val Pearson: 0.571051
2025-09-21 05:41:52,683 [INFO] Fold 0 - Epoch 5 - New best EMA model with Pearson 0.571051.
2025-09-21 05:41:52,683 [INFO] Fold 0 - Epoch 6/6 started.
2025-09-21 05:42:04,671 [INFO] Train step 50/274 - Loss: 0.419950 (MSE 0.028645, CE 1.007856, Corr 0.244015, Cons 0.020845, CSim 0.045251, Rank 0.584904) | LR: 1.109289e-05
2025-09-21 05:42:16,355 [INFO] Train step 100/274 - Loss: 0.419152 (MSE 0.028554, CE 1.005822, Corr 0.243590, Cons 0.020787, CSim 0.045474, Rank 0.584656) | LR: 6.743759e-06
2025-09-21 05:42:28,063 [INFO] Train step 150/274 - Loss: 0.414967 (MSE 0.028579, CE 0.994619, Corr 0.239942, Cons 0.020942, CSim 0.045901, Rank 0.584447) | LR: 3.444140e-06
2025-09-21 05:42:39,648 [INFO] Train step 200/274 - Loss: 0.413912 (MSE 0.028491, CE 0.992285, Corr 0.237781, Cons 0.020927, CSim 0.046075, Rank 0.583960) | LR: 1.231166e-06
2025-09-21 05:42:51,244 [INFO] Train step 250/274 - Loss: 0.413981 (MSE 0.028513, CE 0.992451, Corr 0.237572, Cons 0.020870, CSim 0.046060, Rank 0.584298) | LR: 1.297403e-07
2025-09-21 05:42:56,885 [INFO] Epoch training completed in 64.20s with average loss 0.415979
2025-09-21 05:42:56,886 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:42:58,391 [INFO] Fold 0 - Epoch 6 - Train Loss: 0.415979 | EMA Val MSE: 0.053821 | EMA Val CE: 1.338086 | EMA Val Pearson: 0.580486
2025-09-21 05:42:58,395 [INFO] Fold 0 - Epoch 6 - New best EMA model with Pearson 0.580486.
2025-09-21 05:42:59,657 [INFO] Fold 0 - Final Val Pearson: 0.580486 | Final Val MSE: 0.053821 | Final Val CE: 1.338086
2025-09-21 05:43:00,374 [INFO] ========== Fold 2/5 ==========
2025-09-21 05:43:02,158 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 05:43:02,164 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 05:43:02,164 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 05:43:02,164 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 05:43:02,167 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 05:43:02,315 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 05:43:02,315 [INFO] Initializing EMA state from current model weights.
2025-09-21 05:43:02,459 [INFO] Fold 1 - Epoch 1/6 started.
2025-09-21 05:43:14,386 [INFO] Train step 50/274 - Loss: 0.949943 (MSE 0.084636, CE 2.231026, Corr 0.971416, Cons 0.145699, CSim 0.142635, Rank 0.695502) | LR: 6.097561e-05
2025-09-21 05:43:26,083 [INFO] Train step 100/274 - Loss: 0.883260 (MSE 0.080420, CE 2.062598, Corr 0.927237, Cons 0.133388, CSim 0.114991, Rank 0.690470) | LR: 1.219512e-04
2025-09-21 05:43:37,682 [INFO] Train step 150/274 - Loss: 0.842962 (MSE 0.076676, CE 1.963712, Corr 0.896540, Cons 0.120621, CSim 0.099349, Rank 0.687769) | LR: 1.829268e-04
2025-09-21 05:43:49,320 [INFO] Train step 200/274 - Loss: 0.813551 (MSE 0.074399, CE 1.892426, Corr 0.865522, Cons 0.110247, CSim 0.091405, Rank 0.685307) | LR: 1.997082e-04
2025-09-21 05:44:00,897 [INFO] Train step 250/274 - Loss: 0.790180 (MSE 0.072429, CE 1.837775, Corr 0.833552, Cons 0.100177, CSim 0.086237, Rank 0.682458) | LR: 1.983384e-04
2025-09-21 05:44:06,521 [INFO] Epoch training completed in 64.06s with average loss 0.780721
2025-09-21 05:44:06,522 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:44:08,039 [INFO] Fold 1 - Epoch 1 - Train Loss: 0.780721 | EMA Val MSE: 0.065599 | EMA Val CE: 1.716558 | EMA Val Pearson: 0.185635
2025-09-21 05:44:08,042 [INFO] Fold 1 - Epoch 1 - New best EMA model with Pearson 0.185635.
2025-09-21 05:44:08,042 [INFO] Fold 1 - Epoch 2/6 started.
2025-09-21 05:44:20,104 [INFO] Train step 50/274 - Loss: 0.651996 (MSE 0.058764, CE 1.523777, Corr 0.614457, Cons 0.053322, CSim 0.062781, Rank 0.659842) | LR: 1.942877e-04
2025-09-21 05:44:31,799 [INFO] Train step 100/274 - Loss: 0.640877 (MSE 0.057799, CE 1.500489, Corr 0.586678, Cons 0.052891, CSim 0.062981, Rank 0.655519) | LR: 1.902280e-04
2025-09-21 05:44:43,398 [INFO] Train step 150/274 - Loss: 0.635697 (MSE 0.057300, CE 1.489642, Corr 0.575268, Cons 0.050148, CSim 0.062931, Rank 0.652720) | LR: 1.851529e-04
2025-09-21 05:44:55,048 [INFO] Train step 200/274 - Loss: 0.630436 (MSE 0.056543, CE 1.479383, Corr 0.561631, Cons 0.047105, CSim 0.062417, Rank 0.651202) | LR: 1.791195e-04
2025-09-21 05:45:06,606 [INFO] Train step 250/274 - Loss: 0.621943 (MSE 0.055553, CE 1.460497, Corr 0.546925, Cons 0.045738, CSim 0.062081, Rank 0.648852) | LR: 1.721956e-04
2025-09-21 05:45:12,212 [INFO] Epoch training completed in 64.17s with average loss 0.618339
2025-09-21 05:45:12,214 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:45:13,724 [INFO] Fold 1 - Epoch 2 - Train Loss: 0.618339 | EMA Val MSE: 0.055994 | EMA Val CE: 1.527662 | EMA Val Pearson: 0.408145
2025-09-21 05:45:13,728 [INFO] Fold 1 - Epoch 2 - New best EMA model with Pearson 0.408145.
2025-09-21 05:45:13,729 [INFO] Fold 1 - Epoch 3/6 started.
2025-09-21 05:45:25,661 [INFO] Train step 50/274 - Loss: 0.556762 (MSE 0.046353, CE 1.320074, Corr 0.429978, Cons 0.036601, CSim 0.056609, Rank 0.625629) | LR: 1.604825e-04
2025-09-21 05:45:37,249 [INFO] Train step 100/274 - Loss: 0.547170 (MSE 0.045358, CE 1.297793, Corr 0.416449, Cons 0.033665, CSim 0.056112, Rank 0.624001) | LR: 1.517058e-04
2025-09-21 05:45:48,799 [INFO] Train step 150/274 - Loss: 0.540966 (MSE 0.045063, CE 1.282935, Corr 0.407774, Cons 0.031446, CSim 0.056150, Rank 0.621917) | LR: 1.423473e-04
2025-09-21 05:46:00,362 [INFO] Train step 200/274 - Loss: 0.537773 (MSE 0.044396, CE 1.276687, Corr 0.401052, Cons 0.031088, CSim 0.055780, Rank 0.620396) | LR: 1.325122e-04
2025-09-21 05:46:11,942 [INFO] Train step 250/274 - Loss: 0.532300 (MSE 0.043643, CE 1.264602, Corr 0.391822, Cons 0.030367, CSim 0.055504, Rank 0.619144) | LR: 1.223112e-04
2025-09-21 05:46:17,551 [INFO] Epoch training completed in 63.82s with average loss 0.531900
2025-09-21 05:46:17,552 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:46:19,049 [INFO] Fold 1 - Epoch 3 - Train Loss: 0.531900 | EMA Val MSE: 0.051071 | EMA Val CE: 1.420510 | EMA Val Pearson: 0.501190
2025-09-21 05:46:19,052 [INFO] Fold 1 - Epoch 3 - New best EMA model with Pearson 0.501190.
2025-09-21 05:46:19,053 [INFO] Fold 1 - Epoch 4/6 started.
2025-09-21 05:46:31,053 [INFO] Train step 50/274 - Loss: 0.468034 (MSE 0.035552, CE 1.118584, Corr 0.298921, Cons 0.025301, CSim 0.051267, Rank 0.599871) | LR: 1.067874e-04
2025-09-21 05:46:42,603 [INFO] Train step 100/274 - Loss: 0.470281 (MSE 0.035453, CE 1.125473, Corr 0.299317, Cons 0.023929, CSim 0.051180, Rank 0.597670) | LR: 9.618007e-05
2025-09-21 05:46:54,177 [INFO] Train step 150/274 - Loss: 0.470480 (MSE 0.035372, CE 1.125647, Corr 0.301546, Cons 0.023637, CSim 0.050887, Rank 0.598164) | LR: 8.561573e-05
2025-09-21 05:47:05,855 [INFO] Train step 200/274 - Loss: 0.470410 (MSE 0.035199, CE 1.125635, Corr 0.301711, Cons 0.023760, CSim 0.050629, Rank 0.598692) | LR: 7.521326e-05
2025-09-21 05:47:17,534 [INFO] Train step 250/274 - Loss: 0.469158 (MSE 0.034925, CE 1.122890, Corr 0.299999, Cons 0.023863, CSim 0.050311, Rank 0.598906) | LR: 6.508975e-05
2025-09-21 05:47:23,187 [INFO] Epoch training completed in 64.13s with average loss 0.468934
2025-09-21 05:47:23,188 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:47:24,700 [INFO] Fold 1 - Epoch 4 - Train Loss: 0.468934 | EMA Val MSE: 0.048107 | EMA Val CE: 1.353202 | EMA Val Pearson: 0.553378
2025-09-21 05:47:24,704 [INFO] Fold 1 - Epoch 4 - New best EMA model with Pearson 0.553378.
2025-09-21 05:47:24,704 [INFO] Fold 1 - Epoch 5/6 started.
2025-09-21 05:47:36,712 [INFO] Train step 50/274 - Loss: 0.433826 (MSE 0.030762, CE 1.040152, Corr 0.257122, Cons 0.022266, CSim 0.048043, Rank 0.588727) | LR: 5.086032e-05
2025-09-21 05:47:48,282 [INFO] Train step 100/274 - Loss: 0.432422 (MSE 0.030343, CE 1.037564, Corr 0.254816, Cons 0.022189, CSim 0.047462, Rank 0.587671) | LR: 4.191050e-05
2025-09-21 05:47:59,842 [INFO] Train step 150/274 - Loss: 0.434566 (MSE 0.030459, CE 1.042668, Corr 0.258356, Cons 0.021739, CSim 0.047261, Rank 0.588275) | LR: 3.361443e-05
2025-09-21 05:48:11,382 [INFO] Train step 200/274 - Loss: 0.433523 (MSE 0.030370, CE 1.040329, Corr 0.256251, Cons 0.021294, CSim 0.047509, Rank 0.588062) | LR: 2.606547e-05
2025-09-21 05:48:23,052 [INFO] Train step 250/274 - Loss: 0.432084 (MSE 0.030284, CE 1.036716, Corr 0.254888, Cons 0.021214, CSim 0.047362, Rank 0.587878) | LR: 1.934857e-05
2025-09-21 05:48:28,672 [INFO] Epoch training completed in 63.97s with average loss 0.431747
2025-09-21 05:48:28,673 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:48:30,190 [INFO] Fold 1 - Epoch 5 - Train Loss: 0.431747 | EMA Val MSE: 0.047730 | EMA Val CE: 1.332093 | EMA Val Pearson: 0.577017
2025-09-21 05:48:30,195 [INFO] Fold 1 - Epoch 5 - New best EMA model with Pearson 0.577017.
2025-09-21 05:48:30,195 [INFO] Fold 1 - Epoch 6/6 started.
2025-09-21 05:48:42,104 [INFO] Train step 50/274 - Loss: 0.413867 (MSE 0.027913, CE 0.993911, Corr 0.234776, Cons 0.020502, CSim 0.044900, Rank 0.584176) | LR: 1.109289e-05
2025-09-21 05:48:53,700 [INFO] Train step 100/274 - Loss: 0.417884 (MSE 0.028325, CE 1.003271, Corr 0.240573, Cons 0.020344, CSim 0.045170, Rank 0.584991) | LR: 6.743759e-06
2025-09-21 05:49:05,276 [INFO] Train step 150/274 - Loss: 0.417086 (MSE 0.028343, CE 1.001102, Corr 0.239731, Cons 0.020391, CSim 0.045905, Rank 0.584604) | LR: 3.444140e-06
2025-09-21 05:49:16,843 [INFO] Train step 200/274 - Loss: 0.415383 (MSE 0.028355, CE 0.996859, Corr 0.237182, Cons 0.020634, CSim 0.046163, Rank 0.583847) | LR: 1.231166e-06
2025-09-21 05:49:28,376 [INFO] Train step 250/274 - Loss: 0.415890 (MSE 0.028506, CE 0.997545, Corr 0.239260, Cons 0.020656, CSim 0.046252, Rank 0.584092) | LR: 1.297403e-07
2025-09-21 05:49:33,989 [INFO] Epoch training completed in 63.79s with average loss 0.415775
2025-09-21 05:49:33,990 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:49:35,509 [INFO] Fold 1 - Epoch 6 - Train Loss: 0.415775 | EMA Val MSE: 0.049050 | EMA Val CE: 1.342252 | EMA Val Pearson: 0.584357
2025-09-21 05:49:35,523 [INFO] Fold 1 - Epoch 6 - New best EMA model with Pearson 0.584357.
2025-09-21 05:49:36,765 [INFO] Fold 1 - Final Val Pearson: 0.584357 | Final Val MSE: 0.049050 | Final Val CE: 1.342252
2025-09-21 05:49:37,457 [INFO] ========== Fold 3/5 ==========
2025-09-21 05:49:40,224 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 05:49:40,228 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 05:49:40,228 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 05:49:40,228 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 05:49:40,230 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 05:49:40,378 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 05:49:40,379 [INFO] Initializing EMA state from current model weights.
2025-09-21 05:49:40,522 [INFO] Fold 2 - Epoch 1/6 started.
2025-09-21 05:49:52,520 [INFO] Train step 50/274 - Loss: 1.023805 (MSE 0.095331, CE 2.429557, Corr 0.959928, Cons 0.140369, CSim 0.131874, Rank 0.696661) | LR: 6.097561e-05
2025-09-21 05:50:04,080 [INFO] Train step 100/274 - Loss: 0.923988 (MSE 0.085843, CE 2.173695, Corr 0.919431, Cons 0.131930, CSim 0.104458, Rank 0.691307) | LR: 1.219512e-04
2025-09-21 05:50:15,686 [INFO] Train step 150/274 - Loss: 0.869498 (MSE 0.080875, CE 2.035213, Corr 0.890530, Cons 0.122973, CSim 0.091963, Rank 0.687838) | LR: 1.829268e-04
2025-09-21 05:50:27,273 [INFO] Train step 200/274 - Loss: 0.834773 (MSE 0.077435, CE 1.949865, Corr 0.861769, Cons 0.112605, CSim 0.085371, Rank 0.685384) | LR: 1.997082e-04
2025-09-21 05:50:38,864 [INFO] Train step 250/274 - Loss: 0.808207 (MSE 0.075106, CE 1.885367, Corr 0.833933, Cons 0.104933, CSim 0.081380, Rank 0.682768) | LR: 1.983384e-04
2025-09-21 05:50:44,473 [INFO] Epoch training completed in 63.95s with average loss 0.797136
2025-09-21 05:50:44,474 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:50:45,978 [INFO] Fold 2 - Epoch 1 - Train Loss: 0.797136 | EMA Val MSE: 0.070914 | EMA Val CE: 1.762958 | EMA Val Pearson: 0.202369
2025-09-21 05:50:45,982 [INFO] Fold 2 - Epoch 1 - New best EMA model with Pearson 0.202369.
2025-09-21 05:50:45,982 [INFO] Fold 2 - Epoch 2/6 started.
2025-09-21 05:50:58,017 [INFO] Train step 50/274 - Loss: 0.656652 (MSE 0.059926, CE 1.532458, Corr 0.624207, Cons 0.057492, CSim 0.063924, Rank 0.660999) | LR: 1.942877e-04
2025-09-21 05:51:09,687 [INFO] Train step 100/274 - Loss: 0.648254 (MSE 0.058935, CE 1.514387, Corr 0.608399, Cons 0.055117, CSim 0.062600, Rank 0.658007) | LR: 1.902280e-04
2025-09-21 05:51:21,357 [INFO] Train step 150/274 - Loss: 0.641049 (MSE 0.058069, CE 1.499459, Corr 0.591956, Cons 0.052982, CSim 0.062371, Rank 0.655313) | LR: 1.851529e-04
2025-09-21 05:51:33,060 [INFO] Train step 200/274 - Loss: 0.630790 (MSE 0.056678, CE 1.478402, Corr 0.569065, Cons 0.051131, CSim 0.061651, Rank 0.650929) | LR: 1.791195e-04
2025-09-21 05:51:44,781 [INFO] Train step 250/274 - Loss: 0.623466 (MSE 0.055997, CE 1.462396, Corr 0.554479, Cons 0.049100, CSim 0.061189, Rank 0.648580) | LR: 1.721956e-04
2025-09-21 05:51:50,394 [INFO] Epoch training completed in 64.41s with average loss 0.619860
2025-09-21 05:51:50,395 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:51:51,924 [INFO] Fold 2 - Epoch 2 - Train Loss: 0.619860 | EMA Val MSE: 0.056869 | EMA Val CE: 1.523000 | EMA Val Pearson: 0.405786
2025-09-21 05:51:51,928 [INFO] Fold 2 - Epoch 2 - New best EMA model with Pearson 0.405786.
2025-09-21 05:51:51,928 [INFO] Fold 2 - Epoch 3/6 started.
2025-09-21 05:52:03,854 [INFO] Train step 50/274 - Loss: 0.557988 (MSE 0.047229, CE 1.322031, Corr 0.432268, Cons 0.036271, CSim 0.055611, Rank 0.624272) | LR: 1.604825e-04
2025-09-21 05:52:15,549 [INFO] Train step 100/274 - Loss: 0.548115 (MSE 0.045837, CE 1.300440, Corr 0.416343, Cons 0.033669, CSim 0.054834, Rank 0.620556) | LR: 1.517058e-04
2025-09-21 05:52:27,186 [INFO] Train step 150/274 - Loss: 0.544741 (MSE 0.045286, CE 1.292592, Corr 0.412527, Cons 0.033553, CSim 0.054421, Rank 0.620647) | LR: 1.423473e-04
2025-09-21 05:52:38,735 [INFO] Train step 200/274 - Loss: 0.539683 (MSE 0.044661, CE 1.281342, Corr 0.403885, Cons 0.033010, CSim 0.054381, Rank 0.619187) | LR: 1.325122e-04
2025-09-21 05:52:50,400 [INFO] Train step 250/274 - Loss: 0.534350 (MSE 0.044104, CE 1.268895, Corr 0.396639, Cons 0.032420, CSim 0.054181, Rank 0.617814) | LR: 1.223112e-04
2025-09-21 05:52:56,004 [INFO] Epoch training completed in 64.08s with average loss 0.533855
2025-09-21 05:52:56,005 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:52:57,508 [INFO] Fold 2 - Epoch 3 - Train Loss: 0.533855 | EMA Val MSE: 0.051891 | EMA Val CE: 1.423107 | EMA Val Pearson: 0.504924
2025-09-21 05:52:57,512 [INFO] Fold 2 - Epoch 3 - New best EMA model with Pearson 0.504924.
2025-09-21 05:52:57,512 [INFO] Fold 2 - Epoch 4/6 started.
2025-09-21 05:53:09,520 [INFO] Train step 50/274 - Loss: 0.479825 (MSE 0.036342, CE 1.148823, Corr 0.310517, Cons 0.025204, CSim 0.049639, Rank 0.598429) | LR: 1.067874e-04
2025-09-21 05:53:21,171 [INFO] Train step 100/274 - Loss: 0.481656 (MSE 0.036711, CE 1.151765, Corr 0.315944, Cons 0.026620, CSim 0.050018, Rank 0.600762) | LR: 9.618007e-05
2025-09-21 05:53:32,828 [INFO] Train step 150/274 - Loss: 0.478289 (MSE 0.036178, CE 1.144159, Corr 0.311590, Cons 0.026229, CSim 0.049492, Rank 0.600175) | LR: 8.561573e-05
2025-09-21 05:53:44,495 [INFO] Train step 200/274 - Loss: 0.475929 (MSE 0.035945, CE 1.138240, Corr 0.310132, Cons 0.025596, CSim 0.049000, Rank 0.600068) | LR: 7.521326e-05
2025-09-21 05:53:56,187 [INFO] Train step 250/274 - Loss: 0.474065 (MSE 0.035726, CE 1.133720, Corr 0.308296, Cons 0.025280, CSim 0.048909, Rank 0.599990) | LR: 6.508975e-05
2025-09-21 05:54:01,894 [INFO] Epoch training completed in 64.38s with average loss 0.472420
2025-09-21 05:54:01,895 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:54:03,376 [INFO] Fold 2 - Epoch 4 - Train Loss: 0.472420 | EMA Val MSE: 0.049814 | EMA Val CE: 1.358809 | EMA Val Pearson: 0.560223
2025-09-21 05:54:03,380 [INFO] Fold 2 - Epoch 4 - New best EMA model with Pearson 0.560223.
2025-09-21 05:54:03,380 [INFO] Fold 2 - Epoch 5/6 started.
2025-09-21 05:54:15,396 [INFO] Train step 50/274 - Loss: 0.445588 (MSE 0.031809, CE 1.067162, Corr 0.275430, Cons 0.023605, CSim 0.046685, Rank 0.594917) | LR: 5.086032e-05
2025-09-21 05:54:27,049 [INFO] Train step 100/274 - Loss: 0.441505 (MSE 0.031432, CE 1.058258, Corr 0.267863, Cons 0.023094, CSim 0.046651, Rank 0.592064) | LR: 4.191050e-05
2025-09-21 05:54:38,753 [INFO] Train step 150/274 - Loss: 0.439775 (MSE 0.031503, CE 1.053408, Corr 0.267483, Cons 0.022561, CSim 0.046600, Rank 0.591510) | LR: 3.361443e-05
2025-09-21 05:54:50,390 [INFO] Train step 200/274 - Loss: 0.438147 (MSE 0.031330, CE 1.049583, Corr 0.265484, Cons 0.022453, CSim 0.046630, Rank 0.590833) | LR: 2.606547e-05
2025-09-21 05:55:02,080 [INFO] Train step 250/274 - Loss: 0.435922 (MSE 0.031135, CE 1.044413, Corr 0.262120, Cons 0.022312, CSim 0.046640, Rank 0.590094) | LR: 1.934857e-05
2025-09-21 05:55:07,753 [INFO] Epoch training completed in 64.37s with average loss 0.435936
2025-09-21 05:55:07,754 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:55:09,268 [INFO] Fold 2 - Epoch 5 - Train Loss: 0.435936 | EMA Val MSE: 0.049483 | EMA Val CE: 1.333132 | EMA Val Pearson: 0.587073
2025-09-21 05:55:09,272 [INFO] Fold 2 - Epoch 5 - New best EMA model with Pearson 0.587073.
2025-09-21 05:55:09,272 [INFO] Fold 2 - Epoch 6/6 started.
2025-09-21 05:55:21,281 [INFO] Train step 50/274 - Loss: 0.418667 (MSE 0.029292, CE 1.004034, Corr 0.241046, Cons 0.021193, CSim 0.046729, Rank 0.581999) | LR: 1.109289e-05
2025-09-21 05:55:32,886 [INFO] Train step 100/274 - Loss: 0.422364 (MSE 0.029223, CE 1.013438, Corr 0.245740, Cons 0.021166, CSim 0.045652, Rank 0.584465) | LR: 6.743759e-06
2025-09-21 05:55:44,481 [INFO] Train step 150/274 - Loss: 0.422576 (MSE 0.029449, CE 1.012891, Corr 0.248289, Cons 0.021159, CSim 0.045337, Rank 0.586272) | LR: 3.444140e-06
2025-09-21 05:55:56,016 [INFO] Train step 200/274 - Loss: 0.421908 (MSE 0.029341, CE 1.011464, Corr 0.247001, Cons 0.021175, CSim 0.045318, Rank 0.586168) | LR: 1.231166e-06
2025-09-21 05:56:07,572 [INFO] Train step 250/274 - Loss: 0.421266 (MSE 0.029292, CE 1.009898, Corr 0.246322, Cons 0.021184, CSim 0.045355, Rank 0.585866) | LR: 1.297403e-07
2025-09-21 05:56:13,217 [INFO] Epoch training completed in 63.94s with average loss 0.420868
2025-09-21 05:56:13,218 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:56:14,715 [INFO] Fold 2 - Epoch 6 - Train Loss: 0.420868 | EMA Val MSE: 0.050165 | EMA Val CE: 1.338712 | EMA Val Pearson: 0.597143
2025-09-21 05:56:14,719 [INFO] Fold 2 - Epoch 6 - New best EMA model with Pearson 0.597143.
2025-09-21 05:56:15,966 [INFO] Fold 2 - Final Val Pearson: 0.597143 | Final Val MSE: 0.050165 | Final Val CE: 1.338712
2025-09-21 05:56:16,635 [INFO] ========== Fold 4/5 ==========
2025-09-21 05:56:17,049 [INFO] Applied swap augmentation. New train size: 52522
2025-09-21 05:56:17,054 [INFO] Train split: 52522 samples; Val split: 6564 samples.
2025-09-21 05:56:17,054 [INFO] Initializing PatentDataset with 52522 samples.
2025-09-21 05:56:17,054 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 05:56:17,056 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 05:56:17,204 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 05:56:17,204 [INFO] Initializing EMA state from current model weights.
2025-09-21 05:56:17,350 [INFO] Fold 3 - Epoch 1/6 started.
2025-09-21 05:56:29,262 [INFO] Train step 50/274 - Loss: 0.974297 (MSE 0.089664, CE 2.300262, Corr 0.957283, Cons 0.156748, CSim 0.110726, Rank 0.692731) | LR: 6.097561e-05
2025-09-21 05:56:40,841 [INFO] Train step 100/274 - Loss: 0.894855 (MSE 0.083329, CE 2.091530, Corr 0.934666, Cons 0.147103, CSim 0.094983, Rank 0.690480) | LR: 1.219512e-04
2025-09-21 05:56:52,378 [INFO] Train step 150/274 - Loss: 0.848962 (MSE 0.079463, CE 1.977870, Corr 0.895185, Cons 0.134700, CSim 0.086452, Rank 0.687424) | LR: 1.829268e-04
2025-09-21 05:57:03,949 [INFO] Train step 200/274 - Loss: 0.819580 (MSE 0.076616, CE 1.906918, Corr 0.865452, Cons 0.122687, CSim 0.081320, Rank 0.685283) | LR: 1.997082e-04
2025-09-21 05:57:15,506 [INFO] Train step 250/274 - Loss: 0.797091 (MSE 0.074236, CE 1.853818, Corr 0.839400, Cons 0.111919, CSim 0.078021, Rank 0.682662) | LR: 1.983384e-04
2025-09-21 05:57:21,081 [INFO] Epoch training completed in 63.73s with average loss 0.788258
2025-09-21 05:57:21,082 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:57:22,568 [INFO] Fold 3 - Epoch 1 - Train Loss: 0.788258 | EMA Val MSE: 0.071476 | EMA Val CE: 1.698814 | EMA Val Pearson: 0.147331
2025-09-21 05:57:22,572 [INFO] Fold 3 - Epoch 1 - New best EMA model with Pearson 0.147331.
2025-09-21 05:57:22,572 [INFO] Fold 3 - Epoch 2/6 started.
2025-09-21 05:57:34,670 [INFO] Train step 50/274 - Loss: 0.661070 (MSE 0.060520, CE 1.544195, Corr 0.624886, Cons 0.057627, CSim 0.063052, Rank 0.660395) | LR: 1.942877e-04
2025-09-21 05:57:46,293 [INFO] Train step 100/274 - Loss: 0.653783 (MSE 0.058983, CE 1.530639, Corr 0.606792, Cons 0.054291, CSim 0.062552, Rank 0.657232) | LR: 1.902280e-04
2025-09-21 05:57:57,956 [INFO] Train step 150/274 - Loss: 0.644882 (MSE 0.058035, CE 1.511418, Corr 0.588221, Cons 0.052142, CSim 0.062243, Rank 0.655072) | LR: 1.851529e-04
2025-09-21 05:58:09,618 [INFO] Train step 200/274 - Loss: 0.638089 (MSE 0.057090, CE 1.497576, Corr 0.572684, Cons 0.050058, CSim 0.061701, Rank 0.652742) | LR: 1.791195e-04
2025-09-21 05:58:21,385 [INFO] Train step 250/274 - Loss: 0.631435 (MSE 0.056260, CE 1.482976, Corr 0.560832, Cons 0.049075, CSim 0.061282, Rank 0.650755) | LR: 1.721956e-04
2025-09-21 05:58:27,096 [INFO] Epoch training completed in 64.52s with average loss 0.626887
2025-09-21 05:58:27,097 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:58:28,598 [INFO] Fold 3 - Epoch 2 - Train Loss: 0.626887 | EMA Val MSE: 0.059411 | EMA Val CE: 1.532784 | EMA Val Pearson: 0.374120
2025-09-21 05:58:28,603 [INFO] Fold 3 - Epoch 2 - New best EMA model with Pearson 0.374120.
2025-09-21 05:58:28,603 [INFO] Fold 3 - Epoch 3/6 started.
2025-09-21 05:58:40,678 [INFO] Train step 50/274 - Loss: 0.553961 (MSE 0.044704, CE 1.319478, Corr 0.410999, Cons 0.035092, CSim 0.055235, Rank 0.624252) | LR: 1.604825e-04
2025-09-21 05:58:52,254 [INFO] Train step 100/274 - Loss: 0.542497 (MSE 0.044286, CE 1.290132, Corr 0.400500, Cons 0.034871, CSim 0.055493, Rank 0.621628) | LR: 1.517058e-04
2025-09-21 05:59:03,870 [INFO] Train step 150/274 - Loss: 0.538864 (MSE 0.043664, CE 1.282624, Corr 0.393928, Cons 0.033510, CSim 0.055054, Rank 0.619868) | LR: 1.423473e-04
2025-09-21 05:59:15,444 [INFO] Train step 200/274 - Loss: 0.536142 (MSE 0.043270, CE 1.276632, Corr 0.389768, Cons 0.032665, CSim 0.055049, Rank 0.618708) | LR: 1.325122e-04
2025-09-21 05:59:27,120 [INFO] Train step 250/274 - Loss: 0.532284 (MSE 0.042723, CE 1.268179, Corr 0.383393, Cons 0.031800, CSim 0.054764, Rank 0.617567) | LR: 1.223112e-04
2025-09-21 05:59:32,785 [INFO] Epoch training completed in 64.18s with average loss 0.530565
2025-09-21 05:59:32,786 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 05:59:34,276 [INFO] Fold 3 - Epoch 3 - Train Loss: 0.530565 | EMA Val MSE: 0.054990 | EMA Val CE: 1.431419 | EMA Val Pearson: 0.488119
2025-09-21 05:59:34,280 [INFO] Fold 3 - Epoch 3 - New best EMA model with Pearson 0.488119.
2025-09-21 05:59:34,280 [INFO] Fold 3 - Epoch 4/6 started.
2025-09-21 05:59:46,180 [INFO] Train step 50/274 - Loss: 0.467987 (MSE 0.034215, CE 1.122253, Corr 0.293120, Cons 0.024969, CSim 0.050087, Rank 0.598546) | LR: 1.067874e-04
2025-09-21 05:59:57,841 [INFO] Train step 100/274 - Loss: 0.466692 (MSE 0.034057, CE 1.119113, Corr 0.293138, Cons 0.024176, CSim 0.049630, Rank 0.597094) | LR: 9.618007e-05
2025-09-21 06:00:09,392 [INFO] Train step 150/274 - Loss: 0.465276 (MSE 0.034003, CE 1.115801, Corr 0.290418, Cons 0.024652, CSim 0.049774, Rank 0.596583) | LR: 8.561573e-05
2025-09-21 06:00:21,095 [INFO] Train step 200/274 - Loss: 0.465055 (MSE 0.034016, CE 1.115190, Corr 0.290571, Cons 0.024253, CSim 0.049623, Rank 0.596354) | LR: 7.521326e-05
2025-09-21 06:00:32,713 [INFO] Train step 250/274 - Loss: 0.465076 (MSE 0.033948, CE 1.115343, Corr 0.290936, Cons 0.024080, CSim 0.049509, Rank 0.596055) | LR: 6.508975e-05
2025-09-21 06:00:38,393 [INFO] Epoch training completed in 64.11s with average loss 0.464238
2025-09-21 06:00:38,393 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:00:39,895 [INFO] Fold 3 - Epoch 4 - Train Loss: 0.464238 | EMA Val MSE: 0.052754 | EMA Val CE: 1.359851 | EMA Val Pearson: 0.542819
2025-09-21 06:00:39,899 [INFO] Fold 3 - Epoch 4 - New best EMA model with Pearson 0.542819.
2025-09-21 06:00:39,899 [INFO] Fold 3 - Epoch 5/6 started.
2025-09-21 06:00:51,881 [INFO] Train step 50/274 - Loss: 0.431348 (MSE 0.030566, CE 1.034106, Corr 0.253038, Cons 0.024741, CSim 0.048502, Rank 0.588734) | LR: 5.086032e-05
2025-09-21 06:01:03,561 [INFO] Train step 100/274 - Loss: 0.432143 (MSE 0.030074, CE 1.037919, Corr 0.251918, Cons 0.022894, CSim 0.048010, Rank 0.586383) | LR: 4.191050e-05
2025-09-21 06:01:15,233 [INFO] Train step 150/274 - Loss: 0.430253 (MSE 0.030059, CE 1.032924, Corr 0.250836, Cons 0.022415, CSim 0.047679, Rank 0.585970) | LR: 3.361443e-05
2025-09-21 06:01:26,784 [INFO] Train step 200/274 - Loss: 0.430329 (MSE 0.029839, CE 1.033732, Corr 0.249650, Cons 0.022172, CSim 0.047503, Rank 0.586424) | LR: 2.606547e-05
2025-09-21 06:01:38,350 [INFO] Train step 250/274 - Loss: 0.429374 (MSE 0.029721, CE 1.031462, Corr 0.248626, Cons 0.022068, CSim 0.047432, Rank 0.586250) | LR: 1.934857e-05
2025-09-21 06:01:44,024 [INFO] Epoch training completed in 64.12s with average loss 0.429718
2025-09-21 06:01:44,024 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:01:45,517 [INFO] Fold 3 - Epoch 5 - Train Loss: 0.429718 | EMA Val MSE: 0.052492 | EMA Val CE: 1.336207 | EMA Val Pearson: 0.568425
2025-09-21 06:01:45,522 [INFO] Fold 3 - Epoch 5 - New best EMA model with Pearson 0.568425.
2025-09-21 06:01:45,522 [INFO] Fold 3 - Epoch 6/6 started.
2025-09-21 06:01:57,591 [INFO] Train step 50/274 - Loss: 0.414634 (MSE 0.027322, CE 0.998816, Corr 0.229070, Cons 0.019678, CSim 0.045343, Rank 0.580688) | LR: 1.109289e-05
2025-09-21 06:02:09,249 [INFO] Train step 100/274 - Loss: 0.414356 (MSE 0.027593, CE 0.997263, Corr 0.229625, Cons 0.020048, CSim 0.046009, Rank 0.581331) | LR: 6.743759e-06
2025-09-21 06:02:20,912 [INFO] Train step 150/274 - Loss: 0.414650 (MSE 0.027826, CE 0.997191, Corr 0.231243, Cons 0.019975, CSim 0.046235, Rank 0.582365) | LR: 3.444140e-06
2025-09-21 06:02:32,545 [INFO] Train step 200/274 - Loss: 0.414550 (MSE 0.027779, CE 0.997025, Corr 0.231213, Cons 0.019973, CSim 0.046047, Rank 0.582289) | LR: 1.231166e-06
2025-09-21 06:02:44,196 [INFO] Train step 250/274 - Loss: 0.413490 (MSE 0.027845, CE 0.993868, Corr 0.230984, Cons 0.020019, CSim 0.046369, Rank 0.582481) | LR: 1.297403e-07
2025-09-21 06:02:49,788 [INFO] Epoch training completed in 64.27s with average loss 0.413359
2025-09-21 06:02:49,789 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:02:51,262 [INFO] Fold 3 - Epoch 6 - Train Loss: 0.413359 | EMA Val MSE: 0.053423 | EMA Val CE: 1.345536 | EMA Val Pearson: 0.578466
2025-09-21 06:02:51,266 [INFO] Fold 3 - Epoch 6 - New best EMA model with Pearson 0.578466.
2025-09-21 06:02:52,513 [INFO] Fold 3 - Final Val Pearson: 0.578466 | Final Val MSE: 0.053423 | Final Val CE: 1.345536
2025-09-21 06:02:53,209 [INFO] ========== Fold 5/5 ==========
2025-09-21 06:02:53,267 [INFO] Applied swap augmentation. New train size: 52526
2025-09-21 06:02:53,272 [INFO] Train split: 52526 samples; Val split: 6562 samples.
2025-09-21 06:02:53,272 [INFO] Initializing PatentDataset with 52526 samples.
2025-09-21 06:02:53,272 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 06:02:53,273 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 06:02:53,424 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 06:02:53,424 [INFO] Initializing EMA state from current model weights.
2025-09-21 06:02:53,574 [INFO] Fold 4 - Epoch 1/6 started.
2025-09-21 06:03:05,556 [INFO] Train step 50/274 - Loss: 0.989434 (MSE 0.092317, CE 2.326964, Corr 0.993194, Cons 0.153945, CSim 0.136699, Rank 0.697065) | LR: 6.097561e-05
2025-09-21 06:03:17,155 [INFO] Train step 100/274 - Loss: 0.904674 (MSE 0.084372, CE 2.111845, Corr 0.949620, Cons 0.145213, CSim 0.112038, Rank 0.692966) | LR: 1.219512e-04
2025-09-21 06:03:28,973 [INFO] Train step 150/274 - Loss: 0.855799 (MSE 0.079518, CE 1.992945, Corr 0.907342, Cons 0.131457, CSim 0.097380, Rank 0.688959) | LR: 1.829268e-04
2025-09-21 06:03:40,682 [INFO] Train step 200/274 - Loss: 0.824937 (MSE 0.076748, CE 1.918500, Corr 0.875812, Cons 0.120260, CSim 0.089718, Rank 0.685879) | LR: 1.997082e-04
2025-09-21 06:03:52,418 [INFO] Train step 250/274 - Loss: 0.802035 (MSE 0.074618, CE 1.863377, Corr 0.852032, Cons 0.110252, CSim 0.085055, Rank 0.683863) | LR: 1.983384e-04
2025-09-21 06:03:58,059 [INFO] Epoch training completed in 64.48s with average loss 0.792317
2025-09-21 06:03:58,059 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:03:59,535 [INFO] Fold 4 - Epoch 1 - Train Loss: 0.792317 | EMA Val MSE: 0.070980 | EMA Val CE: 1.741484 | EMA Val Pearson: 0.161069
2025-09-21 06:03:59,539 [INFO] Fold 4 - Epoch 1 - New best EMA model with Pearson 0.161069.
2025-09-21 06:03:59,539 [INFO] Fold 4 - Epoch 2/6 started.
2025-09-21 06:04:11,460 [INFO] Train step 50/274 - Loss: 0.654529 (MSE 0.060567, CE 1.527100, Corr 0.616397, Cons 0.058706, CSim 0.063858, Rank 0.661074) | LR: 1.942877e-04
2025-09-21 06:04:23,073 [INFO] Train step 100/274 - Loss: 0.648003 (MSE 0.058412, CE 1.516793, Corr 0.599458, Cons 0.053960, CSim 0.062324, Rank 0.656945) | LR: 1.902280e-04
2025-09-21 06:04:34,677 [INFO] Train step 150/274 - Loss: 0.640718 (MSE 0.057564, CE 1.501709, Corr 0.583231, Cons 0.051724, CSim 0.061914, Rank 0.653418) | LR: 1.851529e-04
2025-09-21 06:04:46,254 [INFO] Train step 200/274 - Loss: 0.631262 (MSE 0.056768, CE 1.480418, Corr 0.565786, Cons 0.049935, CSim 0.061837, Rank 0.650806) | LR: 1.791195e-04
2025-09-21 06:04:57,881 [INFO] Train step 250/274 - Loss: 0.625295 (MSE 0.055732, CE 1.468738, Corr 0.551750, Cons 0.047696, CSim 0.061155, Rank 0.648660) | LR: 1.721956e-04
2025-09-21 06:05:03,540 [INFO] Epoch training completed in 64.00s with average loss 0.621669
2025-09-21 06:05:03,541 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:05:05,042 [INFO] Fold 4 - Epoch 2 - Train Loss: 0.621669 | EMA Val MSE: 0.059633 | EMA Val CE: 1.528227 | EMA Val Pearson: 0.392355
2025-09-21 06:05:05,046 [INFO] Fold 4 - Epoch 2 - New best EMA model with Pearson 0.392355.
2025-09-21 06:05:05,046 [INFO] Fold 4 - Epoch 3/6 started.
2025-09-21 06:05:17,048 [INFO] Train step 50/274 - Loss: 0.544393 (MSE 0.044044, CE 1.297615, Corr 0.396173, Cons 0.033208, CSim 0.054781, Rank 0.618141) | LR: 1.604825e-04
2025-09-21 06:05:28,635 [INFO] Train step 100/274 - Loss: 0.541729 (MSE 0.044631, CE 1.287711, Corr 0.401698, Cons 0.033378, CSim 0.054429, Rank 0.619168) | LR: 1.517058e-04
2025-09-21 06:05:40,221 [INFO] Train step 150/274 - Loss: 0.537393 (MSE 0.043830, CE 1.278778, Corr 0.393440, Cons 0.032380, CSim 0.054281, Rank 0.617541) | LR: 1.423473e-04
2025-09-21 06:05:51,780 [INFO] Train step 200/274 - Loss: 0.534835 (MSE 0.043398, CE 1.273298, Corr 0.389004, Cons 0.031983, CSim 0.054051, Rank 0.616977) | LR: 1.325122e-04
2025-09-21 06:06:03,343 [INFO] Train step 250/274 - Loss: 0.531653 (MSE 0.043071, CE 1.265791, Corr 0.384931, Cons 0.031537, CSim 0.053872, Rank 0.616361) | LR: 1.223112e-04
2025-09-21 06:06:09,065 [INFO] Epoch training completed in 64.02s with average loss 0.530023
2025-09-21 06:06:09,065 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:06:10,542 [INFO] Fold 4 - Epoch 3 - Train Loss: 0.530023 | EMA Val MSE: 0.052033 | EMA Val CE: 1.409351 | EMA Val Pearson: 0.505548
2025-09-21 06:06:10,546 [INFO] Fold 4 - Epoch 3 - New best EMA model with Pearson 0.505548.
2025-09-21 06:06:10,546 [INFO] Fold 4 - Epoch 4/6 started.
2025-09-21 06:06:22,567 [INFO] Train step 50/274 - Loss: 0.476994 (MSE 0.035410, CE 1.143310, Corr 0.305963, Cons 0.025361, CSim 0.049542, Rank 0.597977) | LR: 1.067874e-04
2025-09-21 06:06:34,130 [INFO] Train step 100/274 - Loss: 0.477486 (MSE 0.035350, CE 1.145172, Corr 0.303743, Cons 0.025606, CSim 0.049208, Rank 0.599237) | LR: 9.618007e-05
2025-09-21 06:06:45,753 [INFO] Train step 150/274 - Loss: 0.476515 (MSE 0.035255, CE 1.142711, Corr 0.303160, Cons 0.025021, CSim 0.049151, Rank 0.599296) | LR: 8.561573e-05
2025-09-21 06:06:57,467 [INFO] Train step 200/274 - Loss: 0.475431 (MSE 0.035045, CE 1.140172, Corr 0.302260, Cons 0.025167, CSim 0.049133, Rank 0.599100) | LR: 7.521326e-05
2025-09-21 06:07:09,145 [INFO] Train step 250/274 - Loss: 0.472961 (MSE 0.035056, CE 1.133447, Corr 0.301181, Cons 0.025088, CSim 0.049050, Rank 0.598503) | LR: 6.508975e-05
2025-09-21 06:07:14,770 [INFO] Epoch training completed in 64.22s with average loss 0.471992
2025-09-21 06:07:14,771 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:07:16,280 [INFO] Fold 4 - Epoch 4 - Train Loss: 0.471992 | EMA Val MSE: 0.050389 | EMA Val CE: 1.345021 | EMA Val Pearson: 0.551369
2025-09-21 06:07:16,283 [INFO] Fold 4 - Epoch 4 - New best EMA model with Pearson 0.551369.
2025-09-21 06:07:16,283 [INFO] Fold 4 - Epoch 5/6 started.
2025-09-21 06:07:28,256 [INFO] Train step 50/274 - Loss: 0.446433 (MSE 0.030835, CE 1.074684, Corr 0.263419, Cons 0.022644, CSim 0.046230, Rank 0.589937) | LR: 5.086032e-05
2025-09-21 06:07:39,960 [INFO] Train step 100/274 - Loss: 0.444014 (MSE 0.031241, CE 1.066738, Corr 0.265724, Cons 0.022985, CSim 0.046300, Rank 0.588820) | LR: 4.191050e-05
2025-09-21 06:07:51,543 [INFO] Train step 150/274 - Loss: 0.441580 (MSE 0.031586, CE 1.058999, Corr 0.266651, Cons 0.023154, CSim 0.046663, Rank 0.588600) | LR: 3.361443e-05
2025-09-21 06:08:03,186 [INFO] Train step 200/274 - Loss: 0.440900 (MSE 0.031510, CE 1.057240, Corr 0.265895, Cons 0.023313, CSim 0.046736, Rank 0.589224) | LR: 2.606547e-05
2025-09-21 06:08:14,882 [INFO] Train step 250/274 - Loss: 0.439644 (MSE 0.031373, CE 1.054162, Corr 0.264488, Cons 0.023407, CSim 0.046783, Rank 0.589322) | LR: 1.934857e-05
2025-09-21 06:08:20,552 [INFO] Epoch training completed in 64.27s with average loss 0.438935
2025-09-21 06:08:20,553 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:08:22,050 [INFO] Fold 4 - Epoch 5 - Train Loss: 0.438935 | EMA Val MSE: 0.050961 | EMA Val CE: 1.329358 | EMA Val Pearson: 0.571813
2025-09-21 06:08:22,054 [INFO] Fold 4 - Epoch 5 - New best EMA model with Pearson 0.571813.
2025-09-21 06:08:22,054 [INFO] Fold 4 - Epoch 6/6 started.
2025-09-21 06:08:33,922 [INFO] Train step 50/274 - Loss: 0.422394 (MSE 0.028756, CE 1.014914, Corr 0.241834, Cons 0.022189, CSim 0.045084, Rank 0.586272) | LR: 1.109289e-05
2025-09-21 06:08:45,557 [INFO] Train step 100/274 - Loss: 0.419187 (MSE 0.029069, CE 1.005140, Corr 0.242653, Cons 0.022369, CSim 0.045972, Rank 0.584828) | LR: 6.743759e-06
2025-09-21 06:08:57,248 [INFO] Train step 150/274 - Loss: 0.419102 (MSE 0.028783, CE 1.005861, Corr 0.240828, Cons 0.022306, CSim 0.045756, Rank 0.584398) | LR: 3.444140e-06
2025-09-21 06:09:08,957 [INFO] Train step 200/274 - Loss: 0.419417 (MSE 0.028631, CE 1.007293, Corr 0.239537, Cons 0.022323, CSim 0.045754, Rank 0.584415) | LR: 1.231166e-06
2025-09-21 06:09:20,607 [INFO] Train step 250/274 - Loss: 0.419358 (MSE 0.028683, CE 1.006898, Corr 0.239982, Cons 0.022351, CSim 0.045850, Rank 0.584602) | LR: 1.297403e-07
2025-09-21 06:09:26,258 [INFO] Epoch training completed in 64.20s with average loss 0.420175
2025-09-21 06:09:26,259 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:09:27,743 [INFO] Fold 4 - Epoch 6 - Train Loss: 0.420175 | EMA Val MSE: 0.052065 | EMA Val CE: 1.343027 | EMA Val Pearson: 0.579275
2025-09-21 06:09:27,747 [INFO] Fold 4 - Epoch 6 - New best EMA model with Pearson 0.579275.
2025-09-21 06:09:28,975 [INFO] Fold 4 - Final Val Pearson: 0.579275 | Final Val MSE: 0.052065 | Final Val CE: 1.343027
2025-09-21 06:09:29,667 [INFO] OOF Pearson correlation across all folds (pre-rule): 0.582847
2025-09-21 06:09:29,667 [INFO] OOF MSE across all folds (pre-rule): 0.051637
2025-09-21 06:09:29,667 [INFO] Ensembling test predictions from all folds by averaging.
2025-09-21 06:09:29,668 [INFO] Applying rule-based post-processing for anchor == target -> score = 1.0
2025-09-21 06:09:29,669 [INFO] Found 24 exact anchor==target pairs in test set.
2025-09-21 06:09:29,670 [INFO] Found 255 exact anchor==target pairs in train (OOF) set; applying same rule to OOF predictions.
2025-09-21 06:09:29,671 [INFO] OOF Pearson after rule: 0.584306
2025-09-21 06:09:29,671 [INFO] OOF MSE after rule: 0.051614
2025-09-21 06:09:29,671 [INFO] Logging final validation results (OOF, post-rule):
2025-09-21 06:09:29,671 [INFO] Final Validation Pearson: 0.584306 | Final Validation MSE: 0.051614
2025-09-21 06:09:29,674 [INFO] Writing submission to task/us-patent-phrase-to-phrase-matching/outputs/1/submission_6.csv
2025-09-21 06:09:29,681 [INFO] Submission file created successfully.
2025-09-21 06:09:29,681 [INFO] All done (v6).
