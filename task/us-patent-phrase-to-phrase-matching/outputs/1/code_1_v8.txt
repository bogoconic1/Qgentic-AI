2025-09-21 06:53:14,476 [INFO] Initialized logging and created output directories for v8 pipeline.
2025-09-21 06:53:14,477 [INFO] Setting all random seeds to 42.
2025-09-21 06:53:14,477 [INFO] Reading train data from task/us-patent-phrase-to-phrase-matching/train.csv.
2025-09-21 06:53:14,514 [INFO] Reading test data from task/us-patent-phrase-to-phrase-matching/test.csv.
2025-09-21 06:53:14,520 [INFO] Train shape: (32825, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 06:53:14,520 [INFO] Test shape:  (3648, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 06:53:14,525 [INFO] Train missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 06:53:14,525 [INFO] Test missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 06:53:14,525 [INFO] Building vocabulary from segmented corpus.
2025-09-21 06:53:14,872 [INFO] Built vocabulary of size 9042.
2025-09-21 06:53:14,872 [INFO] Tokenizing and numericalizing dataframe with 32825 rows (with token types).
2025-09-21 06:53:14,992 [INFO] Processed 5000 rows.
2025-09-21 06:53:15,112 [INFO] Processed 10000 rows.
2025-09-21 06:53:15,233 [INFO] Processed 15000 rows.
2025-09-21 06:53:15,354 [INFO] Processed 20000 rows.
2025-09-21 06:53:15,475 [INFO] Processed 25000 rows.
2025-09-21 06:53:15,596 [INFO] Processed 30000 rows.
2025-09-21 06:53:15,665 [INFO] Tokenizing and numericalizing dataframe with 3648 rows (with token types).
2025-09-21 06:53:15,753 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 3648 rows (anchor<->target).
2025-09-21 06:53:15,840 [INFO] Prepared swapped TTA arrays for test data.
2025-09-21 06:53:15,840 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 32825 rows (anchor<->target).
2025-09-21 06:53:15,962 [INFO] Processed 5000 swapped rows.
2025-09-21 06:53:16,082 [INFO] Processed 10000 swapped rows.
2025-09-21 06:53:16,202 [INFO] Processed 15000 swapped rows.
2025-09-21 06:53:16,322 [INFO] Processed 20000 swapped rows.
2025-09-21 06:53:16,442 [INFO] Processed 25000 swapped rows.
2025-09-21 06:53:16,563 [INFO] Processed 30000 swapped rows.
2025-09-21 06:53:16,631 [INFO] Prepared swapped augmentation arrays for training.
2025-09-21 06:53:16,632 [INFO] Computing handcrafted similarity features.
2025-09-21 06:53:17,063 [INFO] Computed handcrafted features for 5000 rows.
2025-09-21 06:53:17,491 [INFO] Computed handcrafted features for 10000 rows.
2025-09-21 06:53:17,923 [INFO] Computed handcrafted features for 15000 rows.
2025-09-21 06:53:18,355 [INFO] Computed handcrafted features for 20000 rows.
2025-09-21 06:53:18,784 [INFO] Computed handcrafted features for 25000 rows.
2025-09-21 06:53:19,210 [INFO] Computed handcrafted features for 30000 rows.
2025-09-21 06:53:19,457 [INFO] Finished computing handcrafted features.
2025-09-21 06:53:19,457 [INFO] Computing handcrafted similarity features.
2025-09-21 06:53:19,769 [INFO] Finished computing handcrafted features.
2025-09-21 06:53:19,769 [INFO] Handcrafted feature dimension: 8
2025-09-21 06:53:19,769 [INFO] Labels statistics: min=0.0, max=1.0, mean=0.3619, std=0.2588
2025-09-21 06:53:19,774 [INFO] Found 106 unique context codes across train+test.
2025-09-21 06:53:20,027 [INFO] Class counts: [6774.0, 10306.0, 11068.0, 3634.0, 1043.0] | CE weights: [0.15397107601165771, 0.10120318084955215, 0.0942356288433075, 0.2870115637779236, 1.0]
2025-09-21 06:53:20,029 [INFO] Creating stratified folds on 5-class bins.
2025-09-21 06:53:20,044 [INFO] Fold 0: 6567 samples.
2025-09-21 06:53:20,045 [INFO] Fold 1: 6566 samples.
2025-09-21 06:53:20,045 [INFO] Fold 2: 6566 samples.
2025-09-21 06:53:20,045 [INFO] Fold 3: 6564 samples.
2025-09-21 06:53:20,045 [INFO] Fold 4: 6562 samples.
2025-09-21 06:53:20,046 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 06:53:20,046 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 06:53:20,046 [INFO] ========== Fold 1/5 ==========
2025-09-21 06:53:20,793 [INFO] Applied swap augmentation. New train size: 52516
2025-09-21 06:53:20,804 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 06:53:20,804 [INFO] Train split: 52516 samples; Val split: 6567 samples.
2025-09-21 06:53:20,804 [INFO] Initializing PatentDataset with 52516 samples.
2025-09-21 06:53:20,804 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 06:53:20,804 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 06:53:20,804 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 06:53:21,808 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 06:53:21,808 [INFO] Initializing EMA state from current model weights.
2025-09-21 06:53:21,963 [INFO] Fold 0 - Epoch 1/6 started.
2025-09-21 06:53:35,764 [INFO] Train step 50/274 - Loss: 0.318619 (MSE 0.090677, SoftCE 0.433342, Corr 0.953359, Cons 0.252210, CSim 0.136006, Rank 0.693641, RDropCLS 0.773265, RDropREG 0.048319) | LR: 6.097561e-05
2025-09-21 06:53:48,419 [INFO] Train step 100/274 - Loss: 0.289282 (MSE 0.081722, SoftCE 0.386592, Corr 0.898935, Cons 0.221755, CSim 0.105855, Rank 0.687212, RDropCLS 0.598907, RDropREG 0.037530) | LR: 1.219512e-04
2025-09-21 06:54:01,055 [INFO] Train step 150/274 - Loss: 0.273122 (MSE 0.077021, SoftCE 0.363645, Corr 0.859347, Cons 0.200266, CSim 0.092675, Rank 0.684147, RDropCLS 0.487349, RDropREG 0.032589) | LR: 1.829268e-04
2025-09-21 06:54:13,755 [INFO] Train step 200/274 - Loss: 0.262414 (MSE 0.074442, SoftCE 0.348987, Corr 0.828745, Cons 0.180686, CSim 0.086232, Rank 0.681483, RDropCLS 0.409071, RDropREG 0.029407) | LR: 1.997082e-04
2025-09-21 06:54:26,371 [INFO] Train step 250/274 - Loss: 0.253527 (MSE 0.072056, SoftCE 0.338247, Corr 0.796539, Cons 0.165692, CSim 0.081790, Rank 0.678530, RDropCLS 0.354930, RDropREG 0.027033) | LR: 1.983384e-04
2025-09-21 06:54:32,472 [INFO] Epoch training completed in 70.51s with average loss 0.250042
2025-09-21 06:54:32,474 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:54:33,586 [INFO] Fold 0 - Epoch 1 - Train Loss: 0.250042 | EMA Val MSE: 0.064578 | EMA Val SoftCE: 0.307084 | EMA Val Pearson: 0.237862
2025-09-21 06:54:33,590 [INFO] Fold 0 - Epoch 1 - New best EMA model with Pearson 0.237862.
2025-09-21 06:54:33,590 [INFO] Fold 0 - Epoch 2/6 started.
2025-09-21 06:54:46,551 [INFO] Train step 50/274 - Loss: 0.201581 (MSE 0.055476, SoftCE 0.284605, Corr 0.574240, Cons 0.089292, CSim 0.060492, Rank 0.653844, RDropCLS 0.104110, RDropREG 0.016431) | LR: 1.942877e-04
2025-09-21 06:54:59,315 [INFO] Train step 100/274 - Loss: 0.198497 (MSE 0.055129, SoftCE 0.283171, Corr 0.547577, Cons 0.087006, CSim 0.061212, Rank 0.649261, RDropCLS 0.099686, RDropREG 0.016090) | LR: 1.902280e-04
2025-09-21 06:55:12,065 [INFO] Train step 150/274 - Loss: 0.195758 (MSE 0.053961, SoftCE 0.280618, Corr 0.535246, Cons 0.082041, CSim 0.060407, Rank 0.647139, RDropCLS 0.095112, RDropREG 0.015351) | LR: 1.851529e-04
2025-09-21 06:55:24,688 [INFO] Train step 200/274 - Loss: 0.193005 (MSE 0.052801, SoftCE 0.278075, Corr 0.521608, Cons 0.078577, CSim 0.059684, Rank 0.645344, RDropCLS 0.092125, RDropREG 0.014757) | LR: 1.791195e-04
2025-09-21 06:55:37,365 [INFO] Train step 250/274 - Loss: 0.191077 (MSE 0.052152, SoftCE 0.276434, Corr 0.510914, Cons 0.076355, CSim 0.059286, Rank 0.643580, RDropCLS 0.088879, RDropREG 0.014211) | LR: 1.721956e-04
2025-09-21 06:55:43,508 [INFO] Epoch training completed in 69.92s with average loss 0.190174
2025-09-21 06:55:43,509 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:55:44,584 [INFO] Fold 0 - Epoch 2 - Train Loss: 0.190174 | EMA Val MSE: 0.054834 | EMA Val SoftCE: 0.282505 | EMA Val Pearson: 0.427636
2025-09-21 06:55:44,589 [INFO] Fold 0 - Epoch 2 - New best EMA model with Pearson 0.427636.
2025-09-21 06:55:44,589 [INFO] Fold 0 - Epoch 3/6 started.
2025-09-21 06:55:57,513 [INFO] Train step 50/274 - Loss: 0.169209 (MSE 0.043090, SoftCE 0.261421, Corr 0.381878, Cons 0.056343, CSim 0.053401, Rank 0.621298, RDropCLS 0.077530, RDropREG 0.012067) | LR: 1.604825e-04
2025-09-21 06:56:10,191 [INFO] Train step 100/274 - Loss: 0.166686 (MSE 0.041765, SoftCE 0.259214, Corr 0.370996, Cons 0.053296, CSim 0.052722, Rank 0.617956, RDropCLS 0.077457, RDropREG 0.012218) | LR: 1.517058e-04
2025-09-21 06:56:22,798 [INFO] Train step 150/274 - Loss: 0.165296 (MSE 0.040986, SoftCE 0.258017, Corr 0.365113, Cons 0.052904, CSim 0.052391, Rank 0.615923, RDropCLS 0.077219, RDropREG 0.012126) | LR: 1.423473e-04
2025-09-21 06:56:35,444 [INFO] Train step 200/274 - Loss: 0.164272 (MSE 0.040563, SoftCE 0.257222, Corr 0.359838, Cons 0.050851, CSim 0.052076, Rank 0.614886, RDropCLS 0.076229, RDropREG 0.011883) | LR: 1.325122e-04
2025-09-21 06:56:48,071 [INFO] Train step 250/274 - Loss: 0.163176 (MSE 0.040017, SoftCE 0.256223, Corr 0.355273, Cons 0.050001, CSim 0.051646, Rank 0.613437, RDropCLS 0.075299, RDropREG 0.011714) | LR: 1.223112e-04
2025-09-21 06:56:54,204 [INFO] Epoch training completed in 69.61s with average loss 0.162938
2025-09-21 06:56:54,204 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:56:55,320 [INFO] Fold 0 - Epoch 3 - Train Loss: 0.162938 | EMA Val MSE: 0.050033 | EMA Val SoftCE: 0.270746 | EMA Val Pearson: 0.517712
2025-09-21 06:56:55,324 [INFO] Fold 0 - Epoch 3 - New best EMA model with Pearson 0.517712.
2025-09-21 06:56:55,324 [INFO] Fold 0 - Epoch 4/6 started.
2025-09-21 06:57:08,300 [INFO] Train step 50/274 - Loss: 0.150329 (MSE 0.033600, SoftCE 0.247743, Corr 0.285322, Cons 0.041763, CSim 0.047670, Rank 0.598764, RDropCLS 0.071839, RDropREG 0.010604) | LR: 1.067874e-04
2025-09-21 06:57:20,965 [INFO] Train step 100/274 - Loss: 0.149487 (MSE 0.033491, SoftCE 0.245862, Corr 0.283800, Cons 0.040847, CSim 0.047557, Rank 0.597330, RDropCLS 0.071870, RDropREG 0.010561) | LR: 9.618007e-05
2025-09-21 06:57:33,572 [INFO] Train step 150/274 - Loss: 0.149621 (MSE 0.033526, SoftCE 0.245589, Corr 0.286499, Cons 0.040500, CSim 0.046973, Rank 0.597503, RDropCLS 0.072218, RDropREG 0.010734) | LR: 8.561573e-05
2025-09-21 06:57:46,217 [INFO] Train step 200/274 - Loss: 0.149274 (MSE 0.033329, SoftCE 0.245535, Corr 0.284164, Cons 0.040775, CSim 0.046863, Rank 0.596768, RDropCLS 0.072266, RDropREG 0.010649) | LR: 7.521326e-05
2025-09-21 06:57:58,863 [INFO] Train step 250/274 - Loss: 0.148369 (MSE 0.032915, SoftCE 0.244688, Corr 0.280165, Cons 0.040085, CSim 0.046559, Rank 0.595506, RDropCLS 0.071751, RDropREG 0.010594) | LR: 6.508975e-05
2025-09-21 06:58:05,004 [INFO] Epoch training completed in 69.68s with average loss 0.148162
2025-09-21 06:58:05,005 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:58:06,133 [INFO] Fold 0 - Epoch 4 - Train Loss: 0.148162 | EMA Val MSE: 0.048766 | EMA Val SoftCE: 0.262792 | EMA Val Pearson: 0.557723
2025-09-21 06:58:06,137 [INFO] Fold 0 - Epoch 4 - New best EMA model with Pearson 0.557723.
2025-09-21 06:58:06,137 [INFO] Fold 0 - Epoch 5/6 started.
2025-09-21 06:58:19,207 [INFO] Train step 50/274 - Loss: 0.141678 (MSE 0.029510, SoftCE 0.239839, Corr 0.246173, Cons 0.037699, CSim 0.044152, Rank 0.586600, RDropCLS 0.070353, RDropREG 0.010253) | LR: 5.086032e-05
2025-09-21 06:58:31,863 [INFO] Train step 100/274 - Loss: 0.141117 (MSE 0.029042, SoftCE 0.239714, Corr 0.242658, Cons 0.037717, CSim 0.044172, Rank 0.587171, RDropCLS 0.069721, RDropREG 0.010158) | LR: 4.191050e-05
2025-09-21 06:58:44,585 [INFO] Train step 150/274 - Loss: 0.141244 (MSE 0.029070, SoftCE 0.239474, Corr 0.244451, Cons 0.037252, CSim 0.044161, Rank 0.588118, RDropCLS 0.069876, RDropREG 0.010120) | LR: 3.361443e-05
2025-09-21 06:58:57,268 [INFO] Train step 200/274 - Loss: 0.140949 (MSE 0.028994, SoftCE 0.238895, Corr 0.243887, Cons 0.036694, CSim 0.044169, Rank 0.587496, RDropCLS 0.070234, RDropREG 0.010156) | LR: 2.606547e-05
2025-09-21 06:59:09,963 [INFO] Train step 250/274 - Loss: 0.140489 (MSE 0.028812, SoftCE 0.238427, Corr 0.242028, Cons 0.036228, CSim 0.043913, Rank 0.586471, RDropCLS 0.070322, RDropREG 0.010128) | LR: 1.934857e-05
2025-09-21 06:59:16,073 [INFO] Epoch training completed in 69.93s with average loss 0.140416
2025-09-21 06:59:16,073 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:59:17,209 [INFO] Fold 0 - Epoch 5 - Train Loss: 0.140416 | EMA Val MSE: 0.049566 | EMA Val SoftCE: 0.259518 | EMA Val Pearson: 0.573881
2025-09-21 06:59:17,213 [INFO] Fold 0 - Epoch 5 - New best EMA model with Pearson 0.573881.
2025-09-21 06:59:17,213 [INFO] Fold 0 - Epoch 6/6 started.
2025-09-21 06:59:30,170 [INFO] Train step 50/274 - Loss: 0.135011 (MSE 0.025905, SoftCE 0.232535, Corr 0.219488, Cons 0.034650, CSim 0.041736, Rank 0.582567, RDropCLS 0.071671, RDropREG 0.010232) | LR: 1.109289e-05
2025-09-21 06:59:42,866 [INFO] Train step 100/274 - Loss: 0.135795 (MSE 0.026219, SoftCE 0.233892, Corr 0.222148, Cons 0.034278, CSim 0.042129, Rank 0.582387, RDropCLS 0.071068, RDropREG 0.010209) | LR: 6.743759e-06
2025-09-21 06:59:55,556 [INFO] Train step 150/274 - Loss: 0.135794 (MSE 0.026370, SoftCE 0.233818, Corr 0.221623, Cons 0.033845, CSim 0.042382, Rank 0.581948, RDropCLS 0.071025, RDropREG 0.010181) | LR: 3.444140e-06
2025-09-21 07:00:08,237 [INFO] Train step 200/274 - Loss: 0.136515 (MSE 0.026782, SoftCE 0.234811, Corr 0.223572, Cons 0.033670, CSim 0.042814, Rank 0.582199, RDropCLS 0.071420, RDropREG 0.010308) | LR: 1.231166e-06
2025-09-21 07:00:20,873 [INFO] Train step 250/274 - Loss: 0.136720 (MSE 0.026862, SoftCE 0.235253, Corr 0.224103, Cons 0.033617, CSim 0.042780, Rank 0.582039, RDropCLS 0.071299, RDropREG 0.010289) | LR: 1.297403e-07
2025-09-21 07:00:27,097 [INFO] Epoch training completed in 69.88s with average loss 0.136979
2025-09-21 07:00:27,098 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:00:28,243 [INFO] Fold 0 - Epoch 6 - Train Loss: 0.136979 | EMA Val MSE: 0.051254 | EMA Val SoftCE: 0.259527 | EMA Val Pearson: 0.579302
2025-09-21 07:00:28,248 [INFO] Fold 0 - Epoch 6 - New best EMA model with Pearson 0.579302.
2025-09-21 07:00:29,115 [INFO] Fold 0 - Final Val Pearson (no TTA): 0.579302 | Final Val MSE: 0.051254 | Final Val SoftCE: 0.259527
2025-09-21 07:00:29,852 [INFO] Fold 0 - Final Val Pearson (with TTA swap): 0.585093
2025-09-21 07:00:30,904 [INFO] ========== Fold 2/5 ==========
2025-09-21 07:00:34,674 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 07:00:34,681 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 07:00:34,681 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 07:00:34,681 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 07:00:34,682 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 07:00:34,682 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 07:00:34,683 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 07:00:34,844 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 07:00:34,844 [INFO] Initializing EMA state from current model weights.
2025-09-21 07:00:34,990 [INFO] Fold 1 - Epoch 1/6 started.
2025-09-21 07:00:48,023 [INFO] Train step 50/274 - Loss: 0.316622 (MSE 0.085792, SoftCE 0.436311, Corr 0.956462, Cons 0.241626, CSim 0.141316, Rank 0.692958, RDropCLS 0.747295, RDropREG 0.053378) | LR: 6.097561e-05
2025-09-21 07:01:00,674 [INFO] Train step 100/274 - Loss: 0.287630 (MSE 0.079445, SoftCE 0.387770, Corr 0.893193, Cons 0.227389, CSim 0.108313, Rank 0.686290, RDropCLS 0.574245, RDropREG 0.038595) | LR: 1.219512e-04
2025-09-21 07:01:13,373 [INFO] Train step 150/274 - Loss: 0.270951 (MSE 0.075527, SoftCE 0.362403, Corr 0.851830, Cons 0.204910, CSim 0.094685, Rank 0.683243, RDropCLS 0.461125, RDropREG 0.032662) | LR: 1.829268e-04
2025-09-21 07:01:26,022 [INFO] Train step 200/274 - Loss: 0.259809 (MSE 0.072756, SoftCE 0.347178, Corr 0.817939, Cons 0.188052, CSim 0.087167, Rank 0.680616, RDropCLS 0.388087, RDropREG 0.029321) | LR: 1.997082e-04
2025-09-21 07:01:38,677 [INFO] Train step 250/274 - Loss: 0.251761 (MSE 0.070661, SoftCE 0.336765, Corr 0.792589, Cons 0.172532, CSim 0.082499, Rank 0.678297, RDropCLS 0.335036, RDropREG 0.026980) | LR: 1.983384e-04
2025-09-21 07:01:44,829 [INFO] Epoch training completed in 69.84s with average loss 0.248695
2025-09-21 07:01:44,830 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:01:45,907 [INFO] Fold 1 - Epoch 1 - Train Loss: 0.248695 | EMA Val MSE: 0.066609 | EMA Val SoftCE: 0.321867 | EMA Val Pearson: 0.212951
2025-09-21 07:01:45,912 [INFO] Fold 1 - Epoch 1 - New best EMA model with Pearson 0.212951.
2025-09-21 07:01:45,912 [INFO] Fold 1 - Epoch 2/6 started.
2025-09-21 07:01:58,900 [INFO] Train step 50/274 - Loss: 0.204007 (MSE 0.057880, SoftCE 0.286854, Corr 0.582033, Cons 0.088336, CSim 0.061665, Rank 0.656056, RDropCLS 0.087789, RDropREG 0.014593) | LR: 1.942877e-04
2025-09-21 07:02:11,568 [INFO] Train step 100/274 - Loss: 0.199258 (MSE 0.055759, SoftCE 0.282665, Corr 0.557000, Cons 0.086000, CSim 0.060714, Rank 0.651558, RDropCLS 0.087441, RDropREG 0.014849) | LR: 1.902280e-04
2025-09-21 07:02:24,265 [INFO] Train step 150/274 - Loss: 0.196547 (MSE 0.054720, SoftCE 0.280451, Corr 0.541818, Cons 0.082857, CSim 0.060275, Rank 0.648857, RDropCLS 0.085692, RDropREG 0.014291) | LR: 1.851529e-04
2025-09-21 07:02:36,919 [INFO] Train step 200/274 - Loss: 0.194196 (MSE 0.053816, SoftCE 0.278899, Corr 0.527501, Cons 0.080240, CSim 0.059767, Rank 0.646350, RDropCLS 0.083816, RDropREG 0.014075) | LR: 1.791195e-04
2025-09-21 07:02:49,581 [INFO] Train step 250/274 - Loss: 0.191322 (MSE 0.052458, SoftCE 0.276543, Corr 0.513407, Cons 0.076660, CSim 0.058835, Rank 0.643893, RDropCLS 0.081406, RDropREG 0.013651) | LR: 1.721956e-04
2025-09-21 07:02:55,721 [INFO] Epoch training completed in 69.81s with average loss 0.189904
2025-09-21 07:02:55,722 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:02:56,833 [INFO] Fold 1 - Epoch 2 - Train Loss: 0.189904 | EMA Val MSE: 0.057471 | EMA Val SoftCE: 0.284138 | EMA Val Pearson: 0.408362
2025-09-21 07:02:56,838 [INFO] Fold 1 - Epoch 2 - New best EMA model with Pearson 0.408362.
2025-09-21 07:02:56,838 [INFO] Fold 1 - Epoch 3/6 started.
2025-09-21 07:03:09,810 [INFO] Train step 50/274 - Loss: 0.167378 (MSE 0.041851, SoftCE 0.260332, Corr 0.376000, Cons 0.056930, CSim 0.051606, Rank 0.616044, RDropCLS 0.076769, RDropREG 0.012388) | LR: 1.604825e-04
2025-09-21 07:03:22,490 [INFO] Train step 100/274 - Loss: 0.166109 (MSE 0.041289, SoftCE 0.259030, Corr 0.371118, Cons 0.054663, CSim 0.051043, Rank 0.614606, RDropCLS 0.075287, RDropREG 0.011982) | LR: 1.517058e-04
2025-09-21 07:03:35,173 [INFO] Train step 150/274 - Loss: 0.164662 (MSE 0.040629, SoftCE 0.257666, Corr 0.364866, Cons 0.051853, CSim 0.050577, Rank 0.613123, RDropCLS 0.074343, RDropREG 0.011799) | LR: 1.423473e-04
2025-09-21 07:03:47,855 [INFO] Train step 200/274 - Loss: 0.163721 (MSE 0.040273, SoftCE 0.256455, Corr 0.360774, Cons 0.050920, CSim 0.050385, Rank 0.612649, RDropCLS 0.074343, RDropREG 0.011554) | LR: 1.325122e-04
2025-09-21 07:04:00,598 [INFO] Train step 250/274 - Loss: 0.162595 (MSE 0.039820, SoftCE 0.255562, Corr 0.354334, Cons 0.050385, CSim 0.050257, Rank 0.611305, RDropCLS 0.073970, RDropREG 0.011350) | LR: 1.223112e-04
2025-09-21 07:04:06,756 [INFO] Epoch training completed in 69.92s with average loss 0.161929
2025-09-21 07:04:06,757 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:04:07,872 [INFO] Fold 1 - Epoch 3 - Train Loss: 0.161929 | EMA Val MSE: 0.056202 | EMA Val SoftCE: 0.271497 | EMA Val Pearson: 0.497788
2025-09-21 07:04:07,877 [INFO] Fold 1 - Epoch 3 - New best EMA model with Pearson 0.497788.
2025-09-21 07:04:07,877 [INFO] Fold 1 - Epoch 4/6 started.
2025-09-21 07:04:20,839 [INFO] Train step 50/274 - Loss: 0.147481 (MSE 0.032438, SoftCE 0.243899, Corr 0.276532, Cons 0.041674, CSim 0.044873, Rank 0.594681, RDropCLS 0.071419, RDropREG 0.010837) | LR: 1.067874e-04
2025-09-21 07:04:33,548 [INFO] Train step 100/274 - Loss: 0.148075 (MSE 0.032658, SoftCE 0.245136, Corr 0.277476, Cons 0.041901, CSim 0.045492, Rank 0.593900, RDropCLS 0.072977, RDropREG 0.010747) | LR: 9.618007e-05
2025-09-21 07:04:46,222 [INFO] Train step 150/274 - Loss: 0.147753 (MSE 0.032644, SoftCE 0.244220, Corr 0.277433, Cons 0.041656, CSim 0.045474, Rank 0.593123, RDropCLS 0.073372, RDropREG 0.010762) | LR: 8.561573e-05
2025-09-21 07:04:58,968 [INFO] Train step 200/274 - Loss: 0.147607 (MSE 0.032651, SoftCE 0.244093, Corr 0.276604, Cons 0.041070, CSim 0.045421, Rank 0.592763, RDropCLS 0.072800, RDropREG 0.010645) | LR: 7.521326e-05
2025-09-21 07:05:11,616 [INFO] Train step 250/274 - Loss: 0.147144 (MSE 0.032461, SoftCE 0.243514, Corr 0.274978, Cons 0.040449, CSim 0.045161, Rank 0.592364, RDropCLS 0.072371, RDropREG 0.010532) | LR: 6.508975e-05
2025-09-21 07:05:17,760 [INFO] Epoch training completed in 69.88s with average loss 0.146892
2025-09-21 07:05:17,761 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:05:18,870 [INFO] Fold 1 - Epoch 4 - Train Loss: 0.146892 | EMA Val MSE: 0.054462 | EMA Val SoftCE: 0.264025 | EMA Val Pearson: 0.546115
2025-09-21 07:05:18,875 [INFO] Fold 1 - Epoch 4 - New best EMA model with Pearson 0.546115.
2025-09-21 07:05:18,875 [INFO] Fold 1 - Epoch 5/6 started.
2025-09-21 07:05:31,961 [INFO] Train step 50/274 - Loss: 0.138198 (MSE 0.027378, SoftCE 0.237385, Corr 0.230701, Cons 0.036099, CSim 0.042053, Rank 0.582383, RDropCLS 0.071226, RDropREG 0.009986) | LR: 5.086032e-05
2025-09-21 07:05:44,867 [INFO] Train step 100/274 - Loss: 0.139079 (MSE 0.028092, SoftCE 0.237659, Corr 0.234790, Cons 0.035829, CSim 0.042212, Rank 0.583588, RDropCLS 0.072060, RDropREG 0.010016) | LR: 4.191050e-05
2025-09-21 07:05:57,565 [INFO] Train step 150/274 - Loss: 0.139114 (MSE 0.028164, SoftCE 0.237380, Corr 0.235487, Cons 0.035174, CSim 0.042258, Rank 0.584033, RDropCLS 0.072582, RDropREG 0.010127) | LR: 3.361443e-05
2025-09-21 07:06:10,396 [INFO] Train step 200/274 - Loss: 0.138956 (MSE 0.028143, SoftCE 0.237089, Corr 0.234965, Cons 0.035356, CSim 0.042311, Rank 0.583769, RDropCLS 0.072073, RDropREG 0.010034) | LR: 2.606547e-05
2025-09-21 07:06:23,117 [INFO] Train step 250/274 - Loss: 0.138623 (MSE 0.028051, SoftCE 0.236666, Corr 0.233496, Cons 0.034994, CSim 0.042366, Rank 0.583254, RDropCLS 0.071709, RDropREG 0.010012) | LR: 1.934857e-05
2025-09-21 07:06:29,400 [INFO] Epoch training completed in 70.53s with average loss 0.138356
2025-09-21 07:06:29,402 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:06:30,506 [INFO] Fold 1 - Epoch 5 - Train Loss: 0.138356 | EMA Val MSE: 0.054168 | EMA Val SoftCE: 0.261194 | EMA Val Pearson: 0.568948
2025-09-21 07:06:30,511 [INFO] Fold 1 - Epoch 5 - New best EMA model with Pearson 0.568948.
2025-09-21 07:06:30,511 [INFO] Fold 1 - Epoch 6/6 started.
2025-09-21 07:06:43,510 [INFO] Train step 50/274 - Loss: 0.137478 (MSE 0.027045, SoftCE 0.236192, Corr 0.230140, Cons 0.033954, CSim 0.041238, Rank 0.581651, RDropCLS 0.070630, RDropREG 0.009804) | LR: 1.109289e-05
2025-09-21 07:06:56,272 [INFO] Train step 100/274 - Loss: 0.136551 (MSE 0.027009, SoftCE 0.234760, Corr 0.224416, Cons 0.033534, CSim 0.041592, Rank 0.580526, RDropCLS 0.071911, RDropREG 0.009947) | LR: 6.743759e-06
2025-09-21 07:07:08,985 [INFO] Train step 150/274 - Loss: 0.136036 (MSE 0.026728, SoftCE 0.234426, Corr 0.222180, Cons 0.033294, CSim 0.041250, Rank 0.579139, RDropCLS 0.072404, RDropREG 0.009999) | LR: 3.444140e-06
2025-09-21 07:07:21,668 [INFO] Train step 200/274 - Loss: 0.135580 (MSE 0.026550, SoftCE 0.233968, Corr 0.220001, Cons 0.033120, CSim 0.041150, Rank 0.578589, RDropCLS 0.071919, RDropREG 0.009962) | LR: 1.231166e-06
2025-09-21 07:07:34,355 [INFO] Train step 250/274 - Loss: 0.135608 (MSE 0.026539, SoftCE 0.234062, Corr 0.220041, Cons 0.033068, CSim 0.041121, Rank 0.578696, RDropCLS 0.071888, RDropREG 0.009932) | LR: 1.297403e-07
2025-09-21 07:07:40,474 [INFO] Epoch training completed in 69.96s with average loss 0.135653
2025-09-21 07:07:40,475 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:07:41,582 [INFO] Fold 1 - Epoch 6 - Train Loss: 0.135653 | EMA Val MSE: 0.055148 | EMA Val SoftCE: 0.261611 | EMA Val Pearson: 0.579094
2025-09-21 07:07:41,587 [INFO] Fold 1 - Epoch 6 - New best EMA model with Pearson 0.579094.
2025-09-21 07:07:42,419 [INFO] Fold 1 - Final Val Pearson (no TTA): 0.579094 | Final Val MSE: 0.055148 | Final Val SoftCE: 0.261611
2025-09-21 07:07:43,156 [INFO] Fold 1 - Final Val Pearson (with TTA swap): 0.582999
2025-09-21 07:07:44,158 [INFO] ========== Fold 3/5 ==========
2025-09-21 07:07:48,020 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 07:07:48,030 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 07:07:48,030 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 07:07:48,030 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 07:07:48,030 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 07:07:48,030 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 07:07:48,032 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 07:07:48,194 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 07:07:48,195 [INFO] Initializing EMA state from current model weights.
2025-09-21 07:07:48,340 [INFO] Fold 2 - Epoch 1/6 started.
2025-09-21 07:08:01,398 [INFO] Train step 50/274 - Loss: 0.310657 (MSE 0.085948, SoftCE 0.414645, Corr 0.962067, Cons 0.266685, CSim 0.124932, Rank 0.695330, RDropCLS 0.761806, RDropREG 0.044360) | LR: 6.097561e-05
2025-09-21 07:08:14,068 [INFO] Train step 100/274 - Loss: 0.284333 (MSE 0.078712, SoftCE 0.376174, Corr 0.900311, Cons 0.234707, CSim 0.098908, Rank 0.689097, RDropCLS 0.585728, RDropREG 0.035563) | LR: 1.219512e-04
2025-09-21 07:08:26,723 [INFO] Train step 150/274 - Loss: 0.268705 (MSE 0.075182, SoftCE 0.355246, Corr 0.853403, Cons 0.207608, CSim 0.088304, Rank 0.684668, RDropCLS 0.469857, RDropREG 0.031076) | LR: 1.829268e-04
2025-09-21 07:08:39,373 [INFO] Train step 200/274 - Loss: 0.258626 (MSE 0.072760, SoftCE 0.342397, Corr 0.822481, Cons 0.187943, CSim 0.082408, Rank 0.681612, RDropCLS 0.392563, RDropREG 0.027961) | LR: 1.997082e-04
2025-09-21 07:08:52,044 [INFO] Train step 250/274 - Loss: 0.251042 (MSE 0.071005, SoftCE 0.333280, Corr 0.795316, Cons 0.174234, CSim 0.078810, Rank 0.679346, RDropCLS 0.336540, RDropREG 0.025625) | LR: 1.983384e-04
2025-09-21 07:08:58,149 [INFO] Epoch training completed in 69.81s with average loss 0.247645
2025-09-21 07:08:58,150 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:08:59,266 [INFO] Fold 2 - Epoch 1 - Train Loss: 0.247645 | EMA Val MSE: 0.063451 | EMA Val SoftCE: 0.306712 | EMA Val Pearson: 0.272805
2025-09-21 07:08:59,270 [INFO] Fold 2 - Epoch 1 - New best EMA model with Pearson 0.272805.
2025-09-21 07:08:59,270 [INFO] Fold 2 - Epoch 2/6 started.
2025-09-21 07:09:12,242 [INFO] Train step 50/274 - Loss: 0.200296 (MSE 0.054900, SoftCE 0.281440, Corr 0.578113, Cons 0.084618, CSim 0.059798, Rank 0.654402, RDropCLS 0.092656, RDropREG 0.016478) | LR: 1.942877e-04
2025-09-21 07:09:24,964 [INFO] Train step 100/274 - Loss: 0.197659 (MSE 0.054542, SoftCE 0.280683, Corr 0.552201, Cons 0.085547, CSim 0.060563, Rank 0.651640, RDropCLS 0.089980, RDropREG 0.015965) | LR: 1.902280e-04
2025-09-21 07:09:37,723 [INFO] Train step 150/274 - Loss: 0.195683 (MSE 0.053916, SoftCE 0.279408, Corr 0.539806, Cons 0.083216, CSim 0.059836, Rank 0.649157, RDropCLS 0.087677, RDropREG 0.015512) | LR: 1.851529e-04
2025-09-21 07:09:50,419 [INFO] Train step 200/274 - Loss: 0.193483 (MSE 0.053148, SoftCE 0.278244, Corr 0.525764, Cons 0.081029, CSim 0.059489, Rank 0.645721, RDropCLS 0.084302, RDropREG 0.014965) | LR: 1.791195e-04
2025-09-21 07:10:03,113 [INFO] Train step 250/274 - Loss: 0.190992 (MSE 0.052256, SoftCE 0.276267, Corr 0.511234, Cons 0.078389, CSim 0.059040, Rank 0.643545, RDropCLS 0.081582, RDropREG 0.014355) | LR: 1.721956e-04
2025-09-21 07:10:09,256 [INFO] Epoch training completed in 69.98s with average loss 0.189957
2025-09-21 07:10:09,257 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:10:10,369 [INFO] Fold 2 - Epoch 2 - Train Loss: 0.189957 | EMA Val MSE: 0.055851 | EMA Val SoftCE: 0.282000 | EMA Val Pearson: 0.429829
2025-09-21 07:10:10,374 [INFO] Fold 2 - Epoch 2 - New best EMA model with Pearson 0.429829.
2025-09-21 07:10:10,374 [INFO] Fold 2 - Epoch 3/6 started.
2025-09-21 07:10:23,374 [INFO] Train step 50/274 - Loss: 0.167148 (MSE 0.042036, SoftCE 0.260038, Corr 0.372505, Cons 0.054015, CSim 0.052725, Rank 0.618158, RDropCLS 0.073651, RDropREG 0.011864) | LR: 1.604825e-04
2025-09-21 07:10:36,074 [INFO] Train step 100/274 - Loss: 0.165114 (MSE 0.040944, SoftCE 0.258084, Corr 0.364835, Cons 0.052124, CSim 0.051872, Rank 0.615201, RDropCLS 0.073469, RDropREG 0.011769) | LR: 1.517058e-04
2025-09-21 07:10:48,766 [INFO] Train step 150/274 - Loss: 0.165134 (MSE 0.040919, SoftCE 0.257815, Corr 0.366052, Cons 0.052806, CSim 0.051568, Rank 0.615929, RDropCLS 0.072698, RDropREG 0.011487) | LR: 1.423473e-04
2025-09-21 07:11:01,434 [INFO] Train step 200/274 - Loss: 0.164144 (MSE 0.040462, SoftCE 0.256724, Corr 0.362366, Cons 0.052062, CSim 0.051155, Rank 0.614547, RDropCLS 0.072171, RDropREG 0.011401) | LR: 1.325122e-04
2025-09-21 07:11:14,174 [INFO] Train step 250/274 - Loss: 0.163181 (MSE 0.040138, SoftCE 0.255969, Corr 0.356711, Cons 0.050881, CSim 0.051011, Rank 0.613231, RDropCLS 0.071746, RDropREG 0.011226) | LR: 1.223112e-04
2025-09-21 07:11:20,315 [INFO] Epoch training completed in 69.94s with average loss 0.162816
2025-09-21 07:11:20,316 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:11:21,432 [INFO] Fold 2 - Epoch 3 - Train Loss: 0.162816 | EMA Val MSE: 0.051690 | EMA Val SoftCE: 0.270356 | EMA Val Pearson: 0.522848
2025-09-21 07:11:21,436 [INFO] Fold 2 - Epoch 3 - New best EMA model with Pearson 0.522848.
2025-09-21 07:11:21,436 [INFO] Fold 2 - Epoch 4/6 started.
2025-09-21 07:11:34,417 [INFO] Train step 50/274 - Loss: 0.147498 (MSE 0.032034, SoftCE 0.243429, Corr 0.280835, Cons 0.040994, CSim 0.045149, Rank 0.596865, RDropCLS 0.068005, RDropREG 0.010354) | LR: 1.067874e-04
2025-09-21 07:11:47,073 [INFO] Train step 100/274 - Loss: 0.148259 (MSE 0.032632, SoftCE 0.245032, Corr 0.279953, Cons 0.042097, CSim 0.046096, Rank 0.595449, RDropCLS 0.069176, RDropREG 0.010329) | LR: 9.618007e-05
2025-09-21 07:11:59,722 [INFO] Train step 150/274 - Loss: 0.147857 (MSE 0.032474, SoftCE 0.244739, Corr 0.277784, Cons 0.041843, CSim 0.045962, Rank 0.594505, RDropCLS 0.069443, RDropREG 0.010329) | LR: 8.561573e-05
2025-09-21 07:12:12,410 [INFO] Train step 200/274 - Loss: 0.147692 (MSE 0.032535, SoftCE 0.244396, Corr 0.276903, Cons 0.041250, CSim 0.045990, Rank 0.594058, RDropCLS 0.069752, RDropREG 0.010379) | LR: 7.521326e-05
2025-09-21 07:12:25,067 [INFO] Train step 250/274 - Loss: 0.146875 (MSE 0.032221, SoftCE 0.243640, Corr 0.272435, Cons 0.040752, CSim 0.045898, Rank 0.593391, RDropCLS 0.069208, RDropREG 0.010306) | LR: 6.508975e-05
2025-09-21 07:12:31,177 [INFO] Epoch training completed in 69.74s with average loss 0.146690
2025-09-21 07:12:31,178 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:12:32,245 [INFO] Fold 2 - Epoch 4 - Train Loss: 0.146690 | EMA Val MSE: 0.049406 | EMA Val SoftCE: 0.262353 | EMA Val Pearson: 0.572408
2025-09-21 07:12:32,250 [INFO] Fold 2 - Epoch 4 - New best EMA model with Pearson 0.572408.
2025-09-21 07:12:32,250 [INFO] Fold 2 - Epoch 5/6 started.
2025-09-21 07:12:45,169 [INFO] Train step 50/274 - Loss: 0.140099 (MSE 0.028890, SoftCE 0.237229, Corr 0.241576, Cons 0.036520, CSim 0.044039, Rank 0.585689, RDropCLS 0.069934, RDropREG 0.009888) | LR: 5.086032e-05
2025-09-21 07:12:57,817 [INFO] Train step 100/274 - Loss: 0.140134 (MSE 0.028904, SoftCE 0.237875, Corr 0.240180, Cons 0.036275, CSim 0.044009, Rank 0.584186, RDropCLS 0.071197, RDropREG 0.010021) | LR: 4.191050e-05
2025-09-21 07:13:10,486 [INFO] Train step 150/274 - Loss: 0.139774 (MSE 0.028729, SoftCE 0.237653, Corr 0.238229, Cons 0.035911, CSim 0.043704, Rank 0.583784, RDropCLS 0.071200, RDropREG 0.010200) | LR: 3.361443e-05
2025-09-21 07:13:23,183 [INFO] Train step 200/274 - Loss: 0.139599 (MSE 0.028590, SoftCE 0.237390, Corr 0.238219, Cons 0.035743, CSim 0.043335, Rank 0.584051, RDropCLS 0.070733, RDropREG 0.010112) | LR: 2.606547e-05
2025-09-21 07:13:35,912 [INFO] Train step 250/274 - Loss: 0.139326 (MSE 0.028373, SoftCE 0.237241, Corr 0.236975, Cons 0.035726, CSim 0.043170, Rank 0.584227, RDropCLS 0.070294, RDropREG 0.010014) | LR: 1.934857e-05
2025-09-21 07:13:42,070 [INFO] Epoch training completed in 69.82s with average loss 0.139222
2025-09-21 07:13:42,072 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:13:43,170 [INFO] Fold 2 - Epoch 5 - Train Loss: 0.139222 | EMA Val MSE: 0.048754 | EMA Val SoftCE: 0.258797 | EMA Val Pearson: 0.597093
2025-09-21 07:13:43,174 [INFO] Fold 2 - Epoch 5 - New best EMA model with Pearson 0.597093.
2025-09-21 07:13:43,174 [INFO] Fold 2 - Epoch 6/6 started.
2025-09-21 07:13:56,208 [INFO] Train step 50/274 - Loss: 0.135649 (MSE 0.026441, SoftCE 0.234034, Corr 0.220192, Cons 0.034531, CSim 0.042040, Rank 0.580326, RDropCLS 0.068669, RDropREG 0.009879) | LR: 1.109289e-05
2025-09-21 07:14:08,873 [INFO] Train step 100/274 - Loss: 0.136668 (MSE 0.026802, SoftCE 0.235240, Corr 0.225256, Cons 0.034546, CSim 0.042001, Rank 0.581112, RDropCLS 0.069384, RDropREG 0.009980) | LR: 6.743759e-06
2025-09-21 07:14:21,607 [INFO] Train step 150/274 - Loss: 0.136265 (MSE 0.026704, SoftCE 0.234236, Corr 0.224314, Cons 0.034376, CSim 0.041743, Rank 0.582137, RDropCLS 0.069059, RDropREG 0.009908) | LR: 3.444140e-06
2025-09-21 07:14:34,318 [INFO] Train step 200/274 - Loss: 0.135961 (MSE 0.026499, SoftCE 0.234342, Corr 0.221869, Cons 0.034334, CSim 0.041630, Rank 0.581693, RDropCLS 0.069139, RDropREG 0.009869) | LR: 1.231166e-06
2025-09-21 07:14:46,986 [INFO] Train step 250/274 - Loss: 0.136160 (MSE 0.026643, SoftCE 0.234621, Corr 0.221821, Cons 0.034285, CSim 0.041960, Rank 0.581955, RDropCLS 0.069678, RDropREG 0.009923) | LR: 1.297403e-07
2025-09-21 07:14:53,272 [INFO] Epoch training completed in 70.10s with average loss 0.136071
2025-09-21 07:14:53,273 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:14:54,363 [INFO] Fold 2 - Epoch 6 - Train Loss: 0.136071 | EMA Val MSE: 0.049641 | EMA Val SoftCE: 0.258731 | EMA Val Pearson: 0.605332
2025-09-21 07:14:54,372 [INFO] Fold 2 - Epoch 6 - New best EMA model with Pearson 0.605332.
2025-09-21 07:14:55,219 [INFO] Fold 2 - Final Val Pearson (no TTA): 0.605332 | Final Val MSE: 0.049641 | Final Val SoftCE: 0.258731
2025-09-21 07:14:55,956 [INFO] Fold 2 - Final Val Pearson (with TTA swap): 0.608675
2025-09-21 07:14:56,961 [INFO] ========== Fold 4/5 ==========
2025-09-21 07:15:01,011 [INFO] Applied swap augmentation. New train size: 52522
2025-09-21 07:15:01,022 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 07:15:01,022 [INFO] Train split: 52522 samples; Val split: 6564 samples.
2025-09-21 07:15:01,022 [INFO] Initializing PatentDataset with 52522 samples.
2025-09-21 07:15:01,022 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 07:15:01,022 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 07:15:01,025 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 07:15:01,187 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 07:15:01,188 [INFO] Initializing EMA state from current model weights.
2025-09-21 07:15:01,333 [INFO] Fold 3 - Epoch 1/6 started.
2025-09-21 07:15:14,544 [INFO] Train step 50/274 - Loss: 0.312147 (MSE 0.087734, SoftCE 0.427270, Corr 0.933160, Cons 0.262826, CSim 0.139813, Rank 0.692053, RDropCLS 0.690471, RDropREG 0.039979) | LR: 6.097561e-05
2025-09-21 07:15:27,265 [INFO] Train step 100/274 - Loss: 0.283235 (MSE 0.079663, SoftCE 0.380118, Corr 0.873167, Cons 0.231322, CSim 0.107542, Rank 0.685776, RDropCLS 0.545875, RDropREG 0.032200) | LR: 1.219512e-04
2025-09-21 07:15:39,963 [INFO] Train step 150/274 - Loss: 0.267814 (MSE 0.075568, SoftCE 0.357485, Corr 0.837280, Cons 0.205419, CSim 0.093625, Rank 0.682548, RDropCLS 0.441191, RDropREG 0.028247) | LR: 1.829268e-04
2025-09-21 07:15:52,622 [INFO] Train step 200/274 - Loss: 0.257876 (MSE 0.073044, SoftCE 0.343704, Corr 0.809927, Cons 0.186908, CSim 0.086879, Rank 0.680085, RDropCLS 0.372558, RDropREG 0.025935) | LR: 1.997082e-04
2025-09-21 07:16:05,297 [INFO] Train step 250/274 - Loss: 0.250450 (MSE 0.071289, SoftCE 0.334150, Corr 0.785103, Cons 0.173378, CSim 0.082619, Rank 0.677838, RDropCLS 0.322769, RDropREG 0.024185) | LR: 1.983384e-04
2025-09-21 07:16:11,410 [INFO] Epoch training completed in 70.08s with average loss 0.246874
2025-09-21 07:16:11,410 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:16:12,531 [INFO] Fold 3 - Epoch 1 - Train Loss: 0.246874 | EMA Val MSE: 0.068186 | EMA Val SoftCE: 0.314617 | EMA Val Pearson: 0.220979
2025-09-21 07:16:12,535 [INFO] Fold 3 - Epoch 1 - New best EMA model with Pearson 0.220979.
2025-09-21 07:16:12,535 [INFO] Fold 3 - Epoch 2/6 started.
2025-09-21 07:16:25,513 [INFO] Train step 50/274 - Loss: 0.200401 (MSE 0.055589, SoftCE 0.282497, Corr 0.568181, Cons 0.091125, CSim 0.061144, Rank 0.652501, RDropCLS 0.097848, RDropREG 0.016132) | LR: 1.942877e-04
2025-09-21 07:16:38,168 [INFO] Train step 100/274 - Loss: 0.197067 (MSE 0.054647, SoftCE 0.279756, Corr 0.548246, Cons 0.084399, CSim 0.060768, Rank 0.649440, RDropCLS 0.093652, RDropREG 0.015349) | LR: 1.902280e-04
2025-09-21 07:16:50,870 [INFO] Train step 150/274 - Loss: 0.194126 (MSE 0.053333, SoftCE 0.277768, Corr 0.532274, Cons 0.080792, CSim 0.059981, Rank 0.646539, RDropCLS 0.089509, RDropREG 0.014788) | LR: 1.851529e-04
2025-09-21 07:17:03,612 [INFO] Train step 200/274 - Loss: 0.191439 (MSE 0.052331, SoftCE 0.276128, Corr 0.515623, Cons 0.077818, CSim 0.059428, Rank 0.643910, RDropCLS 0.085171, RDropREG 0.014211) | LR: 1.791195e-04
2025-09-21 07:17:16,284 [INFO] Train step 250/274 - Loss: 0.189037 (MSE 0.051504, SoftCE 0.274343, Corr 0.500854, Cons 0.075761, CSim 0.059101, Rank 0.641531, RDropCLS 0.082611, RDropREG 0.013912) | LR: 1.721956e-04
2025-09-21 07:17:22,445 [INFO] Epoch training completed in 69.91s with average loss 0.187836
2025-09-21 07:17:22,446 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:17:23,571 [INFO] Fold 3 - Epoch 2 - Train Loss: 0.187836 | EMA Val MSE: 0.056860 | EMA Val SoftCE: 0.282246 | EMA Val Pearson: 0.413876
2025-09-21 07:17:23,575 [INFO] Fold 3 - Epoch 2 - New best EMA model with Pearson 0.413876.
2025-09-21 07:17:23,576 [INFO] Fold 3 - Epoch 3/6 started.
2025-09-21 07:17:36,718 [INFO] Train step 50/274 - Loss: 0.166408 (MSE 0.041868, SoftCE 0.259077, Corr 0.370589, Cons 0.052307, CSim 0.052661, Rank 0.616012, RDropCLS 0.070586, RDropREG 0.011911) | LR: 1.604825e-04
2025-09-21 07:17:49,412 [INFO] Train step 100/274 - Loss: 0.164523 (MSE 0.040832, SoftCE 0.257691, Corr 0.360669, Cons 0.050751, CSim 0.052030, Rank 0.614998, RDropCLS 0.071046, RDropREG 0.011719) | LR: 1.517058e-04
2025-09-21 07:18:02,070 [INFO] Train step 150/274 - Loss: 0.163218 (MSE 0.040298, SoftCE 0.256156, Corr 0.355069, Cons 0.049482, CSim 0.051932, Rank 0.613931, RDropCLS 0.070224, RDropREG 0.011480) | LR: 1.423473e-04
2025-09-21 07:18:14,956 [INFO] Train step 200/274 - Loss: 0.162054 (MSE 0.039747, SoftCE 0.255230, Corr 0.349419, Cons 0.048836, CSim 0.051435, Rank 0.612130, RDropCLS 0.070092, RDropREG 0.011386) | LR: 1.325122e-04
2025-09-21 07:18:27,749 [INFO] Train step 250/274 - Loss: 0.161284 (MSE 0.039450, SoftCE 0.254584, Corr 0.345355, Cons 0.048259, CSim 0.051213, Rank 0.611221, RDropCLS 0.069199, RDropREG 0.011211) | LR: 1.223112e-04
2025-09-21 07:18:33,880 [INFO] Epoch training completed in 70.30s with average loss 0.160962
2025-09-21 07:18:33,882 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:18:34,973 [INFO] Fold 3 - Epoch 3 - Train Loss: 0.160962 | EMA Val MSE: 0.052477 | EMA Val SoftCE: 0.271296 | EMA Val Pearson: 0.514425
2025-09-21 07:18:34,977 [INFO] Fold 3 - Epoch 3 - New best EMA model with Pearson 0.514425.
2025-09-21 07:18:34,977 [INFO] Fold 3 - Epoch 4/6 started.
2025-09-21 07:18:48,001 [INFO] Train step 50/274 - Loss: 0.146563 (MSE 0.031860, SoftCE 0.243725, Corr 0.271146, Cons 0.038464, CSim 0.046105, Rank 0.592842, RDropCLS 0.070256, RDropREG 0.010850) | LR: 1.067874e-04
2025-09-21 07:19:00,668 [INFO] Train step 100/274 - Loss: 0.146762 (MSE 0.031967, SoftCE 0.243588, Corr 0.273709, Cons 0.038967, CSim 0.045991, Rank 0.593225, RDropCLS 0.068150, RDropREG 0.010424) | LR: 9.618007e-05
2025-09-21 07:19:13,382 [INFO] Train step 150/274 - Loss: 0.146968 (MSE 0.032210, SoftCE 0.243365, Corr 0.274853, Cons 0.038575, CSim 0.046205, Rank 0.593937, RDropCLS 0.068627, RDropREG 0.010343) | LR: 8.561573e-05
2025-09-21 07:19:26,072 [INFO] Train step 200/274 - Loss: 0.146958 (MSE 0.032265, SoftCE 0.243337, Corr 0.274701, Cons 0.037906, CSim 0.046295, Rank 0.593920, RDropCLS 0.068141, RDropREG 0.010314) | LR: 7.521326e-05
2025-09-21 07:19:38,768 [INFO] Train step 250/274 - Loss: 0.146617 (MSE 0.032045, SoftCE 0.243212, Corr 0.272765, Cons 0.037686, CSim 0.046159, Rank 0.593735, RDropCLS 0.067859, RDropREG 0.010205) | LR: 6.508975e-05
2025-09-21 07:19:44,884 [INFO] Epoch training completed in 69.91s with average loss 0.146548
2025-09-21 07:19:44,885 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:19:46,033 [INFO] Fold 3 - Epoch 4 - Train Loss: 0.146548 | EMA Val MSE: 0.049843 | EMA Val SoftCE: 0.263634 | EMA Val Pearson: 0.561147
2025-09-21 07:19:46,038 [INFO] Fold 3 - Epoch 4 - New best EMA model with Pearson 0.561147.
2025-09-21 07:19:46,038 [INFO] Fold 3 - Epoch 5/6 started.
2025-09-21 07:19:59,021 [INFO] Train step 50/274 - Loss: 0.139401 (MSE 0.028247, SoftCE 0.236073, Corr 0.241429, Cons 0.034825, CSim 0.042872, Rank 0.588927, RDropCLS 0.067119, RDropREG 0.009939) | LR: 5.086032e-05
2025-09-21 07:20:11,815 [INFO] Train step 100/274 - Loss: 0.139010 (MSE 0.028295, SoftCE 0.235995, Corr 0.237618, Cons 0.034585, CSim 0.043317, Rank 0.586443, RDropCLS 0.067937, RDropREG 0.010151) | LR: 4.191050e-05
2025-09-21 07:20:24,521 [INFO] Train step 150/274 - Loss: 0.138773 (MSE 0.028232, SoftCE 0.235726, Corr 0.236880, Cons 0.034358, CSim 0.043202, Rank 0.585370, RDropCLS 0.068048, RDropREG 0.010064) | LR: 3.361443e-05
2025-09-21 07:20:37,172 [INFO] Train step 200/274 - Loss: 0.139086 (MSE 0.028347, SoftCE 0.236456, Corr 0.237112, Cons 0.034427, CSim 0.043300, Rank 0.585308, RDropCLS 0.068420, RDropREG 0.010222) | LR: 2.606547e-05
2025-09-21 07:20:49,885 [INFO] Train step 250/274 - Loss: 0.138953 (MSE 0.028229, SoftCE 0.236946, Corr 0.234910, Cons 0.034531, CSim 0.043315, Rank 0.584518, RDropCLS 0.068362, RDropREG 0.010158) | LR: 1.934857e-05
2025-09-21 07:20:56,033 [INFO] Epoch training completed in 70.00s with average loss 0.139065
2025-09-21 07:20:56,034 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:20:57,168 [INFO] Fold 3 - Epoch 5 - Train Loss: 0.139065 | EMA Val MSE: 0.049446 | EMA Val SoftCE: 0.260667 | EMA Val Pearson: 0.583390
2025-09-21 07:20:57,172 [INFO] Fold 3 - Epoch 5 - New best EMA model with Pearson 0.583390.
2025-09-21 07:20:57,173 [INFO] Fold 3 - Epoch 6/6 started.
2025-09-21 07:21:10,168 [INFO] Train step 50/274 - Loss: 0.135729 (MSE 0.026411, SoftCE 0.234508, Corr 0.219170, Cons 0.034267, CSim 0.042401, Rank 0.580618, RDropCLS 0.068960, RDropREG 0.010112) | LR: 1.109289e-05
2025-09-21 07:21:22,849 [INFO] Train step 100/274 - Loss: 0.136084 (MSE 0.026668, SoftCE 0.234435, Corr 0.222021, Cons 0.033454, CSim 0.042453, Rank 0.581497, RDropCLS 0.067952, RDropREG 0.009914) | LR: 6.743759e-06
2025-09-21 07:21:35,519 [INFO] Train step 150/274 - Loss: 0.135763 (MSE 0.026483, SoftCE 0.234287, Corr 0.220451, Cons 0.033331, CSim 0.042348, Rank 0.580474, RDropCLS 0.068423, RDropREG 0.009906) | LR: 3.444140e-06
2025-09-21 07:21:48,197 [INFO] Train step 200/274 - Loss: 0.135721 (MSE 0.026490, SoftCE 0.234420, Corr 0.219774, Cons 0.033060, CSim 0.042314, Rank 0.579997, RDropCLS 0.068442, RDropREG 0.009842) | LR: 1.231166e-06
2025-09-21 07:22:00,867 [INFO] Train step 250/274 - Loss: 0.135498 (MSE 0.026338, SoftCE 0.234401, Corr 0.218268, Cons 0.032924, CSim 0.042319, Rank 0.579797, RDropCLS 0.068388, RDropREG 0.009806) | LR: 1.297403e-07
2025-09-21 07:22:06,980 [INFO] Epoch training completed in 69.81s with average loss 0.135488
2025-09-21 07:22:06,981 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:22:08,073 [INFO] Fold 3 - Epoch 6 - Train Loss: 0.135488 | EMA Val MSE: 0.050470 | EMA Val SoftCE: 0.260934 | EMA Val Pearson: 0.591877
2025-09-21 07:22:08,080 [INFO] Fold 3 - Epoch 6 - New best EMA model with Pearson 0.591877.
2025-09-21 07:22:08,940 [INFO] Fold 3 - Final Val Pearson (no TTA): 0.591877 | Final Val MSE: 0.050470 | Final Val SoftCE: 0.260934
2025-09-21 07:22:09,699 [INFO] Fold 3 - Final Val Pearson (with TTA swap): 0.594235
2025-09-21 07:22:10,728 [INFO] ========== Fold 5/5 ==========
2025-09-21 07:22:10,782 [INFO] Applied swap augmentation. New train size: 52526
2025-09-21 07:22:10,788 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 07:22:10,788 [INFO] Train split: 52526 samples; Val split: 6562 samples.
2025-09-21 07:22:10,789 [INFO] Initializing PatentDataset with 52526 samples.
2025-09-21 07:22:10,789 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 07:22:10,789 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 07:22:10,790 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 07:22:10,951 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 07:22:10,952 [INFO] Initializing EMA state from current model weights.
2025-09-21 07:22:11,097 [INFO] Fold 4 - Epoch 1/6 started.
2025-09-21 07:22:24,162 [INFO] Train step 50/274 - Loss: 0.309533 (MSE 0.092700, SoftCE 0.409809, Corr 0.941171, Cons 0.235532, CSim 0.119330, Rank 0.691739, RDropCLS 0.730989, RDropREG 0.042727) | LR: 6.097561e-05
2025-09-21 07:22:36,861 [INFO] Train step 100/274 - Loss: 0.282468 (MSE 0.081912, SoftCE 0.372478, Corr 0.882988, Cons 0.212409, CSim 0.094599, Rank 0.686280, RDropCLS 0.570255, RDropREG 0.033657) | LR: 1.219512e-04
2025-09-21 07:22:49,518 [INFO] Train step 150/274 - Loss: 0.268075 (MSE 0.077450, SoftCE 0.353508, Corr 0.845604, Cons 0.190036, CSim 0.085326, Rank 0.683027, RDropCLS 0.463323, RDropREG 0.029173) | LR: 1.829268e-04
2025-09-21 07:23:02,170 [INFO] Train step 200/274 - Loss: 0.258584 (MSE 0.074798, SoftCE 0.341093, Corr 0.818458, Cons 0.175203, CSim 0.080825, Rank 0.680814, RDropCLS 0.389749, RDropREG 0.025974) | LR: 1.997082e-04
2025-09-21 07:23:14,860 [INFO] Train step 250/274 - Loss: 0.250501 (MSE 0.072273, SoftCE 0.331623, Corr 0.790258, Cons 0.161019, CSim 0.077472, Rank 0.678340, RDropCLS 0.339454, RDropREG 0.024373) | LR: 1.983384e-04
2025-09-21 07:23:20,988 [INFO] Epoch training completed in 69.89s with average loss 0.246767
2025-09-21 07:23:20,989 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:23:22,100 [INFO] Fold 4 - Epoch 1 - Train Loss: 0.246767 | EMA Val MSE: 0.071246 | EMA Val SoftCE: 0.308471 | EMA Val Pearson: 0.265850
2025-09-21 07:23:22,105 [INFO] Fold 4 - Epoch 1 - New best EMA model with Pearson 0.265850.
2025-09-21 07:23:22,105 [INFO] Fold 4 - Epoch 2/6 started.
2025-09-21 07:23:35,060 [INFO] Train step 50/274 - Loss: 0.201906 (MSE 0.056350, SoftCE 0.284994, Corr 0.571161, Cons 0.080552, CSim 0.062368, Rank 0.655003, RDropCLS 0.104544, RDropREG 0.015189) | LR: 1.942877e-04
2025-09-21 07:23:47,718 [INFO] Train step 100/274 - Loss: 0.199145 (MSE 0.055522, SoftCE 0.282939, Corr 0.553821, Cons 0.082745, CSim 0.062005, Rank 0.651550, RDropCLS 0.097222, RDropREG 0.014351) | LR: 1.902280e-04
2025-09-21 07:24:00,387 [INFO] Train step 150/274 - Loss: 0.196004 (MSE 0.054209, SoftCE 0.280363, Corr 0.537313, Cons 0.079335, CSim 0.061175, Rank 0.648907, RDropCLS 0.093272, RDropREG 0.013867) | LR: 1.851529e-04
2025-09-21 07:24:13,049 [INFO] Train step 200/274 - Loss: 0.193200 (MSE 0.053087, SoftCE 0.278061, Corr 0.522184, Cons 0.076850, CSim 0.060462, Rank 0.646163, RDropCLS 0.090308, RDropREG 0.013457) | LR: 1.791195e-04
2025-09-21 07:24:25,703 [INFO] Train step 250/274 - Loss: 0.191532 (MSE 0.052632, SoftCE 0.276794, Corr 0.512199, Cons 0.074821, CSim 0.060039, Rank 0.644417, RDropCLS 0.086037, RDropREG 0.013010) | LR: 1.721956e-04
2025-09-21 07:24:31,850 [INFO] Epoch training completed in 69.74s with average loss 0.190393
2025-09-21 07:24:31,851 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:24:32,950 [INFO] Fold 4 - Epoch 2 - Train Loss: 0.190393 | EMA Val MSE: 0.056494 | EMA Val SoftCE: 0.280634 | EMA Val Pearson: 0.445805
2025-09-21 07:24:32,955 [INFO] Fold 4 - Epoch 2 - New best EMA model with Pearson 0.445805.
2025-09-21 07:24:32,955 [INFO] Fold 4 - Epoch 3/6 started.
2025-09-21 07:24:45,920 [INFO] Train step 50/274 - Loss: 0.169061 (MSE 0.042627, SoftCE 0.261434, Corr 0.384389, Cons 0.051540, CSim 0.053246, Rank 0.620619, RDropCLS 0.079706, RDropREG 0.011697) | LR: 1.604825e-04
2025-09-21 07:24:58,601 [INFO] Train step 100/274 - Loss: 0.168251 (MSE 0.042430, SoftCE 0.259897, Corr 0.382914, Cons 0.049935, CSim 0.052758, Rank 0.619927, RDropCLS 0.078239, RDropREG 0.011415) | LR: 1.517058e-04
2025-09-21 07:25:11,271 [INFO] Train step 150/274 - Loss: 0.166759 (MSE 0.041659, SoftCE 0.258772, Corr 0.376079, Cons 0.049290, CSim 0.052109, Rank 0.617915, RDropCLS 0.076455, RDropREG 0.011202) | LR: 1.423473e-04
2025-09-21 07:25:23,945 [INFO] Train step 200/274 - Loss: 0.165707 (MSE 0.041302, SoftCE 0.257661, Corr 0.371025, Cons 0.048550, CSim 0.052062, Rank 0.616532, RDropCLS 0.074973, RDropREG 0.011035) | LR: 1.325122e-04
2025-09-21 07:25:36,614 [INFO] Train step 250/274 - Loss: 0.164641 (MSE 0.040758, SoftCE 0.256917, Corr 0.365497, Cons 0.048076, CSim 0.051834, Rank 0.615453, RDropCLS 0.073698, RDropREG 0.010922) | LR: 1.223112e-04
2025-09-21 07:25:42,767 [INFO] Epoch training completed in 69.81s with average loss 0.164141
2025-09-21 07:25:42,768 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:25:43,868 [INFO] Fold 4 - Epoch 3 - Train Loss: 0.164141 | EMA Val MSE: 0.051851 | EMA Val SoftCE: 0.268572 | EMA Val Pearson: 0.528502
2025-09-21 07:25:43,873 [INFO] Fold 4 - Epoch 3 - New best EMA model with Pearson 0.528502.
2025-09-21 07:25:43,873 [INFO] Fold 4 - Epoch 4/6 started.
2025-09-21 07:25:57,018 [INFO] Train step 50/274 - Loss: 0.149820 (MSE 0.033665, SoftCE 0.247341, Corr 0.282602, Cons 0.042243, CSim 0.047619, Rank 0.595168, RDropCLS 0.070290, RDropREG 0.010349) | LR: 1.067874e-04
2025-09-21 07:26:09,750 [INFO] Train step 100/274 - Loss: 0.149938 (MSE 0.033622, SoftCE 0.246722, Corr 0.286618, Cons 0.040566, CSim 0.047169, Rank 0.596402, RDropCLS 0.070375, RDropREG 0.010282) | LR: 9.618007e-05
2025-09-21 07:26:22,463 [INFO] Train step 150/274 - Loss: 0.150087 (MSE 0.033716, SoftCE 0.246507, Corr 0.288157, Cons 0.040366, CSim 0.047097, Rank 0.597429, RDropCLS 0.070145, RDropREG 0.010176) | LR: 8.561573e-05
2025-09-21 07:26:35,374 [INFO] Train step 200/274 - Loss: 0.149584 (MSE 0.033393, SoftCE 0.245894, Corr 0.286871, Cons 0.039830, CSim 0.046546, Rank 0.597703, RDropCLS 0.069498, RDropREG 0.010081) | LR: 7.521326e-05
2025-09-21 07:26:48,113 [INFO] Train step 250/274 - Loss: 0.149306 (MSE 0.033284, SoftCE 0.245558, Corr 0.285737, Cons 0.040026, CSim 0.046427, Rank 0.597306, RDropCLS 0.069258, RDropREG 0.010096) | LR: 6.508975e-05
2025-09-21 07:26:54,240 [INFO] Epoch training completed in 70.37s with average loss 0.148967
2025-09-21 07:26:54,241 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:26:55,385 [INFO] Fold 4 - Epoch 4 - Train Loss: 0.148967 | EMA Val MSE: 0.050546 | EMA Val SoftCE: 0.261431 | EMA Val Pearson: 0.567102
2025-09-21 07:26:55,390 [INFO] Fold 4 - Epoch 4 - New best EMA model with Pearson 0.567102.
2025-09-21 07:26:55,390 [INFO] Fold 4 - Epoch 5/6 started.
2025-09-21 07:27:08,421 [INFO] Train step 50/274 - Loss: 0.141523 (MSE 0.029535, SoftCE 0.237494, Corr 0.251476, Cons 0.036236, CSim 0.043669, Rank 0.589239, RDropCLS 0.072029, RDropREG 0.010210) | LR: 5.086032e-05
2025-09-21 07:27:21,109 [INFO] Train step 100/274 - Loss: 0.141736 (MSE 0.029545, SoftCE 0.238149, Corr 0.252225, Cons 0.035800, CSim 0.043284, Rank 0.589240, RDropCLS 0.071010, RDropREG 0.010151) | LR: 4.191050e-05
2025-09-21 07:27:33,859 [INFO] Train step 150/274 - Loss: 0.141757 (MSE 0.029684, SoftCE 0.238534, Corr 0.250348, Cons 0.035327, CSim 0.043697, Rank 0.588538, RDropCLS 0.071166, RDropREG 0.010099) | LR: 3.361443e-05
2025-09-21 07:27:46,522 [INFO] Train step 200/274 - Loss: 0.141150 (MSE 0.029272, SoftCE 0.238371, Corr 0.247204, Cons 0.035036, CSim 0.043486, Rank 0.587603, RDropCLS 0.070314, RDropREG 0.010050) | LR: 2.606547e-05
2025-09-21 07:27:59,209 [INFO] Train step 250/274 - Loss: 0.141178 (MSE 0.029348, SoftCE 0.238686, Corr 0.246346, Cons 0.034770, CSim 0.043539, Rank 0.586967, RDropCLS 0.070106, RDropREG 0.010026) | LR: 1.934857e-05
2025-09-21 07:28:05,315 [INFO] Epoch training completed in 69.92s with average loss 0.141306
2025-09-21 07:28:05,316 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:28:06,458 [INFO] Fold 4 - Epoch 5 - Train Loss: 0.141306 | EMA Val MSE: 0.050834 | EMA Val SoftCE: 0.258613 | EMA Val Pearson: 0.581920
2025-09-21 07:28:06,462 [INFO] Fold 4 - Epoch 5 - New best EMA model with Pearson 0.581920.
2025-09-21 07:28:06,462 [INFO] Fold 4 - Epoch 6/6 started.
2025-09-21 07:28:19,463 [INFO] Train step 50/274 - Loss: 0.138009 (MSE 0.027687, SoftCE 0.235409, Corr 0.233343, Cons 0.034582, CSim 0.042605, Rank 0.584332, RDropCLS 0.067760, RDropREG 0.009773) | LR: 1.109289e-05
2025-09-21 07:28:32,123 [INFO] Train step 100/274 - Loss: 0.138051 (MSE 0.027810, SoftCE 0.234841, Corr 0.234758, Cons 0.034280, CSim 0.042463, Rank 0.584953, RDropCLS 0.068348, RDropREG 0.009886) | LR: 6.743759e-06
2025-09-21 07:28:44,783 [INFO] Train step 150/274 - Loss: 0.138208 (MSE 0.027782, SoftCE 0.235671, Corr 0.233761, Cons 0.034031, CSim 0.042501, Rank 0.584998, RDropCLS 0.068525, RDropREG 0.009915) | LR: 3.444140e-06
2025-09-21 07:28:57,481 [INFO] Train step 200/274 - Loss: 0.138226 (MSE 0.027769, SoftCE 0.236261, Corr 0.232362, Cons 0.034086, CSim 0.042672, Rank 0.583872, RDropCLS 0.068877, RDropREG 0.009919) | LR: 1.231166e-06
2025-09-21 07:29:10,161 [INFO] Train step 250/274 - Loss: 0.138150 (MSE 0.027656, SoftCE 0.236377, Corr 0.231780, Cons 0.034072, CSim 0.042573, Rank 0.583899, RDropCLS 0.069013, RDropREG 0.009857) | LR: 1.297403e-07
2025-09-21 07:29:16,459 [INFO] Epoch training completed in 70.00s with average loss 0.137913
2025-09-21 07:29:16,460 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:29:17,599 [INFO] Fold 4 - Epoch 6 - Train Loss: 0.137913 | EMA Val MSE: 0.051928 | EMA Val SoftCE: 0.258746 | EMA Val Pearson: 0.586540
2025-09-21 07:29:17,604 [INFO] Fold 4 - Epoch 6 - New best EMA model with Pearson 0.586540.
2025-09-21 07:29:18,464 [INFO] Fold 4 - Final Val Pearson (no TTA): 0.586540 | Final Val MSE: 0.051928 | Final Val SoftCE: 0.258746
2025-09-21 07:29:19,233 [INFO] Fold 4 - Final Val Pearson (with TTA swap): 0.591886
2025-09-21 07:29:20,322 [INFO] OOF Pearson correlation across all folds (pre-rule, pre-calibration): 0.591993
2025-09-21 07:29:20,322 [INFO] OOF MSE across all folds (pre-rule, pre-calibration): 0.050933
2025-09-21 07:29:20,322 [INFO] Ensembling test predictions from all folds by averaging.
2025-09-21 07:29:20,322 [INFO] Applying rule-based post-processing for anchor == target -> score = 1.0
2025-09-21 07:29:20,323 [INFO] Found 24 exact anchor==target pairs in test set.
2025-09-21 07:29:20,325 [INFO] Found 255 exact anchor==target pairs in train (OOF) set; applying same rule to OOF predictions.
2025-09-21 07:29:20,325 [INFO] OOF Pearson after rule (pre-calibration): 0.593707
2025-09-21 07:29:20,325 [INFO] OOF MSE after rule (pre-calibration): 0.050905
2025-09-21 07:29:20,325 [INFO] Fitting isotonic regression (PAV) calibrator on OOF predictions.
2025-09-21 07:29:20,366 [INFO] Isotonic produced 365 monotonic blocks.
2025-09-21 07:29:20,369 [INFO] OOF Pearson after isotonic calibration: 0.606709
2025-09-21 07:29:20,369 [INFO] OOF MSE after isotonic calibration: 0.042339
2025-09-21 07:29:20,369 [INFO] Logging final validation results (OOF, post-rule, post-calibration):
2025-09-21 07:29:20,369 [INFO] Final Validation Pearson: 0.606709 | Final Validation MSE: 0.042339
2025-09-21 07:29:20,371 [INFO] Writing submission to task/us-patent-phrase-to-phrase-matching/outputs/1/submission_8.csv
2025-09-21 07:29:20,379 [INFO] Submission file created successfully.
2025-09-21 07:29:20,379 [INFO] All done (v8).
