2025-09-21 05:23:46,803 [INFO] Initialized logging and created output directories for v5 pipeline.
2025-09-21 05:23:46,803 [INFO] Setting all random seeds to 42.
2025-09-21 05:23:46,804 [INFO] Reading train data from task/us-patent-phrase-to-phrase-matching/train.csv.
2025-09-21 05:23:46,842 [INFO] Reading test data from task/us-patent-phrase-to-phrase-matching/test.csv.
2025-09-21 05:23:46,848 [INFO] Train shape: (32825, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 05:23:46,848 [INFO] Test shape:  (3648, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 05:23:46,853 [INFO] Train missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 05:23:46,853 [INFO] Test missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 05:23:46,853 [INFO] Building vocabulary from segmented corpus.
2025-09-21 05:23:47,200 [INFO] Built vocabulary of size 9042.
2025-09-21 05:23:47,200 [INFO] Tokenizing and numericalizing dataframe with 32825 rows (with token types).
2025-09-21 05:23:47,321 [INFO] Processed 5000 rows.
2025-09-21 05:23:47,441 [INFO] Processed 10000 rows.
2025-09-21 05:23:47,561 [INFO] Processed 15000 rows.
2025-09-21 05:23:47,681 [INFO] Processed 20000 rows.
2025-09-21 05:23:47,801 [INFO] Processed 25000 rows.
2025-09-21 05:23:47,921 [INFO] Processed 30000 rows.
2025-09-21 05:23:47,989 [INFO] Tokenizing and numericalizing dataframe with 3648 rows (with token types).
2025-09-21 05:23:48,077 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 32825 rows (anchor<->target).
2025-09-21 05:23:48,196 [INFO] Processed 5000 swapped rows.
2025-09-21 05:23:48,319 [INFO] Processed 10000 swapped rows.
2025-09-21 05:23:48,438 [INFO] Processed 15000 swapped rows.
2025-09-21 05:23:48,556 [INFO] Processed 20000 swapped rows.
2025-09-21 05:23:48,674 [INFO] Processed 25000 swapped rows.
2025-09-21 05:23:48,793 [INFO] Processed 30000 swapped rows.
2025-09-21 05:23:48,860 [INFO] Prepared swapped augmentation arrays for training.
2025-09-21 05:23:48,862 [INFO] Computing handcrafted similarity features.
2025-09-21 05:23:49,289 [INFO] Computed handcrafted features for 5000 rows.
2025-09-21 05:23:49,715 [INFO] Computed handcrafted features for 10000 rows.
2025-09-21 05:23:50,146 [INFO] Computed handcrafted features for 15000 rows.
2025-09-21 05:23:50,577 [INFO] Computed handcrafted features for 20000 rows.
2025-09-21 05:23:51,006 [INFO] Computed handcrafted features for 25000 rows.
2025-09-21 05:23:51,432 [INFO] Computed handcrafted features for 30000 rows.
2025-09-21 05:23:51,679 [INFO] Finished computing handcrafted features.
2025-09-21 05:23:51,679 [INFO] Computing handcrafted similarity features.
2025-09-21 05:23:51,994 [INFO] Finished computing handcrafted features.
2025-09-21 05:23:51,994 [INFO] Handcrafted feature dimension: 8
2025-09-21 05:23:51,994 [INFO] Labels statistics: min=0.0, max=1.0, mean=0.3619, std=0.2588
2025-09-21 05:23:51,999 [INFO] Found 106 unique context codes across train+test.
2025-09-21 05:23:52,265 [INFO] Class counts: [6774.0, 10306.0, 11068.0, 3634.0, 1043.0] | CE weights: [0.15397107601165771, 0.10120318084955215, 0.0942356288433075, 0.2870115637779236, 1.0]
2025-09-21 05:23:52,267 [INFO] Creating stratified folds on 5-class bins.
2025-09-21 05:23:52,285 [INFO] Fold 0: 6567 samples.
2025-09-21 05:23:52,285 [INFO] Fold 1: 6566 samples.
2025-09-21 05:23:52,285 [INFO] Fold 2: 6566 samples.
2025-09-21 05:23:52,285 [INFO] Fold 3: 6564 samples.
2025-09-21 05:23:52,285 [INFO] Fold 4: 6562 samples.
2025-09-21 05:23:52,285 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 05:23:52,286 [INFO] ========== Fold 1/5 ==========
2025-09-21 05:23:52,349 [INFO] Applied swap augmentation. New train size: 52516
2025-09-21 05:23:52,351 [INFO] Train split: 52516 samples; Val split: 6567 samples.
2025-09-21 05:23:52,352 [INFO] Initializing PatentDataset with 52516 samples.
2025-09-21 05:23:52,352 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 05:23:52,352 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8
2025-09-21 05:23:53,496 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=137, total_steps=1370.
2025-09-21 05:23:53,497 [INFO] Fold 0 - Epoch 1/5 started.
2025-09-21 05:23:58,324 [INFO] Train step 50/274 - Loss: 0.987018 (MSE 0.086334, CE 2.431243, Corr 0.980928, Cons 0.169124, CSim 0.134854) | LR: 7.299270e-05
2025-09-21 05:24:02,156 [INFO] Train step 100/274 - Loss: 0.880580 (MSE 0.079433, CE 2.157132, Corr 0.920167, Cons 0.144080, CSim 0.108011) | LR: 1.459854e-04
2025-09-21 05:24:05,981 [INFO] Train step 150/274 - Loss: 0.830333 (MSE 0.076982, CE 2.027211, Corr 0.888791, Cons 0.129923, CSim 0.095354) | LR: 1.999451e-04
2025-09-21 05:24:09,821 [INFO] Train step 200/274 - Loss: 0.797619 (MSE 0.074162, CE 1.945994, Corr 0.862991, Cons 0.115789, CSim 0.087534) | LR: 1.987144e-04
2025-09-21 05:24:13,645 [INFO] Train step 250/274 - Loss: 0.772795 (MSE 0.072568, CE 1.884978, Corr 0.835393, Cons 0.107018, CSim 0.083383) | LR: 1.958838e-04
2025-09-21 05:24:15,508 [INFO] Epoch training completed in 22.01s with average loss 0.761467
2025-09-21 05:24:16,651 [INFO] Fold 0 - Epoch 1 - Train Loss: 0.761467 | Val MSE: 0.060269 | Val CE: 1.523734 | Val Pearson: 0.381976
2025-09-21 05:24:16,653 [INFO] Fold 0 - Epoch 1 - New best model with Pearson 0.381976.
2025-09-21 05:24:16,653 [INFO] Fold 0 - Epoch 2/5 started.
2025-09-21 05:24:20,648 [INFO] Train step 50/274 - Loss: 0.628994 (MSE 0.059027, CE 1.546829, Corr 0.635410, Cons 0.058688, CSim 0.062651) | LR: 1.888623e-04
2025-09-21 05:24:24,450 [INFO] Train step 100/274 - Loss: 0.611955 (MSE 0.057841, CE 1.508735, Corr 0.597507, Cons 0.056829, CSim 0.062953) | LR: 1.823151e-04
2025-09-21 05:24:28,299 [INFO] Train step 150/274 - Loss: 0.605371 (MSE 0.056922, CE 1.494567, Corr 0.584167, Cons 0.055061, CSim 0.062627) | LR: 1.744337e-04
2025-09-21 05:24:32,106 [INFO] Train step 200/274 - Loss: 0.597642 (MSE 0.055979, CE 1.477716, Corr 0.568659, Cons 0.052588, CSim 0.062164) | LR: 1.653459e-04
2025-09-21 05:24:35,931 [INFO] Train step 250/274 - Loss: 0.589870 (MSE 0.055010, CE 1.460916, Corr 0.552567, Cons 0.050104, CSim 0.061730) | LR: 1.551990e-04
2025-09-21 05:24:37,822 [INFO] Epoch training completed in 21.17s with average loss 0.585512
2025-09-21 05:24:38,972 [INFO] Fold 0 - Epoch 2 - Train Loss: 0.585512 | Val MSE: 0.061617 | Val CE: 1.432323 | Val Pearson: 0.501783
2025-09-21 05:24:38,973 [INFO] Fold 0 - Epoch 2 - New best model with Pearson 0.501783.
2025-09-21 05:24:38,973 [INFO] Fold 0 - Epoch 3/5 started.
2025-09-21 05:24:43,019 [INFO] Train step 50/274 - Loss: 0.513318 (MSE 0.044702, CE 1.291497, Corr 0.414813, Cons 0.035112, CSim 0.056418) | LR: 1.385918e-04
2025-09-21 05:24:46,866 [INFO] Train step 100/274 - Loss: 0.506960 (MSE 0.044214, CE 1.276431, Corr 0.405335, Cons 0.033637, CSim 0.055829) | LR: 1.265581e-04
2025-09-21 05:24:50,722 [INFO] Train step 150/274 - Loss: 0.505986 (MSE 0.043996, CE 1.274499, Corr 0.403462, Cons 0.032584, CSim 0.055699) | LR: 1.140939e-04
2025-09-21 05:24:54,560 [INFO] Train step 200/274 - Loss: 0.502802 (MSE 0.043732, CE 1.267159, Corr 0.397677, Cons 0.032159, CSim 0.055723) | LR: 1.014013e-04
2025-09-21 05:24:58,411 [INFO] Train step 250/274 - Loss: 0.498936 (MSE 0.042995, CE 1.258991, Corr 0.390520, Cons 0.031689, CSim 0.055333) | LR: 8.868601e-05
2025-09-21 05:25:00,331 [INFO] Epoch training completed in 21.36s with average loss 0.498065
2025-09-21 05:25:01,492 [INFO] Fold 0 - Epoch 3 - Train Loss: 0.498065 | Val MSE: 0.057899 | Val CE: 1.384541 | Val Pearson: 0.543859
2025-09-21 05:25:01,493 [INFO] Fold 0 - Epoch 3 - New best model with Pearson 0.543859.
2025-09-21 05:25:01,493 [INFO] Fold 0 - Epoch 4/5 started.
2025-09-21 05:25:05,510 [INFO] Train step 50/274 - Loss: 0.456195 (MSE 0.036468, CE 1.162285, Corr 0.328011, Cons 0.028267, CSim 0.050618) | LR: 7.026373e-05
2025-09-21 05:25:09,339 [INFO] Train step 100/274 - Loss: 0.449945 (MSE 0.036558, CE 1.145164, Corr 0.324340, Cons 0.026883, CSim 0.050921) | LR: 5.837424e-05
2025-09-21 05:25:13,175 [INFO] Train step 150/274 - Loss: 0.449011 (MSE 0.036436, CE 1.143509, Corr 0.320833, Cons 0.025975, CSim 0.051153) | LR: 4.715941e-05
2025-09-21 05:25:17,004 [INFO] Train step 200/274 - Loss: 0.446039 (MSE 0.035949, CE 1.137008, Corr 0.315762, Cons 0.025418, CSim 0.050912) | LR: 3.680102e-05
2025-09-21 05:25:20,841 [INFO] Train step 250/274 - Loss: 0.443849 (MSE 0.035523, CE 1.132288, Corr 0.312129, Cons 0.025188, CSim 0.050729) | LR: 2.746695e-05
2025-09-21 05:25:22,755 [INFO] Epoch training completed in 21.26s with average loss 0.443682
2025-09-21 05:25:23,940 [INFO] Fold 0 - Epoch 4 - Train Loss: 0.443682 | Val MSE: 0.052658 | Val CE: 1.369478 | Val Pearson: 0.569226
2025-09-21 05:25:23,942 [INFO] Fold 0 - Epoch 4 - New best model with Pearson 0.569226.
2025-09-21 05:25:23,942 [INFO] Fold 0 - Epoch 5/5 started.
2025-09-21 05:25:27,974 [INFO] Train step 50/274 - Loss: 0.415349 (MSE 0.032187, CE 1.063757, Corr 0.279038, Cons 0.025657, CSim 0.049886) | LR: 1.584961e-05
2025-09-21 05:25:31,826 [INFO] Train step 100/274 - Loss: 0.415876 (MSE 0.032822, CE 1.063620, Corr 0.281678, Cons 0.025767, CSim 0.050142) | LR: 9.667564e-06
2025-09-21 05:25:35,684 [INFO] Train step 150/274 - Loss: 0.418188 (MSE 0.032621, CE 1.070620, Corr 0.281675, Cons 0.025420, CSim 0.049735) | LR: 4.949614e-06
2025-09-21 05:25:39,540 [INFO] Train step 200/274 - Loss: 0.420970 (MSE 0.032791, CE 1.077605, Corr 0.284909, Cons 0.024970, CSim 0.049622) | LR: 1.772230e-06
2025-09-21 05:25:43,384 [INFO] Train step 250/274 - Loss: 0.420279 (MSE 0.032676, CE 1.076102, Corr 0.283718, Cons 0.024804, CSim 0.049560) | LR: 1.869092e-07
2025-09-21 05:25:45,296 [INFO] Epoch training completed in 21.35s with average loss 0.420811
2025-09-21 05:25:46,485 [INFO] Fold 0 - Epoch 5 - Train Loss: 0.420811 | Val MSE: 0.053306 | Val CE: 1.375396 | Val Pearson: 0.570668
2025-09-21 05:25:46,487 [INFO] Fold 0 - Epoch 5 - New best model with Pearson 0.570668.
2025-09-21 05:25:47,620 [INFO] Fold 0 - Final Val Pearson: 0.570668 | Final Val MSE: 0.053306 | Final Val CE: 1.375396
2025-09-21 05:25:48,303 [INFO] ========== Fold 2/5 ==========
2025-09-21 05:25:48,364 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 05:25:48,373 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 05:25:48,373 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 05:25:48,373 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 05:25:48,375 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8
2025-09-21 05:25:48,525 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=137, total_steps=1370.
2025-09-21 05:25:48,525 [INFO] Fold 1 - Epoch 1/5 started.
2025-09-21 05:25:52,543 [INFO] Train step 50/274 - Loss: 0.984422 (MSE 0.085529, CE 2.430283, Corr 0.967263, Cons 0.139769, CSim 0.132101) | LR: 7.299270e-05
2025-09-21 05:25:56,364 [INFO] Train step 100/274 - Loss: 0.873489 (MSE 0.079926, CE 2.138980, Corr 0.913426, Cons 0.129603, CSim 0.104403) | LR: 1.459854e-04
2025-09-21 05:26:00,198 [INFO] Train step 150/274 - Loss: 0.821819 (MSE 0.075901, CE 2.007819, Corr 0.880261, Cons 0.117078, CSim 0.091480) | LR: 1.999451e-04
2025-09-21 05:26:04,009 [INFO] Train step 200/274 - Loss: 0.788516 (MSE 0.073717, CE 1.923443, Corr 0.854254, Cons 0.107763, CSim 0.085421) | LR: 1.987144e-04
2025-09-21 05:26:07,840 [INFO] Train step 250/274 - Loss: 0.762959 (MSE 0.071723, CE 1.861716, Corr 0.823043, Cons 0.099460, CSim 0.081555) | LR: 1.958838e-04
2025-09-21 05:26:09,748 [INFO] Epoch training completed in 21.22s with average loss 0.753569
2025-09-21 05:26:10,873 [INFO] Fold 1 - Epoch 1 - Train Loss: 0.753569 | Val MSE: 0.067204 | Val CE: 1.591495 | Val Pearson: 0.389209
2025-09-21 05:26:10,874 [INFO] Fold 1 - Epoch 1 - New best model with Pearson 0.389209.
2025-09-21 05:26:10,874 [INFO] Fold 1 - Epoch 2/5 started.
2025-09-21 05:26:14,849 [INFO] Train step 50/274 - Loss: 0.611931 (MSE 0.057803, CE 1.508877, Corr 0.596885, Cons 0.056623, CSim 0.062993) | LR: 1.888623e-04
2025-09-21 05:26:18,696 [INFO] Train step 100/274 - Loss: 0.606148 (MSE 0.057173, CE 1.497216, Corr 0.579904, Cons 0.056223, CSim 0.063202) | LR: 1.823151e-04
2025-09-21 05:26:22,540 [INFO] Train step 150/274 - Loss: 0.599811 (MSE 0.055922, CE 1.484641, Corr 0.565830, Cons 0.052217, CSim 0.062373) | LR: 1.744337e-04
2025-09-21 05:26:26,385 [INFO] Train step 200/274 - Loss: 0.592768 (MSE 0.055508, CE 1.468230, Corr 0.553215, Cons 0.049509, CSim 0.062208) | LR: 1.653459e-04
2025-09-21 05:26:30,228 [INFO] Train step 250/274 - Loss: 0.585284 (MSE 0.054616, CE 1.451366, Corr 0.540291, Cons 0.047777, CSim 0.061759) | LR: 1.551990e-04
2025-09-21 05:26:32,147 [INFO] Epoch training completed in 21.27s with average loss 0.581690
2025-09-21 05:26:33,313 [INFO] Fold 1 - Epoch 2 - Train Loss: 0.581690 | Val MSE: 0.057631 | Val CE: 1.405731 | Val Pearson: 0.495615
2025-09-21 05:26:33,314 [INFO] Fold 1 - Epoch 2 - New best model with Pearson 0.495615.
2025-09-21 05:26:33,314 [INFO] Fold 1 - Epoch 3/5 started.
2025-09-21 05:26:37,333 [INFO] Train step 50/274 - Loss: 0.516291 (MSE 0.044398, CE 1.300895, Corr 0.413758, Cons 0.034837, CSim 0.055225) | LR: 1.385918e-04
2025-09-21 05:26:41,146 [INFO] Train step 100/274 - Loss: 0.509165 (MSE 0.044052, CE 1.283210, Corr 0.404337, Cons 0.035200, CSim 0.055237) | LR: 1.265581e-04
2025-09-21 05:26:44,972 [INFO] Train step 150/274 - Loss: 0.506615 (MSE 0.043742, CE 1.277639, Corr 0.399310, Cons 0.033868, CSim 0.055218) | LR: 1.140939e-04
2025-09-21 05:26:48,801 [INFO] Train step 200/274 - Loss: 0.503783 (MSE 0.043247, CE 1.271469, Corr 0.394834, Cons 0.032514, CSim 0.054930) | LR: 1.014013e-04
2025-09-21 05:26:52,650 [INFO] Train step 250/274 - Loss: 0.499652 (MSE 0.042736, CE 1.261978, Corr 0.388543, Cons 0.031920, CSim 0.054665) | LR: 8.868601e-05
2025-09-21 05:26:54,545 [INFO] Epoch training completed in 21.23s with average loss 0.497436
2025-09-21 05:26:55,706 [INFO] Fold 1 - Epoch 3 - Train Loss: 0.497436 | Val MSE: 0.054977 | Val CE: 1.429561 | Val Pearson: 0.537695
2025-09-21 05:26:55,708 [INFO] Fold 1 - Epoch 3 - New best model with Pearson 0.537695.
2025-09-21 05:26:55,708 [INFO] Fold 1 - Epoch 4/5 started.
2025-09-21 05:26:59,722 [INFO] Train step 50/274 - Loss: 0.457885 (MSE 0.036830, CE 1.166376, Corr 0.327945, Cons 0.029284, CSim 0.051512) | LR: 7.026373e-05
2025-09-21 05:27:03,547 [INFO] Train step 100/274 - Loss: 0.451868 (MSE 0.035915, CE 1.153117, Corr 0.317599, Cons 0.028279, CSim 0.050994) | LR: 5.837424e-05
2025-09-21 05:27:07,331 [INFO] Train step 150/274 - Loss: 0.447596 (MSE 0.035742, CE 1.142396, Corr 0.312486, Cons 0.027445, CSim 0.051027) | LR: 4.715941e-05
2025-09-21 05:27:11,143 [INFO] Train step 200/274 - Loss: 0.446170 (MSE 0.035535, CE 1.138989, Corr 0.311147, Cons 0.027392, CSim 0.050801) | LR: 3.680102e-05
2025-09-21 05:27:14,969 [INFO] Train step 250/274 - Loss: 0.443802 (MSE 0.035270, CE 1.133405, Corr 0.307955, Cons 0.027120, CSim 0.050666) | LR: 2.746695e-05
2025-09-21 05:27:16,870 [INFO] Epoch training completed in 21.16s with average loss 0.444634
2025-09-21 05:27:18,020 [INFO] Fold 1 - Epoch 4 - Train Loss: 0.444634 | Val MSE: 0.054366 | Val CE: 1.390393 | Val Pearson: 0.557172
2025-09-21 05:27:18,022 [INFO] Fold 1 - Epoch 4 - New best model with Pearson 0.557172.
2025-09-21 05:27:18,022 [INFO] Fold 1 - Epoch 5/5 started.
2025-09-21 05:27:22,038 [INFO] Train step 50/274 - Loss: 0.414614 (MSE 0.031783, CE 1.065152, Corr 0.266922, Cons 0.025474, CSim 0.049331) | LR: 1.584961e-05
2025-09-21 05:27:25,877 [INFO] Train step 100/274 - Loss: 0.417950 (MSE 0.032235, CE 1.072772, Corr 0.272411, Cons 0.024785, CSim 0.049246) | LR: 9.667564e-06
2025-09-21 05:27:29,715 [INFO] Train step 150/274 - Loss: 0.419218 (MSE 0.032382, CE 1.075041, Corr 0.277476, Cons 0.024467, CSim 0.049134) | LR: 4.949614e-06
2025-09-21 05:27:33,530 [INFO] Train step 200/274 - Loss: 0.419415 (MSE 0.032278, CE 1.075621, Corr 0.278183, Cons 0.024418, CSim 0.049041) | LR: 1.772230e-06
2025-09-21 05:27:37,344 [INFO] Train step 250/274 - Loss: 0.419706 (MSE 0.032243, CE 1.076461, Corr 0.278497, Cons 0.024534, CSim 0.048809) | LR: 1.869092e-07
2025-09-21 05:27:39,257 [INFO] Epoch training completed in 21.23s with average loss 0.420272
2025-09-21 05:27:40,408 [INFO] Fold 1 - Epoch 5 - Train Loss: 0.420272 | Val MSE: 0.054473 | Val CE: 1.396590 | Val Pearson: 0.558657
2025-09-21 05:27:40,409 [INFO] Fold 1 - Epoch 5 - New best model with Pearson 0.558657.
2025-09-21 05:27:41,555 [INFO] Fold 1 - Final Val Pearson: 0.558657 | Final Val MSE: 0.054473 | Final Val CE: 1.396590
2025-09-21 05:27:42,230 [INFO] ========== Fold 3/5 ==========
2025-09-21 05:27:42,282 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 05:27:42,285 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 05:27:42,285 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 05:27:42,285 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 05:27:42,286 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8
2025-09-21 05:27:42,435 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=137, total_steps=1370.
2025-09-21 05:27:42,435 [INFO] Fold 2 - Epoch 1/5 started.
2025-09-21 05:27:46,477 [INFO] Train step 50/274 - Loss: 0.949976 (MSE 0.084555, CE 2.333101, Corr 0.956804, Cons 0.153187, CSim 0.145549) | LR: 7.299270e-05
2025-09-21 05:27:50,324 [INFO] Train step 100/274 - Loss: 0.860465 (MSE 0.079439, CE 2.102379, Corr 0.906011, Cons 0.138350, CSim 0.113865) | LR: 1.459854e-04
2025-09-21 05:27:54,172 [INFO] Train step 150/274 - Loss: 0.814221 (MSE 0.076895, CE 1.983342, Corr 0.879169, Cons 0.124856, CSim 0.098575) | LR: 1.999451e-04
2025-09-21 05:27:58,016 [INFO] Train step 200/274 - Loss: 0.784282 (MSE 0.074303, CE 1.910107, Corr 0.851239, Cons 0.113089, CSim 0.090342) | LR: 1.987144e-04
2025-09-21 05:28:01,869 [INFO] Train step 250/274 - Loss: 0.761784 (MSE 0.072237, CE 1.856340, Corr 0.825230, Cons 0.102909, CSim 0.085168) | LR: 1.958838e-04
2025-09-21 05:28:03,784 [INFO] Epoch training completed in 21.35s with average loss 0.752295
2025-09-21 05:28:04,956 [INFO] Fold 2 - Epoch 1 - Train Loss: 0.752295 | Val MSE: 0.062250 | Val CE: 1.551462 | Val Pearson: 0.359264
2025-09-21 05:28:04,958 [INFO] Fold 2 - Epoch 1 - New best model with Pearson 0.359264.
2025-09-21 05:28:04,958 [INFO] Fold 2 - Epoch 2/5 started.
2025-09-21 05:28:09,022 [INFO] Train step 50/274 - Loss: 0.612434 (MSE 0.057625, CE 1.510210, Corr 0.599343, Cons 0.057859, CSim 0.061239) | LR: 1.888623e-04
2025-09-21 05:28:12,869 [INFO] Train step 100/274 - Loss: 0.608829 (MSE 0.058146, CE 1.500291, Corr 0.593953, Cons 0.056607, CSim 0.061961) | LR: 1.823151e-04
2025-09-21 05:28:16,717 [INFO] Train step 150/274 - Loss: 0.602503 (MSE 0.057493, CE 1.486836, Corr 0.579117, Cons 0.053107, CSim 0.061960) | LR: 1.744337e-04
2025-09-21 05:28:20,581 [INFO] Train step 200/274 - Loss: 0.596335 (MSE 0.056409, CE 1.474526, Corr 0.564344, Cons 0.050635, CSim 0.061316) | LR: 1.653459e-04
2025-09-21 05:28:24,430 [INFO] Train step 250/274 - Loss: 0.590621 (MSE 0.055490, CE 1.462723, Corr 0.551688, Cons 0.048623, CSim 0.060809) | LR: 1.551990e-04
2025-09-21 05:28:26,336 [INFO] Epoch training completed in 21.38s with average loss 0.588303
2025-09-21 05:28:27,484 [INFO] Fold 2 - Epoch 2 - Train Loss: 0.588303 | Val MSE: 0.054800 | Val CE: 1.423192 | Val Pearson: 0.510613
2025-09-21 05:28:27,486 [INFO] Fold 2 - Epoch 2 - New best model with Pearson 0.510613.
2025-09-21 05:28:27,486 [INFO] Fold 2 - Epoch 3/5 started.
2025-09-21 05:28:31,536 [INFO] Train step 50/274 - Loss: 0.516071 (MSE 0.045042, CE 1.298818, Corr 0.416365, Cons 0.033320, CSim 0.054714) | LR: 1.385918e-04
2025-09-21 05:28:35,394 [INFO] Train step 100/274 - Loss: 0.515140 (MSE 0.044816, CE 1.297459, Corr 0.411824, Cons 0.033902, CSim 0.055131) | LR: 1.265581e-04
2025-09-21 05:28:39,248 [INFO] Train step 150/274 - Loss: 0.511818 (MSE 0.044265, CE 1.290205, Corr 0.406529, Cons 0.032331, CSim 0.054628) | LR: 1.140939e-04
2025-09-21 05:28:43,109 [INFO] Train step 200/274 - Loss: 0.509956 (MSE 0.043961, CE 1.286153, Corr 0.403316, Cons 0.032115, CSim 0.054315) | LR: 1.014013e-04
2025-09-21 05:28:46,970 [INFO] Train step 250/274 - Loss: 0.504603 (MSE 0.043347, CE 1.273689, Corr 0.395493, Cons 0.031543, CSim 0.054017) | LR: 8.868601e-05
2025-09-21 05:28:48,874 [INFO] Epoch training completed in 21.39s with average loss 0.502253
2025-09-21 05:28:50,039 [INFO] Fold 2 - Epoch 3 - Train Loss: 0.502253 | Val MSE: 0.051636 | Val CE: 1.390290 | Val Pearson: 0.567899
2025-09-21 05:28:50,041 [INFO] Fold 2 - Epoch 3 - New best model with Pearson 0.567899.
2025-09-21 05:28:50,041 [INFO] Fold 2 - Epoch 4/5 started.
2025-09-21 05:28:54,101 [INFO] Train step 50/274 - Loss: 0.456134 (MSE 0.037004, CE 1.162201, Corr 0.324075, Cons 0.026543, CSim 0.051082) | LR: 7.026373e-05
2025-09-21 05:28:57,951 [INFO] Train step 100/274 - Loss: 0.456134 (MSE 0.036773, CE 1.162135, Corr 0.326333, Cons 0.027360, CSim 0.050151) | LR: 5.837424e-05
2025-09-21 05:29:01,769 [INFO] Train step 150/274 - Loss: 0.453925 (MSE 0.036348, CE 1.157690, Corr 0.321176, Cons 0.027525, CSim 0.049960) | LR: 4.715941e-05
2025-09-21 05:29:05,624 [INFO] Train step 200/274 - Loss: 0.449741 (MSE 0.035856, CE 1.147777, Corr 0.315904, Cons 0.027324, CSim 0.049589) | LR: 3.680102e-05
2025-09-21 05:29:09,474 [INFO] Train step 250/274 - Loss: 0.447849 (MSE 0.035827, CE 1.142700, Corr 0.314638, Cons 0.027479, CSim 0.049566) | LR: 2.746695e-05
2025-09-21 05:29:11,371 [INFO] Epoch training completed in 21.33s with average loss 0.447024
2025-09-21 05:29:12,487 [INFO] Fold 2 - Epoch 4 - Train Loss: 0.447024 | Val MSE: 0.052463 | Val CE: 1.396322 | Val Pearson: 0.586733
2025-09-21 05:29:12,490 [INFO] Fold 2 - Epoch 4 - New best model with Pearson 0.586733.
2025-09-21 05:29:12,490 [INFO] Fold 2 - Epoch 5/5 started.
2025-09-21 05:29:16,523 [INFO] Train step 50/274 - Loss: 0.432410 (MSE 0.033519, CE 1.107306, Corr 0.293124, Cons 0.026183, CSim 0.048866) | LR: 1.584961e-05
2025-09-21 05:29:20,377 [INFO] Train step 100/274 - Loss: 0.425859 (MSE 0.032718, CE 1.091859, Corr 0.284873, Cons 0.025314, CSim 0.048347) | LR: 9.667564e-06
2025-09-21 05:29:24,188 [INFO] Train step 150/274 - Loss: 0.425614 (MSE 0.032751, CE 1.090841, Corr 0.286063, Cons 0.025149, CSim 0.048371) | LR: 4.949614e-06
2025-09-21 05:29:27,949 [INFO] Train step 200/274 - Loss: 0.423563 (MSE 0.032619, CE 1.085845, Corr 0.283181, Cons 0.025054, CSim 0.048423) | LR: 1.772230e-06
2025-09-21 05:29:31,682 [INFO] Train step 250/274 - Loss: 0.422675 (MSE 0.032707, CE 1.083176, Corr 0.283095, Cons 0.025176, CSim 0.048473) | LR: 1.869092e-07
2025-09-21 05:29:33,589 [INFO] Epoch training completed in 21.10s with average loss 0.422814
2025-09-21 05:29:34,721 [INFO] Fold 2 - Epoch 5 - Train Loss: 0.422814 | Val MSE: 0.053134 | Val CE: 1.406474 | Val Pearson: 0.586786
2025-09-21 05:29:34,723 [INFO] Fold 2 - Epoch 5 - New best model with Pearson 0.586786.
2025-09-21 05:29:35,856 [INFO] Fold 2 - Final Val Pearson: 0.586786 | Final Val MSE: 0.053134 | Final Val CE: 1.406474
2025-09-21 05:29:36,531 [INFO] ========== Fold 4/5 ==========
2025-09-21 05:29:40,058 [INFO] Applied swap augmentation. New train size: 52522
2025-09-21 05:29:40,062 [INFO] Train split: 52522 samples; Val split: 6564 samples.
2025-09-21 05:29:40,063 [INFO] Initializing PatentDataset with 52522 samples.
2025-09-21 05:29:40,063 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 05:29:40,066 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8
2025-09-21 05:29:40,216 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=137, total_steps=1370.
2025-09-21 05:29:40,216 [INFO] Fold 3 - Epoch 1/5 started.
2025-09-21 05:29:44,214 [INFO] Train step 50/274 - Loss: 0.978990 (MSE 0.093995, CE 2.402695, Corr 0.964698, Cons 0.139733, CSim 0.127578) | LR: 7.299270e-05
2025-09-21 05:29:48,038 [INFO] Train step 100/274 - Loss: 0.879854 (MSE 0.084652, CE 2.149359, Corr 0.914681, Cons 0.136363, CSim 0.102349) | LR: 1.459854e-04
2025-09-21 05:29:51,829 [INFO] Train step 150/274 - Loss: 0.827373 (MSE 0.079550, CE 2.017208, Corr 0.883054, Cons 0.121656, CSim 0.090415) | LR: 1.999451e-04
2025-09-21 05:29:55,634 [INFO] Train step 200/274 - Loss: 0.793211 (MSE 0.076405, CE 1.932107, Corr 0.856705, Cons 0.110121, CSim 0.084242) | LR: 1.987144e-04
2025-09-21 05:29:59,386 [INFO] Train step 250/274 - Loss: 0.768482 (MSE 0.074105, CE 1.872413, Corr 0.828969, Cons 0.101944, CSim 0.080460) | LR: 1.958838e-04
2025-09-21 05:30:01,247 [INFO] Epoch training completed in 21.03s with average loss 0.757903
2025-09-21 05:30:02,402 [INFO] Fold 3 - Epoch 1 - Train Loss: 0.757903 | Val MSE: 0.065558 | Val CE: 1.606221 | Val Pearson: 0.366296
2025-09-21 05:30:02,403 [INFO] Fold 3 - Epoch 1 - New best model with Pearson 0.366296.
2025-09-21 05:30:02,403 [INFO] Fold 3 - Epoch 2/5 started.
2025-09-21 05:30:06,353 [INFO] Train step 50/274 - Loss: 0.624312 (MSE 0.058037, CE 1.540666, Corr 0.612406, Cons 0.053777, CSim 0.061808) | LR: 1.888623e-04
2025-09-21 05:30:10,100 [INFO] Train step 100/274 - Loss: 0.614915 (MSE 0.056964, CE 1.520684, Corr 0.589633, Cons 0.054706, CSim 0.061610) | LR: 1.823151e-04
2025-09-21 05:30:13,869 [INFO] Train step 150/274 - Loss: 0.605744 (MSE 0.056022, CE 1.501017, Corr 0.568336, Cons 0.052034, CSim 0.061371) | LR: 1.744337e-04
2025-09-21 05:30:17,630 [INFO] Train step 200/274 - Loss: 0.595641 (MSE 0.055107, CE 1.477477, Corr 0.552087, Cons 0.050050, CSim 0.060954) | LR: 1.653459e-04
2025-09-21 05:30:21,385 [INFO] Train step 250/274 - Loss: 0.587804 (MSE 0.054220, CE 1.459956, Corr 0.537610, Cons 0.048004, CSim 0.060589) | LR: 1.551990e-04
2025-09-21 05:30:23,273 [INFO] Epoch training completed in 20.87s with average loss 0.585235
2025-09-21 05:30:24,428 [INFO] Fold 3 - Epoch 2 - Train Loss: 0.585235 | Val MSE: 0.061864 | Val CE: 1.460499 | Val Pearson: 0.495228
2025-09-21 05:30:24,429 [INFO] Fold 3 - Epoch 2 - New best model with Pearson 0.495228.
2025-09-21 05:30:24,430 [INFO] Fold 3 - Epoch 3/5 started.
2025-09-21 05:30:28,402 [INFO] Train step 50/274 - Loss: 0.520070 (MSE 0.044255, CE 1.311937, Corr 0.414220, Cons 0.036168, CSim 0.053810) | LR: 1.385918e-04
2025-09-21 05:30:32,201 [INFO] Train step 100/274 - Loss: 0.513402 (MSE 0.043677, CE 1.295632, Corr 0.406696, Cons 0.034193, CSim 0.053769) | LR: 1.265581e-04
2025-09-21 05:30:35,956 [INFO] Train step 150/274 - Loss: 0.508486 (MSE 0.043078, CE 1.284988, Corr 0.396236, Cons 0.033377, CSim 0.053629) | LR: 1.140939e-04
2025-09-21 05:30:39,725 [INFO] Train step 200/274 - Loss: 0.503067 (MSE 0.042582, CE 1.272486, Corr 0.386866, Cons 0.032767, CSim 0.053451) | LR: 1.014013e-04
2025-09-21 05:30:43,467 [INFO] Train step 250/274 - Loss: 0.497959 (MSE 0.042188, CE 1.260069, Corr 0.380122, Cons 0.032616, CSim 0.053377) | LR: 8.868601e-05
2025-09-21 05:30:45,348 [INFO] Epoch training completed in 20.92s with average loss 0.496884
2025-09-21 05:30:46,504 [INFO] Fold 3 - Epoch 3 - Train Loss: 0.496884 | Val MSE: 0.050349 | Val CE: 1.360482 | Val Pearson: 0.560479
2025-09-21 05:30:46,506 [INFO] Fold 3 - Epoch 3 - New best model with Pearson 0.560479.
2025-09-21 05:30:46,506 [INFO] Fold 3 - Epoch 4/5 started.
2025-09-21 05:30:50,494 [INFO] Train step 50/274 - Loss: 0.448902 (MSE 0.034972, CE 1.148655, Corr 0.307810, Cons 0.027418, CSim 0.049302) | LR: 7.026373e-05
2025-09-21 05:30:54,319 [INFO] Train step 100/274 - Loss: 0.448762 (MSE 0.035278, CE 1.147271, Corr 0.309709, Cons 0.027825, CSim 0.049612) | LR: 5.837424e-05
2025-09-21 05:30:58,144 [INFO] Train step 150/274 - Loss: 0.444781 (MSE 0.035096, CE 1.137228, Corr 0.305145, Cons 0.027656, CSim 0.049662) | LR: 4.715941e-05
2025-09-21 05:31:01,966 [INFO] Train step 200/274 - Loss: 0.443435 (MSE 0.034876, CE 1.134158, Corr 0.303548, Cons 0.027094, CSim 0.049441) | LR: 3.680102e-05
2025-09-21 05:31:05,811 [INFO] Train step 250/274 - Loss: 0.442529 (MSE 0.034672, CE 1.132310, Corr 0.301915, Cons 0.026685, CSim 0.049281) | LR: 2.746695e-05
2025-09-21 05:31:07,726 [INFO] Epoch training completed in 21.22s with average loss 0.442617
2025-09-21 05:31:08,878 [INFO] Fold 3 - Epoch 4 - Train Loss: 0.442617 | Val MSE: 0.053214 | Val CE: 1.366324 | Val Pearson: 0.569981
2025-09-21 05:31:08,880 [INFO] Fold 3 - Epoch 4 - New best model with Pearson 0.569981.
2025-09-21 05:31:08,880 [INFO] Fold 3 - Epoch 5/5 started.
2025-09-21 05:31:12,892 [INFO] Train step 50/274 - Loss: 0.418730 (MSE 0.031291, CE 1.077648, Corr 0.269190, Cons 0.023133, CSim 0.046904) | LR: 1.584961e-05
2025-09-21 05:31:16,717 [INFO] Train step 100/274 - Loss: 0.419305 (MSE 0.031341, CE 1.078926, Corr 0.270225, Cons 0.023404, CSim 0.047150) | LR: 9.667564e-06
2025-09-21 05:31:20,414 [INFO] Train step 150/274 - Loss: 0.421151 (MSE 0.031702, CE 1.082731, Corr 0.273893, Cons 0.023690, CSim 0.047464) | LR: 4.949614e-06
2025-09-21 05:31:24,047 [INFO] Train step 200/274 - Loss: 0.421953 (MSE 0.031890, CE 1.084247, Corr 0.275982, Cons 0.023654, CSim 0.047511) | LR: 1.772230e-06
2025-09-21 05:31:27,675 [INFO] Train step 250/274 - Loss: 0.420474 (MSE 0.031819, CE 1.080269, Corr 0.275439, Cons 0.023779, CSim 0.047380) | LR: 1.869092e-07
2025-09-21 05:31:29,475 [INFO] Epoch training completed in 20.59s with average loss 0.419947
2025-09-21 05:31:30,629 [INFO] Fold 3 - Epoch 5 - Train Loss: 0.419947 | Val MSE: 0.053409 | Val CE: 1.380143 | Val Pearson: 0.572563
2025-09-21 05:31:30,630 [INFO] Fold 3 - Epoch 5 - New best model with Pearson 0.572563.
2025-09-21 05:31:31,762 [INFO] Fold 3 - Final Val Pearson: 0.572563 | Final Val MSE: 0.053409 | Final Val CE: 1.380143
2025-09-21 05:31:32,439 [INFO] ========== Fold 5/5 ==========
2025-09-21 05:31:32,496 [INFO] Applied swap augmentation. New train size: 52526
2025-09-21 05:31:32,502 [INFO] Train split: 52526 samples; Val split: 6562 samples.
2025-09-21 05:31:32,503 [INFO] Initializing PatentDataset with 52526 samples.
2025-09-21 05:31:32,503 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 05:31:32,504 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8
2025-09-21 05:31:32,654 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=137, total_steps=1370.
2025-09-21 05:31:32,654 [INFO] Fold 4 - Epoch 1/5 started.
2025-09-21 05:31:36,468 [INFO] Train step 50/274 - Loss: 0.914979 (MSE 0.090559, CE 2.226781, Corr 0.962723, Cons 0.153143, CSim 0.114362) | LR: 7.299270e-05
2025-09-21 05:31:40,098 [INFO] Train step 100/274 - Loss: 0.842768 (MSE 0.082048, CE 2.047089, Corr 0.920604, Cons 0.137319, CSim 0.095308) | LR: 1.459854e-04
2025-09-21 05:31:43,722 [INFO] Train step 150/274 - Loss: 0.803142 (MSE 0.078066, CE 1.948661, Corr 0.892074, Cons 0.126111, CSim 0.085738) | LR: 1.999451e-04
2025-09-21 05:31:47,346 [INFO] Train step 200/274 - Loss: 0.775293 (MSE 0.075379, CE 1.880660, Corr 0.865554, Cons 0.115676, CSim 0.080905) | LR: 1.987144e-04
2025-09-21 05:31:50,974 [INFO] Train step 250/274 - Loss: 0.754970 (MSE 0.073086, CE 1.833277, Corr 0.838819, Cons 0.106642, CSim 0.077760) | LR: 1.958838e-04
2025-09-21 05:31:52,770 [INFO] Epoch training completed in 20.12s with average loss 0.745660
2025-09-21 05:31:53,888 [INFO] Fold 4 - Epoch 1 - Train Loss: 0.745660 | Val MSE: 0.056887 | Val CE: 1.517685 | Val Pearson: 0.397833
2025-09-21 05:31:53,889 [INFO] Fold 4 - Epoch 1 - New best model with Pearson 0.397833.
2025-09-21 05:31:53,889 [INFO] Fold 4 - Epoch 2/5 started.
2025-09-21 05:31:57,919 [INFO] Train step 50/274 - Loss: 0.621720 (MSE 0.058394, CE 1.530669, Corr 0.621235, Cons 0.051933, CSim 0.062626) | LR: 1.888623e-04
2025-09-21 05:32:01,779 [INFO] Train step 100/274 - Loss: 0.609859 (MSE 0.057743, CE 1.503157, Corr 0.597494, Cons 0.052467, CSim 0.062928) | LR: 1.823151e-04
2025-09-21 05:32:05,630 [INFO] Train step 150/274 - Loss: 0.602173 (MSE 0.056768, CE 1.487391, Corr 0.577949, Cons 0.050233, CSim 0.062470) | LR: 1.744337e-04
2025-09-21 05:32:09,397 [INFO] Train step 200/274 - Loss: 0.595834 (MSE 0.056060, CE 1.473749, Corr 0.563999, Cons 0.048201, CSim 0.062098) | LR: 1.653459e-04
2025-09-21 05:32:13,189 [INFO] Train step 250/274 - Loss: 0.589376 (MSE 0.055116, CE 1.460034, Corr 0.550532, Cons 0.046351, CSim 0.061630) | LR: 1.551990e-04
2025-09-21 05:32:15,058 [INFO] Epoch training completed in 21.17s with average loss 0.586906
2025-09-21 05:32:16,211 [INFO] Fold 4 - Epoch 2 - Train Loss: 0.586906 | Val MSE: 0.056172 | Val CE: 1.463785 | Val Pearson: 0.512584
2025-09-21 05:32:16,213 [INFO] Fold 4 - Epoch 2 - New best model with Pearson 0.512584.
2025-09-21 05:32:16,213 [INFO] Fold 4 - Epoch 3/5 started.
2025-09-21 05:32:20,194 [INFO] Train step 50/274 - Loss: 0.526402 (MSE 0.047849, CE 1.317857, Corr 0.440560, Cons 0.035014, CSim 0.057800) | LR: 1.385918e-04
2025-09-21 05:32:23,973 [INFO] Train step 100/274 - Loss: 0.520420 (MSE 0.045882, CE 1.308023, Corr 0.423742, Cons 0.033347, CSim 0.056197) | LR: 1.265581e-04
2025-09-21 05:32:27,736 [INFO] Train step 150/274 - Loss: 0.513041 (MSE 0.044728, CE 1.291411, Corr 0.412751, Cons 0.032266, CSim 0.055619) | LR: 1.140939e-04
2025-09-21 05:32:31,509 [INFO] Train step 200/274 - Loss: 0.507598 (MSE 0.043981, CE 1.279163, Corr 0.403840, Cons 0.031714, CSim 0.055190) | LR: 1.014013e-04
2025-09-21 05:32:35,263 [INFO] Train step 250/274 - Loss: 0.503652 (MSE 0.043408, CE 1.270496, Corr 0.396603, Cons 0.031548, CSim 0.054909) | LR: 8.868601e-05
2025-09-21 05:32:37,150 [INFO] Epoch training completed in 20.94s with average loss 0.502416
2025-09-21 05:32:38,274 [INFO] Fold 4 - Epoch 3 - Train Loss: 0.502416 | Val MSE: 0.051496 | Val CE: 1.392651 | Val Pearson: 0.554548
2025-09-21 05:32:38,276 [INFO] Fold 4 - Epoch 3 - New best model with Pearson 0.554548.
2025-09-21 05:32:38,276 [INFO] Fold 4 - Epoch 4/5 started.
2025-09-21 05:32:42,351 [INFO] Train step 50/274 - Loss: 0.447228 (MSE 0.036745, CE 1.139523, Corr 0.312873, Cons 0.026573, CSim 0.052473) | LR: 7.026373e-05
2025-09-21 05:32:46,210 [INFO] Train step 100/274 - Loss: 0.448929 (MSE 0.036327, CE 1.144726, Corr 0.315341, Cons 0.026061, CSim 0.050925) | LR: 5.837424e-05
2025-09-21 05:32:50,063 [INFO] Train step 150/274 - Loss: 0.450425 (MSE 0.036303, CE 1.148422, Corr 0.318098, Cons 0.026130, CSim 0.050811) | LR: 4.715941e-05
2025-09-21 05:32:53,923 [INFO] Train step 200/274 - Loss: 0.449487 (MSE 0.036119, CE 1.146448, Corr 0.316528, Cons 0.025330, CSim 0.050715) | LR: 3.680102e-05
2025-09-21 05:32:57,781 [INFO] Train step 250/274 - Loss: 0.449166 (MSE 0.035963, CE 1.145957, Corr 0.315760, Cons 0.025244, CSim 0.050705) | LR: 2.746695e-05
2025-09-21 05:32:59,701 [INFO] Epoch training completed in 21.42s with average loss 0.448292
2025-09-21 05:33:00,863 [INFO] Fold 4 - Epoch 4 - Train Loss: 0.448292 | Val MSE: 0.052167 | Val CE: 1.376066 | Val Pearson: 0.571193
2025-09-21 05:33:00,865 [INFO] Fold 4 - Epoch 4 - New best model with Pearson 0.571193.
2025-09-21 05:33:00,865 [INFO] Fold 4 - Epoch 5/5 started.
2025-09-21 05:33:04,930 [INFO] Train step 50/274 - Loss: 0.426657 (MSE 0.032781, CE 1.093679, Corr 0.286799, Cons 0.023980, CSim 0.048328) | LR: 1.584961e-05
2025-09-21 05:33:08,772 [INFO] Train step 100/274 - Loss: 0.423658 (MSE 0.032742, CE 1.085861, Corr 0.283511, Cons 0.023813, CSim 0.048834) | LR: 9.667564e-06
2025-09-21 05:33:12,618 [INFO] Train step 150/274 - Loss: 0.425094 (MSE 0.032824, CE 1.089361, Corr 0.285667, Cons 0.023439, CSim 0.048861) | LR: 4.949614e-06
2025-09-21 05:33:16,459 [INFO] Train step 200/274 - Loss: 0.424850 (MSE 0.032969, CE 1.088393, Corr 0.285812, Cons 0.023416, CSim 0.048930) | LR: 1.772230e-06
2025-09-21 05:33:20,300 [INFO] Train step 250/274 - Loss: 0.424748 (MSE 0.032798, CE 1.088803, Corr 0.284017, Cons 0.023315, CSim 0.048807) | LR: 1.869092e-07
2025-09-21 05:33:22,205 [INFO] Epoch training completed in 21.34s with average loss 0.424813
2025-09-21 05:33:23,351 [INFO] Fold 4 - Epoch 5 - Train Loss: 0.424813 | Val MSE: 0.054416 | Val CE: 1.398603 | Val Pearson: 0.572078
2025-09-21 05:33:23,353 [INFO] Fold 4 - Epoch 5 - New best model with Pearson 0.572078.
2025-09-21 05:33:24,541 [INFO] Fold 4 - Final Val Pearson: 0.572078 | Final Val MSE: 0.054416 | Final Val CE: 1.398603
2025-09-21 05:33:25,268 [INFO] OOF Pearson correlation across all folds (pre-rule): 0.571989
2025-09-21 05:33:25,268 [INFO] OOF MSE across all folds (pre-rule): 0.053677
2025-09-21 05:33:25,268 [INFO] Ensembling test predictions from all folds by averaging.
2025-09-21 05:33:25,269 [INFO] Applying rule-based post-processing for anchor == target -> score = 1.0
2025-09-21 05:33:25,270 [INFO] Found 24 exact anchor==target pairs in test set.
2025-09-21 05:33:25,271 [INFO] Found 255 exact anchor==target pairs in train (OOF) set; applying same rule to OOF predictions.
2025-09-21 05:33:25,272 [INFO] OOF Pearson after rule: 0.573646
2025-09-21 05:33:25,272 [INFO] OOF MSE after rule: 0.053649
2025-09-21 05:33:25,272 [INFO] Logging final validation results (OOF, post-rule):
2025-09-21 05:33:25,272 [INFO] Final Validation Pearson: 0.573646 | Final Validation MSE: 0.053649
2025-09-21 05:33:25,275 [INFO] Writing submission to task/us-patent-phrase-to-phrase-matching/outputs/1/submission_5.csv
2025-09-21 05:33:25,282 [INFO] Submission file created successfully.
2025-09-21 05:33:25,282 [INFO] All done (v5).
