2025-09-21 08:14:11,647 [INFO] Initialized logging and created output directories for v10 pipeline.
2025-09-21 08:14:11,647 [INFO] Setting all random seeds to 42.
2025-09-21 08:14:11,647 [INFO] Reading train data from task/us-patent-phrase-to-phrase-matching/train.csv.
2025-09-21 08:14:11,686 [INFO] Reading test data from task/us-patent-phrase-to-phrase-matching/test.csv.
2025-09-21 08:14:11,692 [INFO] Train shape: (32825, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 08:14:11,692 [INFO] Test shape:  (3648, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 08:14:11,699 [INFO] Train missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 08:14:11,699 [INFO] Test missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 08:14:11,699 [INFO] Building vocabulary from segmented corpus.
2025-09-21 08:14:12,063 [INFO] Built vocabulary of size 9042.
2025-09-21 08:14:12,063 [INFO] Tokenizing and numericalizing dataframe with 32825 rows (with token types).
2025-09-21 08:14:12,183 [INFO] Processed 5000 rows.
2025-09-21 08:14:12,302 [INFO] Processed 10000 rows.
2025-09-21 08:14:12,421 [INFO] Processed 15000 rows.
2025-09-21 08:14:12,540 [INFO] Processed 20000 rows.
2025-09-21 08:14:12,659 [INFO] Processed 25000 rows.
2025-09-21 08:14:12,778 [INFO] Processed 30000 rows.
2025-09-21 08:14:12,845 [INFO] Tokenizing and numericalizing dataframe with 3648 rows (with token types).
2025-09-21 08:14:12,932 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 3648 rows (anchor<->target).
2025-09-21 08:14:13,018 [INFO] Prepared swapped TTA arrays for test data.
2025-09-21 08:14:13,018 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 32825 rows (anchor<->target).
2025-09-21 08:14:13,137 [INFO] Processed 5000 swapped rows.
2025-09-21 08:14:13,255 [INFO] Processed 10000 swapped rows.
2025-09-21 08:14:13,374 [INFO] Processed 15000 swapped rows.
2025-09-21 08:14:13,492 [INFO] Processed 20000 swapped rows.
2025-09-21 08:14:13,611 [INFO] Processed 25000 swapped rows.
2025-09-21 08:14:13,729 [INFO] Processed 30000 swapped rows.
2025-09-21 08:14:13,796 [INFO] Prepared swapped augmentation arrays for training.
2025-09-21 08:14:13,797 [INFO] Computing handcrafted similarity features.
2025-09-21 08:14:14,345 [INFO] Computed handcrafted features for 5000 rows.
2025-09-21 08:14:14,879 [INFO] Computed handcrafted features for 10000 rows.
2025-09-21 08:14:15,420 [INFO] Computed handcrafted features for 15000 rows.
2025-09-21 08:14:15,960 [INFO] Computed handcrafted features for 20000 rows.
2025-09-21 08:14:16,498 [INFO] Computed handcrafted features for 25000 rows.
2025-09-21 08:14:17,033 [INFO] Computed handcrafted features for 30000 rows.
2025-09-21 08:14:17,342 [INFO] Finished computing handcrafted features.
2025-09-21 08:14:17,344 [INFO] Computing handcrafted similarity features.
2025-09-21 08:14:17,735 [INFO] Finished computing handcrafted features.
2025-09-21 08:14:17,735 [INFO] Handcrafted feature dimension: 14
2025-09-21 08:14:17,736 [INFO] Labels statistics: min=0.0, max=1.0, mean=0.3619, std=0.2588
2025-09-21 08:14:17,741 [INFO] Found 106 unique context codes across train+test.
2025-09-21 08:14:18,013 [INFO] Class counts: [6774.0, 10306.0, 11068.0, 3634.0, 1043.0] | CE weights: [0.15397107601165771, 0.10120318084955215, 0.0942356288433075, 0.2870115637779236, 1.0]
2025-09-21 08:14:18,015 [INFO] Creating stratified folds on 5-class bins.
2025-09-21 08:14:18,031 [INFO] Fold 0: 6567 samples.
2025-09-21 08:14:18,031 [INFO] Fold 1: 6566 samples.
2025-09-21 08:14:18,031 [INFO] Fold 2: 6566 samples.
2025-09-21 08:14:18,031 [INFO] Fold 3: 6564 samples.
2025-09-21 08:14:18,031 [INFO] Fold 4: 6562 samples.
2025-09-21 08:14:18,032 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 08:14:18,033 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 08:14:18,033 [INFO] ========== Fold 1/5 ==========
2025-09-21 08:14:21,345 [INFO] Applied swap augmentation. New train size: 52516
2025-09-21 08:14:21,358 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 08:14:21,358 [INFO] Train split: 52516 samples; Val split: 6567 samples.
2025-09-21 08:14:21,358 [INFO] Initializing PatentDataset with 52516 samples.
2025-09-21 08:14:21,358 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 08:14:21,358 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 08:14:21,358 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=6, max_len=64, ctxs=106, feat_dim=14, msd=5
2025-09-21 08:14:22,484 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 08:14:22,484 [INFO] Initializing EMA state from current model weights.
2025-09-21 08:14:22,707 [INFO] Fold 0 - Epoch 1/6 started.
2025-09-21 08:14:42,650 [INFO] Train step 50/274 - Loss: 0.374272 (wMSE 0.147172, SoftCE 0.501922, Corr 0.954869, Cons 0.224053, CSim 0.156811, Rank 0.695861, RDropCLS 0.898708, RDropREG 0.076240, GateReg 0.101932) | LR: 6.097561e-05
2025-09-21 08:15:01,385 [INFO] Train step 100/274 - Loss: 0.332287 (wMSE 0.129758, SoftCE 0.426985, Corr 0.915620, Cons 0.204667, CSim 0.118225, Rank 0.690225, RDropCLS 0.701008, RDropREG 0.055455, GateReg 0.096737) | LR: 1.219512e-04
2025-09-21 08:15:20,187 [INFO] Train step 150/274 - Loss: 0.308850 (wMSE 0.120400, SoftCE 0.391120, Corr 0.878321, Cons 0.183274, CSim 0.101608, Rank 0.686162, RDropCLS 0.552708, RDropREG 0.046161, GateReg 0.091741) | LR: 1.829268e-04
2025-09-21 08:15:39,214 [INFO] Train step 200/274 - Loss: 0.293245 (wMSE 0.113774, SoftCE 0.369438, Corr 0.846049, Cons 0.172776, CSim 0.092932, Rank 0.682562, RDropCLS 0.454446, RDropREG 0.038901, GateReg 0.089147) | LR: 1.997082e-04
2025-09-21 08:15:58,287 [INFO] Train step 250/274 - Loss: 0.280808 (wMSE 0.108398, SoftCE 0.354883, Corr 0.808998, Cons 0.161296, CSim 0.087695, Rank 0.678893, RDropCLS 0.383996, RDropREG 0.033819, GateReg 0.086233) | LR: 1.983384e-04
2025-09-21 08:16:07,235 [INFO] Epoch training completed in 104.53s with average loss 0.275313
2025-09-21 08:16:07,236 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:16:08,799 [INFO] Fold 0 - Epoch 1 - Train Loss: 0.275313 | EMA Val wMSE: 0.091989 | EMA Val SoftCE: 0.326915 | EMA Val Pearson: 0.186582
2025-09-21 08:16:08,806 [INFO] Fold 0 - Epoch 1 - New best EMA model with Pearson 0.186582.
2025-09-21 08:16:08,806 [INFO] Fold 0 - Epoch 2/6 started.
2025-09-21 08:16:27,821 [INFO] Train step 50/274 - Loss: 0.209362 (wMSE 0.074801, SoftCE 0.285248, Corr 0.535179, Cons 0.110496, CSim 0.065355, Rank 0.650507, RDropCLS 0.066782, RDropREG 0.007510, GateReg 0.083910) | LR: 1.942877e-04
2025-09-21 08:16:46,482 [INFO] Train step 100/274 - Loss: 0.203600 (wMSE 0.070456, SoftCE 0.281526, Corr 0.513240, Cons 0.104409, CSim 0.063342, Rank 0.645749, RDropCLS 0.063742, RDropREG 0.007199, GateReg 0.088540) | LR: 1.902280e-04
2025-09-21 08:17:05,187 [INFO] Train step 150/274 - Loss: 0.200053 (wMSE 0.068267, SoftCE 0.278148, Corr 0.500164, Cons 0.101569, CSim 0.062694, Rank 0.643029, RDropCLS 0.060343, RDropREG 0.007176, GateReg 0.090522) | LR: 1.851529e-04
2025-09-21 08:17:23,882 [INFO] Train step 200/274 - Loss: 0.197162 (wMSE 0.066429, SoftCE 0.275875, Corr 0.488481, Cons 0.097566, CSim 0.061811, Rank 0.641020, RDropCLS 0.058088, RDropREG 0.006860, GateReg 0.092425) | LR: 1.791195e-04
2025-09-21 08:17:42,555 [INFO] Train step 250/274 - Loss: 0.194724 (wMSE 0.064961, SoftCE 0.274347, Corr 0.476820, Cons 0.094554, CSim 0.061336, Rank 0.638553, RDropCLS 0.056156, RDropREG 0.006679, GateReg 0.094081) | LR: 1.721956e-04
2025-09-21 08:17:51,556 [INFO] Epoch training completed in 102.75s with average loss 0.193599
2025-09-21 08:17:51,558 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:17:53,150 [INFO] Fold 0 - Epoch 2 - Train Loss: 0.193599 | EMA Val wMSE: 0.074049 | EMA Val SoftCE: 0.283258 | EMA Val Pearson: 0.444409
2025-09-21 08:17:53,156 [INFO] Fold 0 - Epoch 2 - New best EMA model with Pearson 0.444409.
2025-09-21 08:17:53,156 [INFO] Fold 0 - Epoch 3/6 started.
2025-09-21 08:18:12,203 [INFO] Train step 50/274 - Loss: 0.171496 (wMSE 0.051878, SoftCE 0.257721, Corr 0.363168, Cons 0.070873, CSim 0.056011, Rank 0.614412, RDropCLS 0.051788, RDropREG 0.006812, GateReg 0.110240) | LR: 1.604825e-04
2025-09-21 08:18:30,882 [INFO] Train step 100/274 - Loss: 0.171320 (wMSE 0.051828, SoftCE 0.257790, Corr 0.362440, Cons 0.068400, CSim 0.054739, Rank 0.614781, RDropCLS 0.051041, RDropREG 0.006412, GateReg 0.110654) | LR: 1.517058e-04
2025-09-21 08:18:49,531 [INFO] Train step 150/274 - Loss: 0.170147 (wMSE 0.051028, SoftCE 0.257576, Corr 0.355549, Cons 0.068152, CSim 0.054606, Rank 0.613012, RDropCLS 0.049997, RDropREG 0.006345, GateReg 0.111493) | LR: 1.423473e-04
2025-09-21 08:19:08,231 [INFO] Train step 200/274 - Loss: 0.169340 (wMSE 0.050506, SoftCE 0.257027, Corr 0.352420, Cons 0.066011, CSim 0.054139, Rank 0.612224, RDropCLS 0.049750, RDropREG 0.006389, GateReg 0.112610) | LR: 1.325122e-04
2025-09-21 08:19:26,909 [INFO] Train step 250/274 - Loss: 0.168591 (wMSE 0.050126, SoftCE 0.256864, Corr 0.347777, Cons 0.065100, CSim 0.053881, Rank 0.610442, RDropCLS 0.049692, RDropREG 0.006442, GateReg 0.112807) | LR: 1.223112e-04
2025-09-21 08:19:35,919 [INFO] Epoch training completed in 102.76s with average loss 0.167948
2025-09-21 08:19:35,921 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:19:37,487 [INFO] Fold 0 - Epoch 3 - Train Loss: 0.167948 | EMA Val wMSE: 0.063139 | EMA Val SoftCE: 0.271223 | EMA Val Pearson: 0.552627
2025-09-21 08:19:37,493 [INFO] Fold 0 - Epoch 3 - New best EMA model with Pearson 0.552627.
2025-09-21 08:19:37,493 [INFO] Fold 0 - Epoch 4/6 started.
2025-09-21 08:19:56,590 [INFO] Train step 50/274 - Loss: 0.152429 (wMSE 0.039738, SoftCE 0.246904, Corr 0.271231, Cons 0.052696, CSim 0.048129, Rank 0.593791, RDropCLS 0.049795, RDropREG 0.006918, GateReg 0.117802) | LR: 1.067874e-04
2025-09-21 08:20:15,327 [INFO] Train step 100/274 - Loss: 0.154122 (wMSE 0.041065, SoftCE 0.248203, Corr 0.277373, Cons 0.053191, CSim 0.048500, Rank 0.594854, RDropCLS 0.049774, RDropREG 0.006843, GateReg 0.117891) | LR: 9.618007e-05
2025-09-21 08:20:34,029 [INFO] Train step 150/274 - Loss: 0.153584 (wMSE 0.040777, SoftCE 0.247780, Corr 0.275427, Cons 0.051806, CSim 0.048432, Rank 0.593365, RDropCLS 0.050261, RDropREG 0.006970, GateReg 0.117624) | LR: 8.561573e-05
2025-09-21 08:20:52,983 [INFO] Train step 200/274 - Loss: 0.153192 (wMSE 0.040658, SoftCE 0.246961, Corr 0.275178, Cons 0.050701, CSim 0.048207, Rank 0.592482, RDropCLS 0.050855, RDropREG 0.006951, GateReg 0.118446) | LR: 7.521326e-05
2025-09-21 08:21:11,756 [INFO] Train step 250/274 - Loss: 0.152707 (wMSE 0.040450, SoftCE 0.246133, Corr 0.274072, Cons 0.050165, CSim 0.048017, Rank 0.592333, RDropCLS 0.050499, RDropREG 0.006876, GateReg 0.118819) | LR: 6.508975e-05
2025-09-21 08:21:20,725 [INFO] Epoch training completed in 103.23s with average loss 0.152501
2025-09-21 08:21:20,727 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:21:22,315 [INFO] Fold 0 - Epoch 4 - Train Loss: 0.152501 | EMA Val wMSE: 0.058049 | EMA Val SoftCE: 0.261970 | EMA Val Pearson: 0.592864
2025-09-21 08:21:22,321 [INFO] Fold 0 - Epoch 4 - New best EMA model with Pearson 0.592864.
2025-09-21 08:21:22,322 [INFO] Fold 0 - Epoch 5/6 started.
2025-09-21 08:21:41,336 [INFO] Train step 50/274 - Loss: 0.147461 (wMSE 0.037182, SoftCE 0.243598, Corr 0.249013, Cons 0.043559, CSim 0.045390, Rank 0.583353, RDropCLS 0.051711, RDropREG 0.007024, GateReg 0.119428) | LR: 5.086032e-05
2025-09-21 08:22:00,038 [INFO] Train step 100/274 - Loss: 0.145653 (wMSE 0.036265, SoftCE 0.240761, Corr 0.243541, Cons 0.042143, CSim 0.045228, Rank 0.583979, RDropCLS 0.051300, RDropREG 0.006909, GateReg 0.119587) | LR: 4.191050e-05
2025-09-21 08:22:18,676 [INFO] Train step 150/274 - Loss: 0.145068 (wMSE 0.036011, SoftCE 0.239791, Corr 0.242006, Cons 0.041718, CSim 0.044942, Rank 0.583730, RDropCLS 0.051360, RDropREG 0.006830, GateReg 0.120027) | LR: 3.361443e-05
2025-09-21 08:22:37,361 [INFO] Train step 200/274 - Loss: 0.145014 (wMSE 0.035954, SoftCE 0.240044, Corr 0.240779, Cons 0.041537, CSim 0.044947, Rank 0.583447, RDropCLS 0.052094, RDropREG 0.006941, GateReg 0.120309) | LR: 2.606547e-05
2025-09-21 08:22:56,047 [INFO] Train step 250/274 - Loss: 0.144470 (wMSE 0.035608, SoftCE 0.239424, Corr 0.239454, Cons 0.040947, CSim 0.044535, Rank 0.582765, RDropCLS 0.052315, RDropREG 0.007049, GateReg 0.120271) | LR: 1.934857e-05
2025-09-21 08:23:05,069 [INFO] Epoch training completed in 102.75s with average loss 0.144597
2025-09-21 08:23:05,071 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:23:06,659 [INFO] Fold 0 - Epoch 5 - Train Loss: 0.144597 | EMA Val wMSE: 0.058013 | EMA Val SoftCE: 0.257734 | EMA Val Pearson: 0.606061
2025-09-21 08:23:06,666 [INFO] Fold 0 - Epoch 5 - New best EMA model with Pearson 0.606061.
2025-09-21 08:23:06,666 [INFO] Fold 0 - Epoch 6/6 started.
2025-09-21 08:23:25,700 [INFO] Train step 50/274 - Loss: 0.140987 (wMSE 0.033323, SoftCE 0.236840, Corr 0.224780, Cons 0.040305, CSim 0.043015, Rank 0.579234, RDropCLS 0.051802, RDropREG 0.007022, GateReg 0.120013) | LR: 1.109289e-05
2025-09-21 08:23:44,393 [INFO] Train step 100/274 - Loss: 0.140831 (wMSE 0.033054, SoftCE 0.237545, Corr 0.222418, Cons 0.040355, CSim 0.043584, Rank 0.578213, RDropCLS 0.051485, RDropREG 0.007028, GateReg 0.119540) | LR: 6.743759e-06
2025-09-21 08:24:03,103 [INFO] Train step 150/274 - Loss: 0.140718 (wMSE 0.033101, SoftCE 0.237241, Corr 0.221621, Cons 0.040361, CSim 0.043601, Rank 0.578337, RDropCLS 0.051838, RDropREG 0.006999, GateReg 0.120003) | LR: 3.444140e-06
2025-09-21 08:24:21,827 [INFO] Train step 200/274 - Loss: 0.140778 (wMSE 0.033140, SoftCE 0.237048, Corr 0.222846, Cons 0.040112, CSim 0.043184, Rank 0.578583, RDropCLS 0.052406, RDropREG 0.007021, GateReg 0.119892) | LR: 1.231166e-06
2025-09-21 08:24:40,528 [INFO] Train step 250/274 - Loss: 0.141000 (wMSE 0.033388, SoftCE 0.236955, Corr 0.223955, Cons 0.039954, CSim 0.043465, Rank 0.578745, RDropCLS 0.052652, RDropREG 0.007045, GateReg 0.120048) | LR: 1.297403e-07
2025-09-21 08:24:49,521 [INFO] Epoch training completed in 102.85s with average loss 0.140923
2025-09-21 08:24:49,522 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:24:51,117 [INFO] Fold 0 - Epoch 6 - Train Loss: 0.140923 | EMA Val wMSE: 0.059699 | EMA Val SoftCE: 0.256837 | EMA Val Pearson: 0.609817
2025-09-21 08:24:51,125 [INFO] Fold 0 - Epoch 6 - New best EMA model with Pearson 0.609817.
2025-09-21 08:24:51,276 [INFO] Generating validation predictions with MC-Dropout TTA and Swap TTA
2025-09-21 08:24:52,275 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 08:24:57,817 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 08:25:02,333 [INFO] Fold 0 - Final Val Pearson (with MC-Dropout TTA + swap): 0.619041 | Final Val MSE: 0.042961
2025-09-21 08:25:02,334 [INFO] Generating test predictions with MC-Dropout TTA and Swap TTA
2025-09-21 08:25:02,974 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 08:25:07,955 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 08:25:12,391 [INFO] ========== Fold 2/5 ==========
2025-09-21 08:25:14,054 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 08:25:14,061 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 08:25:14,061 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 08:25:14,061 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 08:25:14,061 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 08:25:14,061 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 08:25:14,063 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=6, max_len=64, ctxs=106, feat_dim=14, msd=5
2025-09-21 08:25:14,293 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 08:25:14,293 [INFO] Initializing EMA state from current model weights.
2025-09-21 08:25:14,504 [INFO] Fold 1 - Epoch 1/6 started.
2025-09-21 08:25:33,609 [INFO] Train step 50/274 - Loss: 0.357397 (wMSE 0.152640, SoftCE 0.440648, Corr 0.946759, Cons 0.270357, CSim 0.132129, Rank 0.692545, RDropCLS 0.897514, RDropREG 0.037274, GateReg 0.109236) | LR: 6.097561e-05
2025-09-21 08:25:52,316 [INFO] Train step 100/274 - Loss: 0.320421 (wMSE 0.130117, SoftCE 0.392988, Corr 0.901503, Cons 0.227226, CSim 0.106274, Rank 0.687628, RDropCLS 0.685792, RDropREG 0.033684, GateReg 0.109600) | LR: 1.219512e-04
2025-09-21 08:26:11,001 [INFO] Train step 150/274 - Loss: 0.301860 (wMSE 0.121362, SoftCE 0.367787, Corr 0.877962, Cons 0.197103, CSim 0.094178, Rank 0.685225, RDropCLS 0.539434, RDropREG 0.029847, GateReg 0.107598) | LR: 1.829268e-04
2025-09-21 08:26:29,762 [INFO] Train step 200/274 - Loss: 0.288285 (wMSE 0.114340, SoftCE 0.352652, Corr 0.849380, Cons 0.172671, CSim 0.087213, Rank 0.682492, RDropCLS 0.444569, RDropREG 0.026922, GateReg 0.110496) | LR: 1.997082e-04
2025-09-21 08:26:48,509 [INFO] Train step 250/274 - Loss: 0.276323 (wMSE 0.108260, SoftCE 0.340587, Corr 0.813456, Cons 0.159559, CSim 0.082951, Rank 0.678513, RDropCLS 0.374572, RDropREG 0.024215, GateReg 0.105903) | LR: 1.983384e-04
2025-09-21 08:26:57,481 [INFO] Epoch training completed in 102.98s with average loss 0.271153
2025-09-21 08:26:57,482 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:26:59,030 [INFO] Fold 1 - Epoch 1 - Train Loss: 0.271153 | EMA Val wMSE: 0.088931 | EMA Val SoftCE: 0.311080 | EMA Val Pearson: 0.212767
2025-09-21 08:26:59,036 [INFO] Fold 1 - Epoch 1 - New best EMA model with Pearson 0.212767.
2025-09-21 08:26:59,036 [INFO] Fold 1 - Epoch 2/6 started.
2025-09-21 08:27:18,129 [INFO] Train step 50/274 - Loss: 0.207663 (wMSE 0.073563, SoftCE 0.281851, Corr 0.541752, Cons 0.089080, CSim 0.063719, Rank 0.650995, RDropCLS 0.060914, RDropREG 0.008017, GateReg 0.098350) | LR: 1.942877e-04
2025-09-21 08:27:36,829 [INFO] Train step 100/274 - Loss: 0.201476 (wMSE 0.069180, SoftCE 0.277905, Corr 0.514688, Cons 0.085261, CSim 0.062402, Rank 0.646915, RDropCLS 0.056284, RDropREG 0.007175, GateReg 0.102471) | LR: 1.902280e-04
2025-09-21 08:27:55,577 [INFO] Train step 150/274 - Loss: 0.198544 (wMSE 0.067650, SoftCE 0.276022, Corr 0.498309, Cons 0.082594, CSim 0.062274, Rank 0.644699, RDropCLS 0.053564, RDropREG 0.006705, GateReg 0.105337) | LR: 1.851529e-04
2025-09-21 08:28:14,333 [INFO] Train step 200/274 - Loss: 0.196083 (wMSE 0.066154, SoftCE 0.274204, Corr 0.487652, Cons 0.079455, CSim 0.061613, Rank 0.642242, RDropCLS 0.051905, RDropREG 0.006510, GateReg 0.107862) | LR: 1.791195e-04
2025-09-21 08:28:33,030 [INFO] Train step 250/274 - Loss: 0.194108 (wMSE 0.064874, SoftCE 0.273092, Corr 0.478878, Cons 0.076371, CSim 0.060883, Rank 0.639978, RDropCLS 0.050375, RDropREG 0.006467, GateReg 0.109324) | LR: 1.721956e-04
2025-09-21 08:28:42,065 [INFO] Epoch training completed in 103.03s with average loss 0.193069
2025-09-21 08:28:42,066 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:28:43,650 [INFO] Fold 1 - Epoch 2 - Train Loss: 0.193069 | EMA Val wMSE: 0.083884 | EMA Val SoftCE: 0.284295 | EMA Val Pearson: 0.424285
2025-09-21 08:28:43,656 [INFO] Fold 1 - Epoch 2 - New best EMA model with Pearson 0.424285.
2025-09-21 08:28:43,656 [INFO] Fold 1 - Epoch 3/6 started.
2025-09-21 08:29:02,774 [INFO] Train step 50/274 - Loss: 0.170694 (wMSE 0.050953, SoftCE 0.258909, Corr 0.360080, Cons 0.058690, CSim 0.054621, Rank 0.613071, RDropCLS 0.048675, RDropREG 0.006943, GateReg 0.118004) | LR: 1.604825e-04
2025-09-21 08:29:21,500 [INFO] Train step 100/274 - Loss: 0.171138 (wMSE 0.051430, SoftCE 0.258970, Corr 0.362227, Cons 0.056338, CSim 0.054738, Rank 0.613647, RDropCLS 0.048179, RDropREG 0.006917, GateReg 0.120794) | LR: 1.517058e-04
2025-09-21 08:29:40,227 [INFO] Train step 150/274 - Loss: 0.170098 (wMSE 0.051070, SoftCE 0.257853, Corr 0.357406, Cons 0.054994, CSim 0.054446, Rank 0.612201, RDropCLS 0.047750, RDropREG 0.006674, GateReg 0.121427) | LR: 1.423473e-04
2025-09-21 08:29:58,949 [INFO] Train step 200/274 - Loss: 0.169698 (wMSE 0.051023, SoftCE 0.257629, Corr 0.353983, Cons 0.053897, CSim 0.054058, Rank 0.612724, RDropCLS 0.046585, RDropREG 0.006486, GateReg 0.122897) | LR: 1.325122e-04
2025-09-21 08:30:17,683 [INFO] Train step 250/274 - Loss: 0.168260 (wMSE 0.050296, SoftCE 0.256412, Corr 0.347273, Cons 0.052193, CSim 0.053725, Rank 0.611210, RDropCLS 0.045881, RDropREG 0.006426, GateReg 0.123822) | LR: 1.223112e-04
2025-09-21 08:30:26,715 [INFO] Epoch training completed in 103.06s with average loss 0.167628
2025-09-21 08:30:26,717 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:30:28,294 [INFO] Fold 1 - Epoch 3 - Train Loss: 0.167628 | EMA Val wMSE: 0.069994 | EMA Val SoftCE: 0.272618 | EMA Val Pearson: 0.545562
2025-09-21 08:30:28,300 [INFO] Fold 1 - Epoch 3 - New best EMA model with Pearson 0.545562.
2025-09-21 08:30:28,301 [INFO] Fold 1 - Epoch 4/6 started.
2025-09-21 08:30:47,429 [INFO] Train step 50/274 - Loss: 0.154797 (wMSE 0.042043, SoftCE 0.249288, Corr 0.277662, Cons 0.044177, CSim 0.049064, Rank 0.594815, RDropCLS 0.044837, RDropREG 0.006257, GateReg 0.127420) | LR: 1.067874e-04
2025-09-21 08:31:06,128 [INFO] Train step 100/274 - Loss: 0.154064 (wMSE 0.041704, SoftCE 0.247222, Corr 0.279239, Cons 0.043975, CSim 0.048380, Rank 0.593658, RDropCLS 0.047266, RDropREG 0.006461, GateReg 0.126004) | LR: 9.618007e-05
2025-09-21 08:31:24,830 [INFO] Train step 150/274 - Loss: 0.154254 (wMSE 0.041952, SoftCE 0.247054, Corr 0.281117, Cons 0.042036, CSim 0.048267, Rank 0.593203, RDropCLS 0.048150, RDropREG 0.006494, GateReg 0.126496) | LR: 8.561573e-05
2025-09-21 08:31:43,547 [INFO] Train step 200/274 - Loss: 0.153662 (wMSE 0.041563, SoftCE 0.246603, Corr 0.278739, Cons 0.041920, CSim 0.047963, Rank 0.592537, RDropCLS 0.047900, RDropREG 0.006516, GateReg 0.127133) | LR: 7.521326e-05
2025-09-21 08:32:02,467 [INFO] Train step 250/274 - Loss: 0.152775 (wMSE 0.041010, SoftCE 0.245687, Corr 0.276039, Cons 0.041408, CSim 0.047535, Rank 0.591280, RDropCLS 0.047886, RDropREG 0.006594, GateReg 0.127636) | LR: 6.508975e-05
2025-09-21 08:32:11,567 [INFO] Epoch training completed in 103.27s with average loss 0.152569
2025-09-21 08:32:11,568 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:32:13,136 [INFO] Fold 1 - Epoch 4 - Train Loss: 0.152569 | EMA Val wMSE: 0.062380 | EMA Val SoftCE: 0.262827 | EMA Val Pearson: 0.586833
2025-09-21 08:32:13,143 [INFO] Fold 1 - Epoch 4 - New best EMA model with Pearson 0.586833.
2025-09-21 08:32:13,143 [INFO] Fold 1 - Epoch 5/6 started.
2025-09-21 08:32:32,758 [INFO] Train step 50/274 - Loss: 0.145303 (wMSE 0.036812, SoftCE 0.239756, Corr 0.241484, Cons 0.036327, CSim 0.045463, Rank 0.581285, RDropCLS 0.051462, RDropREG 0.007199, GateReg 0.129278) | LR: 5.086032e-05
2025-09-21 08:32:52,024 [INFO] Train step 100/274 - Loss: 0.144261 (wMSE 0.035859, SoftCE 0.238874, Corr 0.238301, Cons 0.035305, CSim 0.044784, Rank 0.582914, RDropCLS 0.050249, RDropREG 0.007033, GateReg 0.129216) | LR: 4.191050e-05
2025-09-21 08:33:11,205 [INFO] Train step 150/274 - Loss: 0.144291 (wMSE 0.035745, SoftCE 0.238842, Corr 0.239623, Cons 0.035338, CSim 0.044439, Rank 0.583267, RDropCLS 0.050203, RDropREG 0.006894, GateReg 0.128817) | LR: 3.361443e-05
2025-09-21 08:33:30,089 [INFO] Train step 200/274 - Loss: 0.144469 (wMSE 0.035888, SoftCE 0.239181, Corr 0.239735, Cons 0.035554, CSim 0.044612, Rank 0.582637, RDropCLS 0.050600, RDropREG 0.006948, GateReg 0.128584) | LR: 2.606547e-05
2025-09-21 08:33:48,794 [INFO] Train step 250/274 - Loss: 0.144675 (wMSE 0.035954, SoftCE 0.239508, Corr 0.240585, Cons 0.035563, CSim 0.044625, Rank 0.582746, RDropCLS 0.050437, RDropREG 0.006956, GateReg 0.128607) | LR: 1.934857e-05
2025-09-21 08:33:58,212 [INFO] Epoch training completed in 105.07s with average loss 0.144508
2025-09-21 08:33:58,213 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:33:59,739 [INFO] Fold 1 - Epoch 5 - Train Loss: 0.144508 | EMA Val wMSE: 0.060486 | EMA Val SoftCE: 0.257901 | EMA Val Pearson: 0.602742
2025-09-21 08:33:59,745 [INFO] Fold 1 - Epoch 5 - New best EMA model with Pearson 0.602742.
2025-09-21 08:33:59,746 [INFO] Fold 1 - Epoch 6/6 started.
2025-09-21 08:34:19,063 [INFO] Train step 50/274 - Loss: 0.141937 (wMSE 0.033987, SoftCE 0.238126, Corr 0.227783, Cons 0.037057, CSim 0.043193, Rank 0.579039, RDropCLS 0.050901, RDropREG 0.007038, GateReg 0.128512) | LR: 1.109289e-05
2025-09-21 08:34:38,231 [INFO] Train step 100/274 - Loss: 0.140908 (wMSE 0.033578, SoftCE 0.236730, Corr 0.223922, Cons 0.035810, CSim 0.043107, Rank 0.577857, RDropCLS 0.051275, RDropREG 0.007106, GateReg 0.129236) | LR: 6.743759e-06
2025-09-21 08:34:57,448 [INFO] Train step 150/274 - Loss: 0.140780 (wMSE 0.033450, SoftCE 0.236830, Corr 0.223189, Cons 0.035169, CSim 0.043020, Rank 0.577632, RDropCLS 0.051294, RDropREG 0.007109, GateReg 0.129036) | LR: 3.444140e-06
2025-09-21 08:35:16,403 [INFO] Train step 200/274 - Loss: 0.140706 (wMSE 0.033479, SoftCE 0.236527, Corr 0.223358, Cons 0.034783, CSim 0.042906, Rank 0.577792, RDropCLS 0.051002, RDropREG 0.007086, GateReg 0.128974) | LR: 1.231166e-06
2025-09-21 08:35:35,276 [INFO] Train step 250/274 - Loss: 0.140560 (wMSE 0.033332, SoftCE 0.236412, Corr 0.222653, Cons 0.034507, CSim 0.042956, Rank 0.578316, RDropCLS 0.051119, RDropREG 0.007085, GateReg 0.128983) | LR: 1.297403e-07
2025-09-21 08:35:44,283 [INFO] Epoch training completed in 104.54s with average loss 0.140695
2025-09-21 08:35:44,285 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:35:45,820 [INFO] Fold 1 - Epoch 6 - Train Loss: 0.140695 | EMA Val wMSE: 0.061385 | EMA Val SoftCE: 0.256830 | EMA Val Pearson: 0.608354
2025-09-21 08:35:45,827 [INFO] Fold 1 - Epoch 6 - New best EMA model with Pearson 0.608354.
2025-09-21 08:35:45,976 [INFO] Generating validation predictions with MC-Dropout TTA and Swap TTA
2025-09-21 08:35:46,940 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 08:35:52,379 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 08:35:56,856 [INFO] Fold 1 - Final Val Pearson (with MC-Dropout TTA + swap): 0.618748 | Final Val MSE: 0.043032
2025-09-21 08:35:56,857 [INFO] Generating test predictions with MC-Dropout TTA and Swap TTA
2025-09-21 08:35:57,481 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 08:36:02,516 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 08:36:06,980 [INFO] ========== Fold 3/5 ==========
2025-09-21 08:36:07,159 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 08:36:07,170 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 08:36:07,170 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 08:36:07,170 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 08:36:07,170 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 08:36:07,170 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 08:36:07,172 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=6, max_len=64, ctxs=106, feat_dim=14, msd=5
2025-09-21 08:36:07,393 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 08:36:07,393 [INFO] Initializing EMA state from current model weights.
2025-09-21 08:36:07,604 [INFO] Fold 2 - Epoch 1/6 started.
2025-09-21 08:36:27,725 [INFO] Train step 50/274 - Loss: 0.343025 (wMSE 0.127456, SoftCE 0.437074, Corr 0.936306, Cons 0.225172, CSim 0.156694, Rank 0.695086, RDropCLS 0.933791, RDropREG 0.045797, GateReg 0.134168) | LR: 6.097561e-05
2025-09-21 08:36:46,945 [INFO] Train step 100/274 - Loss: 0.315898 (wMSE 0.118021, SoftCE 0.396973, Corr 0.899372, Cons 0.198072, CSim 0.122243, Rank 0.688788, RDropCLS 0.717819, RDropREG 0.037406, GateReg 0.140410) | LR: 1.219512e-04
2025-09-21 08:37:05,820 [INFO] Train step 150/274 - Loss: 0.299023 (wMSE 0.112659, SoftCE 0.373029, Corr 0.872771, Cons 0.180578, CSim 0.105037, Rank 0.685436, RDropCLS 0.559986, RDropREG 0.031203, GateReg 0.136576) | LR: 1.829268e-04
2025-09-21 08:37:24,522 [INFO] Train step 200/274 - Loss: 0.284937 (wMSE 0.107236, SoftCE 0.355044, Corr 0.838907, Cons 0.167229, CSim 0.095629, Rank 0.681982, RDropCLS 0.460397, RDropREG 0.027309, GateReg 0.125027) | LR: 1.997082e-04
2025-09-21 08:37:43,307 [INFO] Train step 250/274 - Loss: 0.273556 (wMSE 0.102628, SoftCE 0.342939, Corr 0.801597, Cons 0.155564, CSim 0.089829, Rank 0.678323, RDropCLS 0.387783, RDropREG 0.024447, GateReg 0.116413) | LR: 1.983384e-04
2025-09-21 08:37:52,303 [INFO] Epoch training completed in 104.70s with average loss 0.268347
2025-09-21 08:37:52,304 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:37:53,837 [INFO] Fold 2 - Epoch 1 - Train Loss: 0.268347 | EMA Val wMSE: 0.090357 | EMA Val SoftCE: 0.308178 | EMA Val Pearson: 0.259352
2025-09-21 08:37:53,843 [INFO] Fold 2 - Epoch 1 - New best EMA model with Pearson 0.259352.
2025-09-21 08:37:53,843 [INFO] Fold 2 - Epoch 2/6 started.
2025-09-21 08:38:13,014 [INFO] Train step 50/274 - Loss: 0.205382 (wMSE 0.070485, SoftCE 0.280746, Corr 0.535628, Cons 0.100777, CSim 0.062611, Rank 0.648959, RDropCLS 0.071875, RDropREG 0.009159, GateReg 0.083552) | LR: 1.942877e-04
2025-09-21 08:38:31,916 [INFO] Train step 100/274 - Loss: 0.201644 (wMSE 0.068626, SoftCE 0.278531, Corr 0.515792, Cons 0.091891, CSim 0.062472, Rank 0.644372, RDropCLS 0.067803, RDropREG 0.009013, GateReg 0.090157) | LR: 1.902280e-04
2025-09-21 08:38:50,629 [INFO] Train step 150/274 - Loss: 0.198775 (wMSE 0.066998, SoftCE 0.276707, Corr 0.500984, Cons 0.087785, CSim 0.062240, Rank 0.642435, RDropCLS 0.064495, RDropREG 0.008358, GateReg 0.094341) | LR: 1.851529e-04
2025-09-21 08:39:09,329 [INFO] Train step 200/274 - Loss: 0.196401 (wMSE 0.065804, SoftCE 0.274935, Corr 0.490308, Cons 0.081903, CSim 0.061339, Rank 0.640250, RDropCLS 0.060942, RDropREG 0.007997, GateReg 0.097518) | LR: 1.791195e-04
2025-09-21 08:39:28,063 [INFO] Train step 250/274 - Loss: 0.193878 (wMSE 0.064491, SoftCE 0.273134, Corr 0.477737, Cons 0.078076, CSim 0.060811, Rank 0.638072, RDropCLS 0.058334, RDropREG 0.007769, GateReg 0.099712) | LR: 1.721956e-04
2025-09-21 08:39:37,097 [INFO] Epoch training completed in 103.25s with average loss 0.193076
2025-09-21 08:39:37,098 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:39:38,672 [INFO] Fold 2 - Epoch 2 - Train Loss: 0.193076 | EMA Val wMSE: 0.071997 | EMA Val SoftCE: 0.281407 | EMA Val Pearson: 0.477475
2025-09-21 08:39:38,678 [INFO] Fold 2 - Epoch 2 - New best EMA model with Pearson 0.477475.
2025-09-21 08:39:38,678 [INFO] Fold 2 - Epoch 3/6 started.
2025-09-21 08:39:57,749 [INFO] Train step 50/274 - Loss: 0.170155 (wMSE 0.050677, SoftCE 0.257111, Corr 0.361785, Cons 0.055849, CSim 0.054032, Rank 0.613662, RDropCLS 0.053506, RDropREG 0.007070, GateReg 0.112892) | LR: 1.604825e-04
2025-09-21 08:40:16,524 [INFO] Train step 100/274 - Loss: 0.168850 (wMSE 0.050210, SoftCE 0.256496, Corr 0.352867, Cons 0.056146, CSim 0.054156, Rank 0.610451, RDropCLS 0.053459, RDropREG 0.007353, GateReg 0.112153) | LR: 1.517058e-04
2025-09-21 08:40:35,200 [INFO] Train step 150/274 - Loss: 0.168831 (wMSE 0.050313, SoftCE 0.256793, Corr 0.351544, Cons 0.055011, CSim 0.053831, Rank 0.610218, RDropCLS 0.052735, RDropREG 0.007295, GateReg 0.113483) | LR: 1.423473e-04
2025-09-21 08:40:53,910 [INFO] Train step 200/274 - Loss: 0.167814 (wMSE 0.049716, SoftCE 0.255998, Corr 0.347237, Cons 0.053872, CSim 0.053553, Rank 0.609013, RDropCLS 0.052075, RDropREG 0.007176, GateReg 0.114537) | LR: 1.325122e-04
2025-09-21 08:41:12,629 [INFO] Train step 250/274 - Loss: 0.167224 (wMSE 0.049434, SoftCE 0.255338, Corr 0.345108, Cons 0.052342, CSim 0.053140, Rank 0.608883, RDropCLS 0.051499, RDropREG 0.007021, GateReg 0.115441) | LR: 1.223112e-04
2025-09-21 08:41:21,623 [INFO] Epoch training completed in 102.94s with average loss 0.166658
2025-09-21 08:41:21,624 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:41:23,211 [INFO] Fold 2 - Epoch 3 - Train Loss: 0.166658 | EMA Val wMSE: 0.061510 | EMA Val SoftCE: 0.270772 | EMA Val Pearson: 0.565915
2025-09-21 08:41:23,217 [INFO] Fold 2 - Epoch 3 - New best EMA model with Pearson 0.565915.
2025-09-21 08:41:23,217 [INFO] Fold 2 - Epoch 4/6 started.
2025-09-21 08:41:42,327 [INFO] Train step 50/274 - Loss: 0.153843 (wMSE 0.041656, SoftCE 0.247101, Corr 0.276893, Cons 0.043887, CSim 0.048909, Rank 0.590868, RDropCLS 0.054953, RDropREG 0.008037, GateReg 0.120872) | LR: 1.067874e-04
2025-09-21 08:42:01,056 [INFO] Train step 100/274 - Loss: 0.153529 (wMSE 0.041378, SoftCE 0.246560, Corr 0.277468, Cons 0.043430, CSim 0.048794, Rank 0.590732, RDropCLS 0.053783, RDropREG 0.007605, GateReg 0.121717) | LR: 9.618007e-05
2025-09-21 08:42:19,778 [INFO] Train step 150/274 - Loss: 0.153439 (wMSE 0.041313, SoftCE 0.246018, Corr 0.279356, Cons 0.042563, CSim 0.048170, Rank 0.591008, RDropCLS 0.053343, RDropREG 0.007549, GateReg 0.122033) | LR: 8.561573e-05
2025-09-21 08:42:38,503 [INFO] Train step 200/274 - Loss: 0.152695 (wMSE 0.040854, SoftCE 0.245380, Corr 0.276225, Cons 0.042015, CSim 0.047938, Rank 0.590745, RDropCLS 0.052732, RDropREG 0.007326, GateReg 0.122233) | LR: 7.521326e-05
2025-09-21 08:42:57,229 [INFO] Train step 250/274 - Loss: 0.152067 (wMSE 0.040512, SoftCE 0.244929, Corr 0.273371, Cons 0.040795, CSim 0.047622, Rank 0.589901, RDropCLS 0.052908, RDropREG 0.007279, GateReg 0.122866) | LR: 6.508975e-05
2025-09-21 08:43:06,303 [INFO] Epoch training completed in 103.08s with average loss 0.151684
2025-09-21 08:43:06,304 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:43:07,869 [INFO] Fold 2 - Epoch 4 - Train Loss: 0.151684 | EMA Val wMSE: 0.056362 | EMA Val SoftCE: 0.262104 | EMA Val Pearson: 0.604277
2025-09-21 08:43:07,875 [INFO] Fold 2 - Epoch 4 - New best EMA model with Pearson 0.604277.
2025-09-21 08:43:07,876 [INFO] Fold 2 - Epoch 5/6 started.
2025-09-21 08:43:27,079 [INFO] Train step 50/274 - Loss: 0.143908 (wMSE 0.035891, SoftCE 0.238214, Corr 0.235725, Cons 0.036725, CSim 0.045788, Rank 0.581921, RDropCLS 0.051681, RDropREG 0.006768, GateReg 0.124907) | LR: 5.086032e-05
2025-09-21 08:43:45,883 [INFO] Train step 100/274 - Loss: 0.145080 (wMSE 0.036332, SoftCE 0.239201, Corr 0.243506, Cons 0.035739, CSim 0.044892, Rank 0.582855, RDropCLS 0.053158, RDropREG 0.006919, GateReg 0.124392) | LR: 4.191050e-05
2025-09-21 08:44:04,625 [INFO] Train step 150/274 - Loss: 0.145050 (wMSE 0.036170, SoftCE 0.239801, Corr 0.242159, Cons 0.035025, CSim 0.044686, Rank 0.582935, RDropCLS 0.053618, RDropREG 0.006946, GateReg 0.124024) | LR: 3.361443e-05
2025-09-21 08:44:23,374 [INFO] Train step 200/274 - Loss: 0.144538 (wMSE 0.035840, SoftCE 0.239003, Corr 0.241347, Cons 0.034095, CSim 0.044354, Rank 0.583346, RDropCLS 0.053095, RDropREG 0.006955, GateReg 0.123838) | LR: 2.606547e-05
2025-09-21 08:44:42,127 [INFO] Train step 250/274 - Loss: 0.144339 (wMSE 0.035717, SoftCE 0.238931, Corr 0.240242, Cons 0.033356, CSim 0.044296, Rank 0.583188, RDropCLS 0.053327, RDropREG 0.006997, GateReg 0.123641) | LR: 1.934857e-05
2025-09-21 08:44:51,170 [INFO] Epoch training completed in 103.29s with average loss 0.144143
2025-09-21 08:44:51,171 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:44:52,752 [INFO] Fold 2 - Epoch 5 - Train Loss: 0.144143 | EMA Val wMSE: 0.056368 | EMA Val SoftCE: 0.257466 | EMA Val Pearson: 0.619807
2025-09-21 08:44:52,758 [INFO] Fold 2 - Epoch 5 - New best EMA model with Pearson 0.619807.
2025-09-21 08:44:52,758 [INFO] Fold 2 - Epoch 6/6 started.
2025-09-21 08:45:11,849 [INFO] Train step 50/274 - Loss: 0.139559 (wMSE 0.032512, SoftCE 0.235092, Corr 0.218951, Cons 0.033896, CSim 0.043276, Rank 0.579394, RDropCLS 0.056759, RDropREG 0.007199, GateReg 0.122930) | LR: 1.109289e-05
2025-09-21 08:45:30,603 [INFO] Train step 100/274 - Loss: 0.140111 (wMSE 0.033075, SoftCE 0.236122, Corr 0.219313, Cons 0.033594, CSim 0.043463, Rank 0.577489, RDropCLS 0.056601, RDropREG 0.007165, GateReg 0.122997) | LR: 6.743759e-06
2025-09-21 08:45:49,362 [INFO] Train step 150/274 - Loss: 0.139964 (wMSE 0.033129, SoftCE 0.235618, Corr 0.219246, Cons 0.033203, CSim 0.043459, Rank 0.577467, RDropCLS 0.056016, RDropREG 0.007170, GateReg 0.123124) | LR: 3.444140e-06
2025-09-21 08:46:08,127 [INFO] Train step 200/274 - Loss: 0.139990 (wMSE 0.033038, SoftCE 0.235886, Corr 0.219459, Cons 0.033091, CSim 0.043169, Rank 0.577474, RDropCLS 0.055789, RDropREG 0.007149, GateReg 0.122992) | LR: 1.231166e-06
2025-09-21 08:46:26,876 [INFO] Train step 250/274 - Loss: 0.139961 (wMSE 0.032948, SoftCE 0.235887, Corr 0.220172, Cons 0.032791, CSim 0.042856, Rank 0.577261, RDropCLS 0.055631, RDropREG 0.007117, GateReg 0.122785) | LR: 1.297403e-07
2025-09-21 08:46:35,914 [INFO] Epoch training completed in 103.16s with average loss 0.139979
2025-09-21 08:46:35,916 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:46:37,490 [INFO] Fold 2 - Epoch 6 - Train Loss: 0.139979 | EMA Val wMSE: 0.058190 | EMA Val SoftCE: 0.256537 | EMA Val Pearson: 0.623993
2025-09-21 08:46:37,496 [INFO] Fold 2 - Epoch 6 - New best EMA model with Pearson 0.623993.
2025-09-21 08:46:37,646 [INFO] Generating validation predictions with MC-Dropout TTA and Swap TTA
2025-09-21 08:46:38,605 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 08:46:44,080 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 08:46:48,597 [INFO] Fold 2 - Final Val Pearson (with MC-Dropout TTA + swap): 0.635077 | Final Val MSE: 0.041367
2025-09-21 08:46:48,597 [INFO] Generating test predictions with MC-Dropout TTA and Swap TTA
2025-09-21 08:46:49,248 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 08:46:54,301 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 08:46:58,819 [INFO] ========== Fold 4/5 ==========
2025-09-21 08:46:59,483 [INFO] Applied swap augmentation. New train size: 52522
2025-09-21 08:46:59,493 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 08:46:59,493 [INFO] Train split: 52522 samples; Val split: 6564 samples.
2025-09-21 08:46:59,493 [INFO] Initializing PatentDataset with 52522 samples.
2025-09-21 08:46:59,493 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 08:46:59,493 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 08:46:59,496 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=6, max_len=64, ctxs=106, feat_dim=14, msd=5
2025-09-21 08:46:59,719 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 08:46:59,719 [INFO] Initializing EMA state from current model weights.
2025-09-21 08:46:59,930 [INFO] Fold 3 - Epoch 1/6 started.
2025-09-21 08:47:19,029 [INFO] Train step 50/274 - Loss: 0.353895 (wMSE 0.139242, SoftCE 0.458009, Corr 0.937673, Cons 0.188663, CSim 0.147801, Rank 0.692442, RDropCLS 0.887731, RDropREG 0.058786, GateReg 0.153598) | LR: 6.097561e-05
2025-09-21 08:47:37,709 [INFO] Train step 100/274 - Loss: 0.321854 (wMSE 0.127563, SoftCE 0.403734, Corr 0.903750, Cons 0.175418, CSim 0.114512, Rank 0.688373, RDropCLS 0.676881, RDropREG 0.047108, GateReg 0.122637) | LR: 1.219512e-04
2025-09-21 08:47:56,446 [INFO] Train step 150/274 - Loss: 0.300508 (wMSE 0.117973, SoftCE 0.374832, Corr 0.867681, Cons 0.152516, CSim 0.098954, Rank 0.683697, RDropCLS 0.529922, RDropREG 0.040008, GateReg 0.115451) | LR: 1.829268e-04
2025-09-21 08:48:15,276 [INFO] Train step 200/274 - Loss: 0.285930 (wMSE 0.111915, SoftCE 0.356679, Corr 0.831359, Cons 0.139183, CSim 0.091118, Rank 0.679649, RDropCLS 0.432524, RDropREG 0.034458, GateReg 0.109533) | LR: 1.997082e-04
2025-09-21 08:48:34,209 [INFO] Train step 250/274 - Loss: 0.273338 (wMSE 0.105817, SoftCE 0.343435, Corr 0.791080, Cons 0.130491, CSim 0.085806, Rank 0.675648, RDropCLS 0.365227, RDropREG 0.029993, GateReg 0.106964) | LR: 1.983384e-04
2025-09-21 08:48:43,189 [INFO] Epoch training completed in 103.26s with average loss 0.268418
2025-09-21 08:48:43,190 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:48:44,749 [INFO] Fold 3 - Epoch 1 - Train Loss: 0.268418 | EMA Val wMSE: 0.110394 | EMA Val SoftCE: 0.309294 | EMA Val Pearson: 0.210340
2025-09-21 08:48:44,755 [INFO] Fold 3 - Epoch 1 - New best EMA model with Pearson 0.210340.
2025-09-21 08:48:44,755 [INFO] Fold 3 - Epoch 2/6 started.
2025-09-21 08:49:03,867 [INFO] Train step 50/274 - Loss: 0.204466 (wMSE 0.071055, SoftCE 0.281035, Corr 0.522800, Cons 0.091218, CSim 0.063241, Rank 0.649676, RDropCLS 0.059597, RDropREG 0.007152, GateReg 0.103530) | LR: 1.942877e-04
2025-09-21 08:49:22,631 [INFO] Train step 100/274 - Loss: 0.199398 (wMSE 0.067749, SoftCE 0.278388, Corr 0.497091, Cons 0.088833, CSim 0.062482, Rank 0.643820, RDropCLS 0.057713, RDropREG 0.006672, GateReg 0.107905) | LR: 1.902280e-04
2025-09-21 08:49:41,363 [INFO] Train step 150/274 - Loss: 0.197062 (wMSE 0.066377, SoftCE 0.276252, Corr 0.488943, Cons 0.082883, CSim 0.061997, Rank 0.642143, RDropCLS 0.053905, RDropREG 0.006327, GateReg 0.111537) | LR: 1.851529e-04
2025-09-21 08:50:00,046 [INFO] Train step 200/274 - Loss: 0.194486 (wMSE 0.064853, SoftCE 0.274349, Corr 0.477611, Cons 0.079501, CSim 0.061353, Rank 0.639304, RDropCLS 0.052171, RDropREG 0.006199, GateReg 0.114963) | LR: 1.791195e-04
2025-09-21 08:50:18,776 [INFO] Train step 250/274 - Loss: 0.191832 (wMSE 0.063421, SoftCE 0.272423, Corr 0.464763, Cons 0.075055, CSim 0.060865, Rank 0.636511, RDropCLS 0.050994, RDropREG 0.006233, GateReg 0.118136) | LR: 1.721956e-04
2025-09-21 08:50:27,836 [INFO] Epoch training completed in 103.08s with average loss 0.191082
2025-09-21 08:50:27,838 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:50:29,425 [INFO] Fold 3 - Epoch 2 - Train Loss: 0.191082 | EMA Val wMSE: 0.081855 | EMA Val SoftCE: 0.285457 | EMA Val Pearson: 0.434736
2025-09-21 08:50:29,431 [INFO] Fold 3 - Epoch 2 - New best EMA model with Pearson 0.434736.
2025-09-21 08:50:29,432 [INFO] Fold 3 - Epoch 3/6 started.
2025-09-21 08:50:48,502 [INFO] Train step 50/274 - Loss: 0.170808 (wMSE 0.051935, SoftCE 0.258096, Corr 0.360871, Cons 0.050321, CSim 0.053851, Rank 0.611632, RDropCLS 0.047037, RDropREG 0.006466, GateReg 0.131773) | LR: 1.604825e-04
2025-09-21 08:51:07,168 [INFO] Train step 100/274 - Loss: 0.169796 (wMSE 0.051019, SoftCE 0.258580, Corr 0.352588, Cons 0.051549, CSim 0.053981, Rank 0.612033, RDropCLS 0.045140, RDropREG 0.006135, GateReg 0.131654) | LR: 1.517058e-04
2025-09-21 08:51:25,910 [INFO] Train step 150/274 - Loss: 0.168413 (wMSE 0.050216, SoftCE 0.257656, Corr 0.346042, Cons 0.051410, CSim 0.053933, Rank 0.609347, RDropCLS 0.045030, RDropREG 0.006059, GateReg 0.132105) | LR: 1.423473e-04
2025-09-21 08:51:44,629 [INFO] Train step 200/274 - Loss: 0.167506 (wMSE 0.049676, SoftCE 0.256446, Corr 0.343838, Cons 0.050719, CSim 0.053557, Rank 0.608544, RDropCLS 0.045159, RDropREG 0.006025, GateReg 0.132080) | LR: 1.325122e-04
2025-09-21 08:52:03,389 [INFO] Train step 250/274 - Loss: 0.166931 (wMSE 0.049308, SoftCE 0.255925, Corr 0.341842, Cons 0.050281, CSim 0.053270, Rank 0.608160, RDropCLS 0.044425, RDropREG 0.005979, GateReg 0.132343) | LR: 1.223112e-04
2025-09-21 08:52:12,396 [INFO] Epoch training completed in 102.96s with average loss 0.166569
2025-09-21 08:52:12,397 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:52:13,966 [INFO] Fold 3 - Epoch 3 - Train Loss: 0.166569 | EMA Val wMSE: 0.067338 | EMA Val SoftCE: 0.273658 | EMA Val Pearson: 0.538640
2025-09-21 08:52:13,972 [INFO] Fold 3 - Epoch 3 - New best EMA model with Pearson 0.538640.
2025-09-21 08:52:13,973 [INFO] Fold 3 - Epoch 4/6 started.
2025-09-21 08:52:33,068 [INFO] Train step 50/274 - Loss: 0.154785 (wMSE 0.042070, SoftCE 0.248566, Corr 0.282646, Cons 0.041194, CSim 0.049074, Rank 0.590063, RDropCLS 0.046831, RDropREG 0.006852, GateReg 0.135885) | LR: 1.067874e-04
2025-09-21 08:52:51,830 [INFO] Train step 100/274 - Loss: 0.154211 (wMSE 0.041620, SoftCE 0.247351, Corr 0.282053, Cons 0.040001, CSim 0.048679, Rank 0.593337, RDropCLS 0.045303, RDropREG 0.006666, GateReg 0.135317) | LR: 9.618007e-05
2025-09-21 08:53:10,610 [INFO] Train step 150/274 - Loss: 0.153962 (wMSE 0.041595, SoftCE 0.246602, Corr 0.282384, Cons 0.039384, CSim 0.048453, Rank 0.592902, RDropCLS 0.045783, RDropREG 0.006559, GateReg 0.135259) | LR: 8.561573e-05
2025-09-21 08:53:29,329 [INFO] Train step 200/274 - Loss: 0.153310 (wMSE 0.041066, SoftCE 0.246336, Corr 0.279528, Cons 0.038745, CSim 0.048015, Rank 0.592509, RDropCLS 0.045899, RDropREG 0.006455, GateReg 0.135338) | LR: 7.521326e-05
2025-09-21 08:53:48,031 [INFO] Train step 250/274 - Loss: 0.152661 (wMSE 0.040770, SoftCE 0.245516, Corr 0.277052, Cons 0.038731, CSim 0.047829, Rank 0.591618, RDropCLS 0.046435, RDropREG 0.006395, GateReg 0.135763) | LR: 6.508975e-05
2025-09-21 08:53:57,094 [INFO] Epoch training completed in 103.12s with average loss 0.152479
2025-09-21 08:53:57,095 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:53:58,658 [INFO] Fold 3 - Epoch 4 - Train Loss: 0.152479 | EMA Val wMSE: 0.060096 | EMA Val SoftCE: 0.264347 | EMA Val Pearson: 0.579959
2025-09-21 08:53:58,664 [INFO] Fold 3 - Epoch 4 - New best EMA model with Pearson 0.579959.
2025-09-21 08:53:58,664 [INFO] Fold 3 - Epoch 5/6 started.
2025-09-21 08:54:17,810 [INFO] Train step 50/274 - Loss: 0.146861 (wMSE 0.037099, SoftCE 0.243136, Corr 0.245126, Cons 0.035111, CSim 0.046027, Rank 0.583858, RDropCLS 0.047790, RDropREG 0.006462, GateReg 0.137797) | LR: 5.086032e-05
2025-09-21 08:54:36,630 [INFO] Train step 100/274 - Loss: 0.145824 (wMSE 0.036601, SoftCE 0.241088, Corr 0.243364, Cons 0.034946, CSim 0.045641, Rank 0.584098, RDropCLS 0.047976, RDropREG 0.006317, GateReg 0.137852) | LR: 4.191050e-05
2025-09-21 08:54:55,333 [INFO] Train step 150/274 - Loss: 0.145761 (wMSE 0.036470, SoftCE 0.241272, Corr 0.242980, Cons 0.035267, CSim 0.045716, Rank 0.583577, RDropCLS 0.048110, RDropREG 0.006298, GateReg 0.137360) | LR: 3.361443e-05
2025-09-21 08:55:14,062 [INFO] Train step 200/274 - Loss: 0.145310 (wMSE 0.036158, SoftCE 0.240729, Corr 0.242213, Cons 0.035093, CSim 0.045234, Rank 0.583021, RDropCLS 0.048223, RDropREG 0.006304, GateReg 0.137038) | LR: 2.606547e-05
2025-09-21 08:55:32,729 [INFO] Train step 250/274 - Loss: 0.144924 (wMSE 0.035946, SoftCE 0.240206, Corr 0.241057, Cons 0.034771, CSim 0.045032, Rank 0.582871, RDropCLS 0.048424, RDropREG 0.006381, GateReg 0.136866) | LR: 1.934857e-05
2025-09-21 08:55:41,773 [INFO] Epoch training completed in 103.11s with average loss 0.144721
2025-09-21 08:55:41,774 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:55:43,371 [INFO] Fold 3 - Epoch 5 - Train Loss: 0.144721 | EMA Val wMSE: 0.058620 | EMA Val SoftCE: 0.260462 | EMA Val Pearson: 0.596535
2025-09-21 08:55:43,377 [INFO] Fold 3 - Epoch 5 - New best EMA model with Pearson 0.596535.
2025-09-21 08:55:43,377 [INFO] Fold 3 - Epoch 6/6 started.
2025-09-21 08:56:02,543 [INFO] Train step 50/274 - Loss: 0.141431 (wMSE 0.033670, SoftCE 0.238161, Corr 0.225169, Cons 0.032995, CSim 0.042581, Rank 0.578451, RDropCLS 0.049514, RDropREG 0.006645, GateReg 0.136476) | LR: 1.109289e-05
2025-09-21 08:56:21,230 [INFO] Train step 100/274 - Loss: 0.141126 (wMSE 0.033493, SoftCE 0.237179, Corr 0.224967, Cons 0.033485, CSim 0.043438, Rank 0.579437, RDropCLS 0.049414, RDropREG 0.006664, GateReg 0.137120) | LR: 6.743759e-06
2025-09-21 08:56:40,067 [INFO] Train step 150/274 - Loss: 0.141121 (wMSE 0.033613, SoftCE 0.236853, Corr 0.225354, Cons 0.033650, CSim 0.043571, Rank 0.579194, RDropCLS 0.049403, RDropREG 0.006652, GateReg 0.136719) | LR: 3.444140e-06
2025-09-21 08:56:58,794 [INFO] Train step 200/274 - Loss: 0.141604 (wMSE 0.033860, SoftCE 0.237699, Corr 0.226385, Cons 0.033543, CSim 0.043752, Rank 0.579277, RDropCLS 0.049403, RDropREG 0.006638, GateReg 0.136647) | LR: 1.231166e-06
2025-09-21 08:57:17,485 [INFO] Train step 250/274 - Loss: 0.141267 (wMSE 0.033662, SoftCE 0.237292, Corr 0.225716, Cons 0.033570, CSim 0.043555, Rank 0.578469, RDropCLS 0.049300, RDropREG 0.006640, GateReg 0.136528) | LR: 1.297403e-07
2025-09-21 08:57:26,510 [INFO] Epoch training completed in 103.13s with average loss 0.141396
2025-09-21 08:57:26,512 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:57:28,116 [INFO] Fold 3 - Epoch 6 - Train Loss: 0.141396 | EMA Val wMSE: 0.059602 | EMA Val SoftCE: 0.259928 | EMA Val Pearson: 0.602333
2025-09-21 08:57:28,123 [INFO] Fold 3 - Epoch 6 - New best EMA model with Pearson 0.602333.
2025-09-21 08:57:28,272 [INFO] Generating validation predictions with MC-Dropout TTA and Swap TTA
2025-09-21 08:57:29,297 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 08:57:35,062 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 08:57:39,825 [INFO] Fold 3 - Final Val Pearson (with MC-Dropout TTA + swap): 0.613615 | Final Val MSE: 0.043214
2025-09-21 08:57:39,825 [INFO] Generating test predictions with MC-Dropout TTA and Swap TTA
2025-09-21 08:57:40,502 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 08:57:45,913 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 08:57:50,743 [INFO] ========== Fold 5/5 ==========
2025-09-21 08:57:50,801 [INFO] Applied swap augmentation. New train size: 52526
2025-09-21 08:57:50,819 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 08:57:50,819 [INFO] Train split: 52526 samples; Val split: 6562 samples.
2025-09-21 08:57:50,819 [INFO] Initializing PatentDataset with 52526 samples.
2025-09-21 08:57:50,819 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 08:57:50,819 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 08:57:50,827 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=6, max_len=64, ctxs=106, feat_dim=14, msd=5
2025-09-21 08:57:51,056 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 08:57:51,056 [INFO] Initializing EMA state from current model weights.
2025-09-21 08:57:51,272 [INFO] Fold 4 - Epoch 1/6 started.
2025-09-21 08:58:10,415 [INFO] Train step 50/274 - Loss: 0.363915 (wMSE 0.141158, SoftCE 0.480416, Corr 0.974453, Cons 0.154902, CSim 0.155982, Rank 0.697707, RDropCLS 0.804516, RDropREG 0.073435, GateReg 0.318080) | LR: 6.097561e-05
2025-09-21 08:58:29,133 [INFO] Train step 100/274 - Loss: 0.328002 (wMSE 0.125792, SoftCE 0.420950, Corr 0.932508, Cons 0.128368, CSim 0.119982, Rank 0.693260, RDropCLS 0.637146, RDropREG 0.064456, GateReg 0.262793) | LR: 1.219512e-04
2025-09-21 08:58:47,862 [INFO] Train step 150/274 - Loss: 0.308635 (wMSE 0.118502, SoftCE 0.390927, Corr 0.902326, Cons 0.117241, CSim 0.103665, Rank 0.689575, RDropCLS 0.514342, RDropREG 0.052708, GateReg 0.234764) | LR: 1.829268e-04
2025-09-21 08:59:06,604 [INFO] Train step 200/274 - Loss: 0.294264 (wMSE 0.113066, SoftCE 0.369876, Corr 0.874904, Cons 0.109386, CSim 0.094454, Rank 0.686303, RDropCLS 0.420263, RDropREG 0.044114, GateReg 0.210040) | LR: 1.997082e-04
2025-09-21 08:59:25,461 [INFO] Train step 250/274 - Loss: 0.281781 (wMSE 0.107580, SoftCE 0.355210, Corr 0.837635, Cons 0.105993, CSim 0.088810, Rank 0.682255, RDropCLS 0.352476, RDropREG 0.037689, GateReg 0.187645) | LR: 1.983384e-04
2025-09-21 08:59:34,485 [INFO] Epoch training completed in 103.21s with average loss 0.276657
2025-09-21 08:59:34,487 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:59:36,062 [INFO] Fold 4 - Epoch 1 - Train Loss: 0.276657 | EMA Val wMSE: 0.092642 | EMA Val SoftCE: 0.317721 | EMA Val Pearson: 0.222247
2025-09-21 08:59:36,069 [INFO] Fold 4 - Epoch 1 - New best EMA model with Pearson 0.222247.
2025-09-21 08:59:36,069 [INFO] Fold 4 - Epoch 2/6 started.
2025-09-21 08:59:55,309 [INFO] Train step 50/274 - Loss: 0.210368 (wMSE 0.074049, SoftCE 0.285877, Corr 0.555360, Cons 0.092186, CSim 0.063741, Rank 0.654151, RDropCLS 0.054565, RDropREG 0.007059, GateReg 0.109788) | LR: 1.942877e-04
2025-09-21 09:00:14,131 [INFO] Train step 100/274 - Loss: 0.205665 (wMSE 0.071364, SoftCE 0.282789, Corr 0.532137, Cons 0.086838, CSim 0.062782, Rank 0.649852, RDropCLS 0.050641, RDropREG 0.006563, GateReg 0.112060) | LR: 1.902280e-04
2025-09-21 09:00:32,831 [INFO] Train step 150/274 - Loss: 0.202089 (wMSE 0.069147, SoftCE 0.280570, Corr 0.514402, Cons 0.083427, CSim 0.062125, Rank 0.646779, RDropCLS 0.049359, RDropREG 0.006528, GateReg 0.113687) | LR: 1.851529e-04
2025-09-21 09:00:51,561 [INFO] Train step 200/274 - Loss: 0.198235 (wMSE 0.066759, SoftCE 0.278166, Corr 0.495305, Cons 0.080708, CSim 0.061468, Rank 0.642638, RDropCLS 0.048992, RDropREG 0.006420, GateReg 0.115856) | LR: 1.791195e-04
2025-09-21 09:01:10,302 [INFO] Train step 250/274 - Loss: 0.195570 (wMSE 0.065292, SoftCE 0.276323, Corr 0.481456, Cons 0.077661, CSim 0.060908, Rank 0.640394, RDropCLS 0.048716, RDropREG 0.006335, GateReg 0.118027) | LR: 1.721956e-04
2025-09-21 09:01:19,325 [INFO] Epoch training completed in 103.26s with average loss 0.194444
2025-09-21 09:01:19,327 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 09:01:20,910 [INFO] Fold 4 - Epoch 2 - Train Loss: 0.194444 | EMA Val wMSE: 0.077995 | EMA Val SoftCE: 0.286802 | EMA Val Pearson: 0.438595
2025-09-21 09:01:20,917 [INFO] Fold 4 - Epoch 2 - New best EMA model with Pearson 0.438595.
2025-09-21 09:01:20,917 [INFO] Fold 4 - Epoch 3/6 started.
2025-09-21 09:01:40,062 [INFO] Train step 50/274 - Loss: 0.170419 (wMSE 0.051439, SoftCE 0.261089, Corr 0.345829, Cons 0.058815, CSim 0.054613, Rank 0.610198, RDropCLS 0.050371, RDropREG 0.006627, GateReg 0.128819) | LR: 1.604825e-04
2025-09-21 09:01:58,890 [INFO] Train step 100/274 - Loss: 0.169113 (wMSE 0.050074, SoftCE 0.259436, Corr 0.345922, Cons 0.056103, CSim 0.054012, Rank 0.610968, RDropCLS 0.048878, RDropREG 0.006611, GateReg 0.130578) | LR: 1.517058e-04
2025-09-21 09:02:17,631 [INFO] Train step 150/274 - Loss: 0.169076 (wMSE 0.050100, SoftCE 0.259116, Corr 0.346906, Cons 0.054233, CSim 0.053505, Rank 0.611470, RDropCLS 0.048748, RDropREG 0.006589, GateReg 0.131834) | LR: 1.423473e-04
2025-09-21 09:02:36,389 [INFO] Train step 200/274 - Loss: 0.167800 (wMSE 0.049368, SoftCE 0.257651, Corr 0.343306, Cons 0.052146, CSim 0.052773, Rank 0.610097, RDropCLS 0.048526, RDropREG 0.006548, GateReg 0.133054) | LR: 1.325122e-04
2025-09-21 09:02:55,230 [INFO] Train step 250/274 - Loss: 0.166869 (wMSE 0.048921, SoftCE 0.256981, Corr 0.338507, Cons 0.051555, CSim 0.052708, Rank 0.608144, RDropCLS 0.048899, RDropREG 0.006564, GateReg 0.134032) | LR: 1.223112e-04
2025-09-21 09:03:04,266 [INFO] Epoch training completed in 103.35s with average loss 0.166367
2025-09-21 09:03:04,268 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 09:03:05,849 [INFO] Fold 4 - Epoch 3 - Train Loss: 0.166367 | EMA Val wMSE: 0.064899 | EMA Val SoftCE: 0.273730 | EMA Val Pearson: 0.559009
2025-09-21 09:03:05,855 [INFO] Fold 4 - Epoch 3 - New best EMA model with Pearson 0.559009.
2025-09-21 09:03:05,855 [INFO] Fold 4 - Epoch 4/6 started.
2025-09-21 09:03:25,065 [INFO] Train step 50/274 - Loss: 0.151326 (wMSE 0.039802, SoftCE 0.245917, Corr 0.264073, Cons 0.043443, CSim 0.047626, Rank 0.590242, RDropCLS 0.050429, RDropREG 0.006656, GateReg 0.139383) | LR: 1.067874e-04
2025-09-21 09:03:43,810 [INFO] Train step 100/274 - Loss: 0.150535 (wMSE 0.039246, SoftCE 0.243677, Corr 0.266895, Cons 0.040059, CSim 0.047028, Rank 0.590595, RDropCLS 0.052504, RDropREG 0.006823, GateReg 0.138826) | LR: 9.618007e-05
2025-09-21 09:04:02,576 [INFO] Train step 150/274 - Loss: 0.151065 (wMSE 0.039645, SoftCE 0.244318, Corr 0.268338, Cons 0.039370, CSim 0.047278, Rank 0.590812, RDropCLS 0.052159, RDropREG 0.006855, GateReg 0.138786) | LR: 8.561573e-05
2025-09-21 09:04:21,354 [INFO] Train step 200/274 - Loss: 0.150846 (wMSE 0.039418, SoftCE 0.244518, Corr 0.267137, Cons 0.038996, CSim 0.047011, Rank 0.590100, RDropCLS 0.052029, RDropREG 0.006905, GateReg 0.139024) | LR: 7.521326e-05
2025-09-21 09:04:40,102 [INFO] Train step 250/274 - Loss: 0.150956 (wMSE 0.039467, SoftCE 0.244716, Corr 0.267586, Cons 0.038630, CSim 0.046963, Rank 0.589977, RDropCLS 0.052205, RDropREG 0.006912, GateReg 0.138916) | LR: 6.508975e-05
2025-09-21 09:04:49,132 [INFO] Epoch training completed in 103.28s with average loss 0.150887
2025-09-21 09:04:49,134 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 09:04:50,715 [INFO] Fold 4 - Epoch 4 - Train Loss: 0.150887 | EMA Val wMSE: 0.057996 | EMA Val SoftCE: 0.262881 | EMA Val Pearson: 0.601857
2025-09-21 09:04:50,722 [INFO] Fold 4 - Epoch 4 - New best EMA model with Pearson 0.601857.
2025-09-21 09:04:50,722 [INFO] Fold 4 - Epoch 5/6 started.
2025-09-21 09:05:09,803 [INFO] Train step 50/274 - Loss: 0.143767 (wMSE 0.035299, SoftCE 0.239609, Corr 0.234387, Cons 0.033033, CSim 0.044139, Rank 0.580201, RDropCLS 0.053623, RDropREG 0.007058, GateReg 0.139721) | LR: 5.086032e-05
2025-09-21 09:05:28,559 [INFO] Train step 100/274 - Loss: 0.144269 (wMSE 0.035378, SoftCE 0.240351, Corr 0.235993, Cons 0.034210, CSim 0.044622, Rank 0.581415, RDropCLS 0.053713, RDropREG 0.006969, GateReg 0.139218) | LR: 4.191050e-05
2025-09-21 09:05:47,295 [INFO] Train step 150/274 - Loss: 0.143966 (wMSE 0.035271, SoftCE 0.239899, Corr 0.235299, Cons 0.033365, CSim 0.044071, Rank 0.581112, RDropCLS 0.053959, RDropREG 0.007127, GateReg 0.139386) | LR: 3.361443e-05
2025-09-21 09:06:06,010 [INFO] Train step 200/274 - Loss: 0.143729 (wMSE 0.035056, SoftCE 0.239195, Corr 0.236448, Cons 0.032863, CSim 0.043816, Rank 0.581633, RDropCLS 0.053846, RDropREG 0.007051, GateReg 0.139310) | LR: 2.606547e-05
2025-09-21 09:06:24,763 [INFO] Train step 250/274 - Loss: 0.143126 (wMSE 0.034796, SoftCE 0.238497, Corr 0.233216, Cons 0.033096, CSim 0.044012, Rank 0.581259, RDropCLS 0.054392, RDropREG 0.007115, GateReg 0.139372) | LR: 1.934857e-05
2025-09-21 09:06:33,798 [INFO] Epoch training completed in 103.07s with average loss 0.143117
2025-09-21 09:06:33,799 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 09:06:35,367 [INFO] Fold 4 - Epoch 5 - Train Loss: 0.143117 | EMA Val wMSE: 0.057982 | EMA Val SoftCE: 0.258007 | EMA Val Pearson: 0.613106
2025-09-21 09:06:35,373 [INFO] Fold 4 - Epoch 5 - New best EMA model with Pearson 0.613106.
2025-09-21 09:06:35,373 [INFO] Fold 4 - Epoch 6/6 started.
2025-09-21 09:06:54,731 [INFO] Train step 50/274 - Loss: 0.137793 (wMSE 0.031369, SoftCE 0.234510, Corr 0.209475, Cons 0.031784, CSim 0.042116, Rank 0.576437, RDropCLS 0.055373, RDropREG 0.007052, GateReg 0.137636) | LR: 1.109289e-05
2025-09-21 09:07:13,462 [INFO] Train step 100/274 - Loss: 0.138724 (wMSE 0.032174, SoftCE 0.235228, Corr 0.213145, Cons 0.031672, CSim 0.042458, Rank 0.575522, RDropCLS 0.055524, RDropREG 0.007177, GateReg 0.138780) | LR: 6.743759e-06
2025-09-21 09:07:32,205 [INFO] Train step 150/274 - Loss: 0.139143 (wMSE 0.032358, SoftCE 0.235427, Corr 0.216014, Cons 0.031452, CSim 0.042383, Rank 0.576385, RDropCLS 0.055193, RDropREG 0.007199, GateReg 0.138714) | LR: 3.444140e-06
2025-09-21 09:07:50,931 [INFO] Train step 200/274 - Loss: 0.139357 (wMSE 0.032455, SoftCE 0.235942, Corr 0.216287, Cons 0.031220, CSim 0.042551, Rank 0.576051, RDropCLS 0.054965, RDropREG 0.007203, GateReg 0.139002) | LR: 1.231166e-06
2025-09-21 09:08:09,709 [INFO] Train step 250/274 - Loss: 0.139345 (wMSE 0.032376, SoftCE 0.236139, Corr 0.215656, Cons 0.031196, CSim 0.042664, Rank 0.576465, RDropCLS 0.054871, RDropREG 0.007190, GateReg 0.138916) | LR: 1.297403e-07
2025-09-21 09:08:18,730 [INFO] Epoch training completed in 103.36s with average loss 0.139232
2025-09-21 09:08:18,732 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 09:08:20,314 [INFO] Fold 4 - Epoch 6 - Train Loss: 0.139232 | EMA Val wMSE: 0.060394 | EMA Val SoftCE: 0.257082 | EMA Val Pearson: 0.614387
2025-09-21 09:08:20,320 [INFO] Fold 4 - Epoch 6 - New best EMA model with Pearson 0.614387.
2025-09-21 09:08:20,470 [INFO] Generating validation predictions with MC-Dropout TTA and Swap TTA
2025-09-21 09:08:21,524 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 09:08:27,212 [INFO] Running MC-Dropout prediction with 4 samples.
2025-09-21 09:08:31,901 [INFO] Fold 4 - Final Val Pearson (with MC-Dropout TTA + swap): 0.626484 | Final Val MSE: 0.042241
2025-09-21 09:08:31,902 [INFO] Generating test predictions with MC-Dropout TTA and Swap TTA
2025-09-21 09:08:32,589 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 09:08:37,954 [INFO] Running MC-Dropout prediction with 6 samples.
2025-09-21 09:08:42,627 [INFO] OOF Pearson correlation across all folds (pre-rule, pre-calibration): 0.622500
2025-09-21 09:08:42,628 [INFO] OOF MSE across all folds (pre-rule, pre-calibration): 0.042563
2025-09-21 09:08:42,628 [INFO] Ensembling test predictions from all folds by averaging.
2025-09-21 09:08:42,628 [INFO] Applying rule-based post-processing for anchor == target -> score = 1.0
2025-09-21 09:08:42,629 [INFO] Found 24 exact anchor==target pairs in test set.
2025-09-21 09:08:42,632 [INFO] Found 255 exact anchor==target pairs in train (OOF) set; applying same rule to OOF predictions.
2025-09-21 09:08:42,632 [INFO] OOF Pearson after rule (pre-calibration): 0.623430
2025-09-21 09:08:42,632 [INFO] OOF MSE after rule (pre-calibration): 0.042547
2025-09-21 09:08:42,632 [INFO] Fitting isotonic regression (PAV) calibrator on OOF predictions.
2025-09-21 09:08:42,673 [INFO] Isotonic produced 373 monotonic blocks.
2025-09-21 09:08:42,676 [INFO] OOF Pearson after isotonic calibration: 0.633036
2025-09-21 09:08:42,676 [INFO] OOF MSE after isotonic calibration: 0.040153
2025-09-21 09:08:42,676 [INFO] Logging final validation results (OOF, post-rule, post-calibration):
2025-09-21 09:08:42,676 [INFO] Final Validation Pearson: 0.633036 | Final Validation MSE: 0.040153
2025-09-21 09:08:42,680 [INFO] Writing submission to task/us-patent-phrase-to-phrase-matching/outputs/1/submission_10.csv
2025-09-21 09:08:42,689 [INFO] Submission file created successfully.
2025-09-21 09:08:42,689 [INFO] All done (v10).
