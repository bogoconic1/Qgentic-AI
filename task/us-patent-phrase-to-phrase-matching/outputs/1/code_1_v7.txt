2025-09-21 06:16:18,599 [INFO] Initialized logging and created output directories for v7 pipeline.
2025-09-21 06:16:18,600 [INFO] Setting all random seeds to 42.
2025-09-21 06:16:18,600 [INFO] Reading train data from task/us-patent-phrase-to-phrase-matching/train.csv.
2025-09-21 06:16:18,637 [INFO] Reading test data from task/us-patent-phrase-to-phrase-matching/test.csv.
2025-09-21 06:16:18,643 [INFO] Train shape: (32825, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 06:16:18,643 [INFO] Test shape:  (3648, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 06:16:18,648 [INFO] Train missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 06:16:18,648 [INFO] Test missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 06:16:18,648 [INFO] Building vocabulary from segmented corpus.
2025-09-21 06:16:18,986 [INFO] Built vocabulary of size 9042.
2025-09-21 06:16:18,986 [INFO] Tokenizing and numericalizing dataframe with 32825 rows (with token types).
2025-09-21 06:16:19,103 [INFO] Processed 5000 rows.
2025-09-21 06:16:19,219 [INFO] Processed 10000 rows.
2025-09-21 06:16:19,336 [INFO] Processed 15000 rows.
2025-09-21 06:16:19,452 [INFO] Processed 20000 rows.
2025-09-21 06:16:19,568 [INFO] Processed 25000 rows.
2025-09-21 06:16:19,685 [INFO] Processed 30000 rows.
2025-09-21 06:16:19,751 [INFO] Tokenizing and numericalizing dataframe with 3648 rows (with token types).
2025-09-21 06:16:19,836 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 3648 rows (anchor<->target).
2025-09-21 06:16:19,921 [INFO] Prepared swapped TTA arrays for test data.
2025-09-21 06:16:19,921 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 32825 rows (anchor<->target).
2025-09-21 06:16:20,037 [INFO] Processed 5000 swapped rows.
2025-09-21 06:16:20,155 [INFO] Processed 10000 swapped rows.
2025-09-21 06:16:20,271 [INFO] Processed 15000 swapped rows.
2025-09-21 06:16:20,387 [INFO] Processed 20000 swapped rows.
2025-09-21 06:16:20,502 [INFO] Processed 25000 swapped rows.
2025-09-21 06:16:20,618 [INFO] Processed 30000 swapped rows.
2025-09-21 06:16:20,683 [INFO] Prepared swapped augmentation arrays for training.
2025-09-21 06:16:20,684 [INFO] Computing handcrafted similarity features.
2025-09-21 06:16:21,114 [INFO] Computed handcrafted features for 5000 rows.
2025-09-21 06:16:21,537 [INFO] Computed handcrafted features for 10000 rows.
2025-09-21 06:16:21,966 [INFO] Computed handcrafted features for 15000 rows.
2025-09-21 06:16:22,415 [INFO] Computed handcrafted features for 20000 rows.
2025-09-21 06:16:22,869 [INFO] Computed handcrafted features for 25000 rows.
2025-09-21 06:16:23,298 [INFO] Computed handcrafted features for 30000 rows.
2025-09-21 06:16:23,545 [INFO] Finished computing handcrafted features.
2025-09-21 06:16:23,546 [INFO] Computing handcrafted similarity features.
2025-09-21 06:16:23,859 [INFO] Finished computing handcrafted features.
2025-09-21 06:16:23,859 [INFO] Handcrafted feature dimension: 8
2025-09-21 06:16:23,859 [INFO] Labels statistics: min=0.0, max=1.0, mean=0.3619, std=0.2588
2025-09-21 06:16:23,864 [INFO] Found 106 unique context codes across train+test.
2025-09-21 06:16:24,122 [INFO] Class counts: [6774.0, 10306.0, 11068.0, 3634.0, 1043.0] | CE weights: [0.15397107601165771, 0.10120318084955215, 0.0942356288433075, 0.2870115637779236, 1.0]
2025-09-21 06:16:24,124 [INFO] Creating stratified folds on 5-class bins.
2025-09-21 06:16:24,140 [INFO] Fold 0: 6567 samples.
2025-09-21 06:16:24,140 [INFO] Fold 1: 6566 samples.
2025-09-21 06:16:24,140 [INFO] Fold 2: 6566 samples.
2025-09-21 06:16:24,140 [INFO] Fold 3: 6564 samples.
2025-09-21 06:16:24,140 [INFO] Fold 4: 6562 samples.
2025-09-21 06:16:24,141 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 06:16:24,141 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 06:16:24,141 [INFO] ========== Fold 1/5 ==========
2025-09-21 06:16:25,749 [INFO] Applied swap augmentation. New train size: 52516
2025-09-21 06:16:25,761 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 06:16:25,761 [INFO] Train split: 52516 samples; Val split: 6567 samples.
2025-09-21 06:16:25,761 [INFO] Initializing PatentDataset with 52516 samples.
2025-09-21 06:16:25,761 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 06:16:25,761 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 06:16:25,762 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 06:16:26,776 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 06:16:26,776 [INFO] Initializing EMA state from current model weights.
2025-09-21 06:16:26,925 [INFO] Fold 0 - Epoch 1/6 started.
2025-09-21 06:16:39,834 [INFO] Train step 50/274 - Loss: 1.026068 (MSE 0.114246, CE 2.410213, Corr 0.950247, Cons 0.135003, CSim 0.126776, Rank 0.692003) | LR: 6.097561e-05
2025-09-21 06:16:51,647 [INFO] Train step 100/274 - Loss: 0.922844 (MSE 0.094048, CE 2.159648, Corr 0.913064, Cons 0.127984, CSim 0.104402, Rank 0.688318) | LR: 1.219512e-04
2025-09-21 06:17:03,437 [INFO] Train step 150/274 - Loss: 0.870165 (MSE 0.086734, CE 2.029789, Corr 0.884629, Cons 0.116520, CSim 0.092773, Rank 0.685911) | LR: 1.829268e-04
2025-09-21 06:17:15,223 [INFO] Train step 200/274 - Loss: 0.835120 (MSE 0.081706, CE 1.945048, Corr 0.859126, Cons 0.109564, CSim 0.085996, Rank 0.683878) | LR: 1.997082e-04
2025-09-21 06:17:26,985 [INFO] Train step 250/274 - Loss: 0.806980 (MSE 0.078626, CE 1.877223, Corr 0.830910, Cons 0.102462, CSim 0.082280, Rank 0.681429) | LR: 1.983384e-04
2025-09-21 06:17:32,686 [INFO] Epoch training completed in 65.76s with average loss 0.795828
2025-09-21 06:17:32,687 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:17:34,203 [INFO] Fold 0 - Epoch 1 - Train Loss: 0.795828 | EMA Val MSE: 0.075913 | EMA Val CE: 1.752642 | EMA Val Pearson: 0.167220
2025-09-21 06:17:34,207 [INFO] Fold 0 - Epoch 1 - New best EMA model with Pearson 0.167220.
2025-09-21 06:17:34,207 [INFO] Fold 0 - Epoch 2/6 started.
2025-09-21 06:17:46,244 [INFO] Train step 50/274 - Loss: 0.658976 (MSE 0.060960, CE 1.538804, Corr 0.619745, Cons 0.060884, CSim 0.064517, Rank 0.656871) | LR: 1.942877e-04
2025-09-21 06:17:58,092 [INFO] Train step 100/274 - Loss: 0.648292 (MSE 0.059006, CE 1.517394, Corr 0.596261, Cons 0.056530, CSim 0.063806, Rank 0.654574) | LR: 1.902280e-04
2025-09-21 06:18:09,819 [INFO] Train step 150/274 - Loss: 0.640536 (MSE 0.057963, CE 1.501140, Corr 0.579869, Cons 0.053749, CSim 0.063576, Rank 0.652290) | LR: 1.851529e-04
2025-09-21 06:18:21,675 [INFO] Train step 200/274 - Loss: 0.631527 (MSE 0.056766, CE 1.481430, Corr 0.564349, Cons 0.050924, CSim 0.062860, Rank 0.649920) | LR: 1.791195e-04
2025-09-21 06:18:33,516 [INFO] Train step 250/274 - Loss: 0.626113 (MSE 0.055808, CE 1.470806, Corr 0.551619, Cons 0.048770, CSim 0.062410, Rank 0.648214) | LR: 1.721956e-04
2025-09-21 06:18:39,246 [INFO] Epoch training completed in 65.04s with average loss 0.622748
2025-09-21 06:18:39,247 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:18:40,766 [INFO] Fold 0 - Epoch 2 - Train Loss: 0.622748 | EMA Val MSE: 0.057744 | EMA Val CE: 1.517294 | EMA Val Pearson: 0.397073
2025-09-21 06:18:40,770 [INFO] Fold 0 - Epoch 2 - New best EMA model with Pearson 0.397073.
2025-09-21 06:18:40,770 [INFO] Fold 0 - Epoch 3/6 started.
2025-09-21 06:18:53,006 [INFO] Train step 50/274 - Loss: 0.547404 (MSE 0.045968, CE 1.297259, Corr 0.417621, Cons 0.035668, CSim 0.056990, Rank 0.622173) | LR: 1.604825e-04
2025-09-21 06:19:04,805 [INFO] Train step 100/274 - Loss: 0.542184 (MSE 0.044843, CE 1.286515, Corr 0.408034, Cons 0.034078, CSim 0.056411, Rank 0.621906) | LR: 1.517058e-04
2025-09-21 06:19:16,577 [INFO] Train step 150/274 - Loss: 0.542184 (MSE 0.044836, CE 1.287189, Corr 0.406274, Cons 0.033381, CSim 0.056179, Rank 0.620593) | LR: 1.423473e-04
2025-09-21 06:19:28,344 [INFO] Train step 200/274 - Loss: 0.538567 (MSE 0.044168, CE 1.279653, Corr 0.399313, Cons 0.033246, CSim 0.055739, Rank 0.619980) | LR: 1.325122e-04
2025-09-21 06:19:40,132 [INFO] Train step 250/274 - Loss: 0.533530 (MSE 0.043559, CE 1.268075, Corr 0.392614, Cons 0.032143, CSim 0.055402, Rank 0.618482) | LR: 1.223112e-04
2025-09-21 06:19:45,816 [INFO] Epoch training completed in 65.05s with average loss 0.532201
2025-09-21 06:19:45,817 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:19:47,336 [INFO] Fold 0 - Epoch 3 - Train Loss: 0.532201 | EMA Val MSE: 0.051012 | EMA Val CE: 1.400047 | EMA Val Pearson: 0.507289
2025-09-21 06:19:47,340 [INFO] Fold 0 - Epoch 3 - New best EMA model with Pearson 0.507289.
2025-09-21 06:19:47,340 [INFO] Fold 0 - Epoch 4/6 started.
2025-09-21 06:19:59,387 [INFO] Train step 50/274 - Loss: 0.476674 (MSE 0.036110, CE 1.138197, Corr 0.313833, Cons 0.027134, CSim 0.051118, Rank 0.604792) | LR: 1.067874e-04
2025-09-21 06:20:11,128 [INFO] Train step 100/274 - Loss: 0.475122 (MSE 0.035950, CE 1.135426, Corr 0.309488, Cons 0.026688, CSim 0.051170, Rank 0.601990) | LR: 9.618007e-05
2025-09-21 06:20:22,903 [INFO] Train step 150/274 - Loss: 0.474146 (MSE 0.035730, CE 1.133407, Corr 0.308219, Cons 0.026078, CSim 0.051013, Rank 0.601456) | LR: 8.561573e-05
2025-09-21 06:20:34,789 [INFO] Train step 200/274 - Loss: 0.470366 (MSE 0.035399, CE 1.124053, Corr 0.304866, Cons 0.025808, CSim 0.050778, Rank 0.600680) | LR: 7.521326e-05
2025-09-21 06:20:46,497 [INFO] Train step 250/274 - Loss: 0.467944 (MSE 0.035291, CE 1.117955, Corr 0.302895, Cons 0.025233, CSim 0.050792, Rank 0.599487) | LR: 6.508975e-05
2025-09-21 06:20:52,253 [INFO] Epoch training completed in 64.91s with average loss 0.467680
2025-09-21 06:20:52,254 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:20:53,767 [INFO] Fold 0 - Epoch 4 - Train Loss: 0.467680 | EMA Val MSE: 0.048498 | EMA Val CE: 1.330267 | EMA Val Pearson: 0.556062
2025-09-21 06:20:53,771 [INFO] Fold 0 - Epoch 4 - New best EMA model with Pearson 0.556062.
2025-09-21 06:20:53,771 [INFO] Fold 0 - Epoch 5/6 started.
2025-09-21 06:21:05,959 [INFO] Train step 50/274 - Loss: 0.433012 (MSE 0.029959, CE 1.039590, Corr 0.255920, Cons 0.022938, CSim 0.047868, Rank 0.587047) | LR: 5.086032e-05
2025-09-21 06:21:17,700 [INFO] Train step 100/274 - Loss: 0.431328 (MSE 0.029613, CE 1.035881, Corr 0.252930, Cons 0.022957, CSim 0.048043, Rank 0.587743) | LR: 4.191050e-05
2025-09-21 06:21:29,448 [INFO] Train step 150/274 - Loss: 0.431785 (MSE 0.030193, CE 1.035440, Corr 0.256316, Cons 0.022748, CSim 0.048303, Rank 0.587975) | LR: 3.361443e-05
2025-09-21 06:21:41,144 [INFO] Train step 200/274 - Loss: 0.433752 (MSE 0.030519, CE 1.039905, Corr 0.258537, Cons 0.022652, CSim 0.048471, Rank 0.588790) | LR: 2.606547e-05
2025-09-21 06:21:52,830 [INFO] Train step 250/274 - Loss: 0.431267 (MSE 0.030316, CE 1.033922, Corr 0.255657, Cons 0.022543, CSim 0.048216, Rank 0.588133) | LR: 1.934857e-05
2025-09-21 06:21:58,580 [INFO] Epoch training completed in 64.81s with average loss 0.431038
2025-09-21 06:21:58,581 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:22:00,061 [INFO] Fold 0 - Epoch 5 - Train Loss: 0.431038 | EMA Val MSE: 0.048912 | EMA Val CE: 1.310599 | EMA Val Pearson: 0.576422
2025-09-21 06:22:00,065 [INFO] Fold 0 - Epoch 5 - New best EMA model with Pearson 0.576422.
2025-09-21 06:22:00,065 [INFO] Fold 0 - Epoch 6/6 started.
2025-09-21 06:22:12,164 [INFO] Train step 50/274 - Loss: 0.414116 (MSE 0.028011, CE 0.993934, Corr 0.235218, Cons 0.021234, CSim 0.046985, Rank 0.584837) | LR: 1.109289e-05
2025-09-21 06:22:23,936 [INFO] Train step 100/274 - Loss: 0.415035 (MSE 0.028665, CE 0.994611, Corr 0.239205, Cons 0.021276, CSim 0.047350, Rank 0.584522) | LR: 6.743759e-06
2025-09-21 06:22:35,668 [INFO] Train step 150/274 - Loss: 0.414422 (MSE 0.028590, CE 0.993275, Corr 0.238408, Cons 0.021136, CSim 0.047164, Rank 0.583966) | LR: 3.444140e-06
2025-09-21 06:22:47,458 [INFO] Train step 200/274 - Loss: 0.414055 (MSE 0.028563, CE 0.992404, Corr 0.237631, Cons 0.021047, CSim 0.047111, Rank 0.584335) | LR: 1.231166e-06
2025-09-21 06:22:59,151 [INFO] Train step 250/274 - Loss: 0.415847 (MSE 0.028729, CE 0.996642, Corr 0.240104, Cons 0.020942, CSim 0.047002, Rank 0.584871) | LR: 1.297403e-07
2025-09-21 06:23:04,827 [INFO] Epoch training completed in 64.76s with average loss 0.415638
2025-09-21 06:23:04,828 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:23:06,317 [INFO] Fold 0 - Epoch 6 - Train Loss: 0.415638 | EMA Val MSE: 0.050586 | EMA Val CE: 1.322964 | EMA Val Pearson: 0.582197
2025-09-21 06:23:06,321 [INFO] Fold 0 - Epoch 6 - New best EMA model with Pearson 0.582197.
2025-09-21 06:23:07,563 [INFO] Fold 0 - Final Val Pearson (no TTA): 0.582197 | Final Val MSE: 0.050586 | Final Val CE: 1.322964
2025-09-21 06:23:08,705 [INFO] Fold 0 - Final Val Pearson (with TTA swap): 0.585011
2025-09-21 06:23:10,067 [INFO] ========== Fold 2/5 ==========
2025-09-21 06:23:10,135 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 06:23:10,150 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 06:23:10,150 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 06:23:10,150 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 06:23:10,150 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 06:23:10,150 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 06:23:10,152 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 06:23:10,302 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 06:23:10,302 [INFO] Initializing EMA state from current model weights.
2025-09-21 06:23:10,452 [INFO] Fold 1 - Epoch 1/6 started.
2025-09-21 06:23:22,652 [INFO] Train step 50/274 - Loss: 0.973882 (MSE 0.109858, CE 2.263349, Corr 0.970669, Cons 0.163055, CSim 0.114037, Rank 0.693446) | LR: 6.097561e-05
2025-09-21 06:23:34,488 [INFO] Train step 100/274 - Loss: 0.894039 (MSE 0.092405, CE 2.078004, Corr 0.923038, Cons 0.140920, CSim 0.095207, Rank 0.689862) | LR: 1.219512e-04
2025-09-21 06:23:46,287 [INFO] Train step 150/274 - Loss: 0.849995 (MSE 0.085045, CE 1.973169, Corr 0.893144, Cons 0.124932, CSim 0.086158, Rank 0.687074) | LR: 1.829268e-04
2025-09-21 06:23:58,056 [INFO] Train step 200/274 - Loss: 0.821895 (MSE 0.080674, CE 1.906344, Corr 0.871476, Cons 0.111972, CSim 0.081170, Rank 0.685766) | LR: 1.997082e-04
2025-09-21 06:24:09,905 [INFO] Train step 250/274 - Loss: 0.798792 (MSE 0.077889, CE 1.852540, Corr 0.842778, Cons 0.103268, CSim 0.078312, Rank 0.683212) | LR: 1.983384e-04
2025-09-21 06:24:15,645 [INFO] Epoch training completed in 65.19s with average loss 0.790075
2025-09-21 06:24:15,646 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:24:17,149 [INFO] Fold 1 - Epoch 1 - Train Loss: 0.790075 | EMA Val MSE: 0.070862 | EMA Val CE: 1.737355 | EMA Val Pearson: 0.182176
2025-09-21 06:24:17,153 [INFO] Fold 1 - Epoch 1 - New best EMA model with Pearson 0.182176.
2025-09-21 06:24:17,153 [INFO] Fold 1 - Epoch 2/6 started.
2025-09-21 06:24:29,364 [INFO] Train step 50/274 - Loss: 0.656645 (MSE 0.060885, CE 1.532849, Corr 0.616605, Cons 0.055008, CSim 0.064839, Rank 0.659812) | LR: 1.942877e-04
2025-09-21 06:24:41,094 [INFO] Train step 100/274 - Loss: 0.649815 (MSE 0.058921, CE 1.521380, Corr 0.598802, Cons 0.052597, CSim 0.063405, Rank 0.655979) | LR: 1.902280e-04
2025-09-21 06:24:52,757 [INFO] Train step 150/274 - Loss: 0.643457 (MSE 0.057688, CE 1.508900, Corr 0.584932, Cons 0.050503, CSim 0.062546, Rank 0.653635) | LR: 1.851529e-04
2025-09-21 06:25:04,560 [INFO] Train step 200/274 - Loss: 0.635884 (MSE 0.056817, CE 1.492825, Corr 0.569224, Cons 0.048829, CSim 0.061857, Rank 0.650773) | LR: 1.791195e-04
2025-09-21 06:25:16,285 [INFO] Train step 250/274 - Loss: 0.626431 (MSE 0.055781, CE 1.471711, Corr 0.553025, Cons 0.047872, CSim 0.061245, Rank 0.647827) | LR: 1.721956e-04
2025-09-21 06:25:21,943 [INFO] Epoch training completed in 64.79s with average loss 0.624024
2025-09-21 06:25:21,944 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:25:23,462 [INFO] Fold 1 - Epoch 2 - Train Loss: 0.624024 | EMA Val MSE: 0.059253 | EMA Val CE: 1.548634 | EMA Val Pearson: 0.383658
2025-09-21 06:25:23,465 [INFO] Fold 1 - Epoch 2 - New best EMA model with Pearson 0.383658.
2025-09-21 06:25:23,466 [INFO] Fold 1 - Epoch 3/6 started.
2025-09-21 06:25:35,416 [INFO] Train step 50/274 - Loss: 0.548319 (MSE 0.046338, CE 1.299498, Corr 0.417382, Cons 0.037245, CSim 0.054531, Rank 0.622940) | LR: 1.604825e-04
2025-09-21 06:25:47,085 [INFO] Train step 100/274 - Loss: 0.544464 (MSE 0.045247, CE 1.293651, Corr 0.404473, Cons 0.034056, CSim 0.054489, Rank 0.620742) | LR: 1.517058e-04
2025-09-21 06:25:58,771 [INFO] Train step 150/274 - Loss: 0.538371 (MSE 0.044370, CE 1.280151, Corr 0.395285, Cons 0.033105, CSim 0.054329, Rank 0.618276) | LR: 1.423473e-04
2025-09-21 06:26:10,612 [INFO] Train step 200/274 - Loss: 0.531898 (MSE 0.043696, CE 1.264762, Corr 0.387780, Cons 0.031764, CSim 0.053850, Rank 0.616964) | LR: 1.325122e-04
2025-09-21 06:26:22,413 [INFO] Train step 250/274 - Loss: 0.530876 (MSE 0.043228, CE 1.263471, Corr 0.385166, Cons 0.030673, CSim 0.053256, Rank 0.615932) | LR: 1.223112e-04
2025-09-21 06:26:28,161 [INFO] Epoch training completed in 64.70s with average loss 0.529732
2025-09-21 06:26:28,162 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:26:29,655 [INFO] Fold 1 - Epoch 3 - Train Loss: 0.529732 | EMA Val MSE: 0.052964 | EMA Val CE: 1.439308 | EMA Val Pearson: 0.487693
2025-09-21 06:26:29,659 [INFO] Fold 1 - Epoch 3 - New best EMA model with Pearson 0.487693.
2025-09-21 06:26:29,659 [INFO] Fold 1 - Epoch 4/6 started.
2025-09-21 06:26:41,960 [INFO] Train step 50/274 - Loss: 0.472051 (MSE 0.035714, CE 1.128544, Corr 0.306504, Cons 0.028456, CSim 0.048966, Rank 0.597604) | LR: 1.067874e-04
2025-09-21 06:26:53,685 [INFO] Train step 100/274 - Loss: 0.474419 (MSE 0.035449, CE 1.136315, Corr 0.304708, Cons 0.025795, CSim 0.048563, Rank 0.597822) | LR: 9.618007e-05
2025-09-21 06:27:05,551 [INFO] Train step 150/274 - Loss: 0.470771 (MSE 0.035442, CE 1.126478, Corr 0.302066, Cons 0.025242, CSim 0.049054, Rank 0.597749) | LR: 8.561573e-05
2025-09-21 06:27:17,317 [INFO] Train step 200/274 - Loss: 0.468551 (MSE 0.034853, CE 1.122266, Corr 0.297800, Cons 0.024540, CSim 0.048573, Rank 0.596916) | LR: 7.521326e-05
2025-09-21 06:27:29,113 [INFO] Train step 250/274 - Loss: 0.467061 (MSE 0.034653, CE 1.118535, Corr 0.297057, Cons 0.024039, CSim 0.048426, Rank 0.596969) | LR: 6.508975e-05
2025-09-21 06:27:34,765 [INFO] Epoch training completed in 65.11s with average loss 0.466121
2025-09-21 06:27:34,766 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:27:36,251 [INFO] Fold 1 - Epoch 4 - Train Loss: 0.466121 | EMA Val MSE: 0.049961 | EMA Val CE: 1.368049 | EMA Val Pearson: 0.541812
2025-09-21 06:27:36,265 [INFO] Fold 1 - Epoch 4 - New best EMA model with Pearson 0.541812.
2025-09-21 06:27:36,265 [INFO] Fold 1 - Epoch 5/6 started.
2025-09-21 06:27:48,459 [INFO] Train step 50/274 - Loss: 0.435478 (MSE 0.030212, CE 1.046303, Corr 0.256372, Cons 0.022324, CSim 0.045460, Rank 0.588518) | LR: 5.086032e-05
2025-09-21 06:28:00,184 [INFO] Train step 100/274 - Loss: 0.432491 (MSE 0.030273, CE 1.038463, Corr 0.253818, Cons 0.021327, CSim 0.045687, Rank 0.587252) | LR: 4.191050e-05
2025-09-21 06:28:11,961 [INFO] Train step 150/274 - Loss: 0.433107 (MSE 0.030315, CE 1.039803, Corr 0.255505, Cons 0.021368, CSim 0.045416, Rank 0.587279) | LR: 3.361443e-05
2025-09-21 06:28:23,682 [INFO] Train step 200/274 - Loss: 0.433017 (MSE 0.030565, CE 1.038508, Corr 0.257524, Cons 0.021486, CSim 0.045740, Rank 0.588207) | LR: 2.606547e-05
2025-09-21 06:28:35,566 [INFO] Train step 250/274 - Loss: 0.432177 (MSE 0.030370, CE 1.036892, Corr 0.255754, Cons 0.021391, CSim 0.045660, Rank 0.587798) | LR: 1.934857e-05
2025-09-21 06:28:41,287 [INFO] Epoch training completed in 65.02s with average loss 0.431988
2025-09-21 06:28:41,287 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:28:42,799 [INFO] Fold 1 - Epoch 5 - Train Loss: 0.431988 | EMA Val MSE: 0.049656 | EMA Val CE: 1.344853 | EMA Val Pearson: 0.567198
2025-09-21 06:28:42,803 [INFO] Fold 1 - Epoch 5 - New best EMA model with Pearson 0.567198.
2025-09-21 06:28:42,803 [INFO] Fold 1 - Epoch 6/6 started.
2025-09-21 06:28:54,934 [INFO] Train step 50/274 - Loss: 0.414500 (MSE 0.028443, CE 0.995016, Corr 0.235647, Cons 0.021231, CSim 0.044771, Rank 0.581713) | LR: 1.109289e-05
2025-09-21 06:29:06,595 [INFO] Train step 100/274 - Loss: 0.414440 (MSE 0.028570, CE 0.994656, Corr 0.235971, Cons 0.021064, CSim 0.044727, Rank 0.581239) | LR: 6.743759e-06
2025-09-21 06:29:18,389 [INFO] Train step 150/274 - Loss: 0.415538 (MSE 0.028448, CE 0.997709, Corr 0.236879, Cons 0.020686, CSim 0.044400, Rank 0.582190) | LR: 3.444140e-06
2025-09-21 06:29:30,163 [INFO] Train step 200/274 - Loss: 0.417016 (MSE 0.028643, CE 1.001102, Corr 0.238753, Cons 0.020763, CSim 0.044632, Rank 0.582596) | LR: 1.231166e-06
2025-09-21 06:29:41,845 [INFO] Train step 250/274 - Loss: 0.416053 (MSE 0.028569, CE 0.998419, Corr 0.238666, Cons 0.020731, CSim 0.044575, Rank 0.583134) | LR: 1.297403e-07
2025-09-21 06:29:47,682 [INFO] Epoch training completed in 64.88s with average loss 0.415678
2025-09-21 06:29:47,682 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:29:49,203 [INFO] Fold 1 - Epoch 6 - Train Loss: 0.415678 | EMA Val MSE: 0.050747 | EMA Val CE: 1.353971 | EMA Val Pearson: 0.576093
2025-09-21 06:29:49,207 [INFO] Fold 1 - Epoch 6 - New best EMA model with Pearson 0.576093.
2025-09-21 06:29:50,443 [INFO] Fold 1 - Final Val Pearson (no TTA): 0.576093 | Final Val MSE: 0.050747 | Final Val CE: 1.353971
2025-09-21 06:29:51,574 [INFO] Fold 1 - Final Val Pearson (with TTA swap): 0.581233
2025-09-21 06:29:52,921 [INFO] ========== Fold 3/5 ==========
2025-09-21 06:29:55,836 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 06:29:55,846 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 06:29:55,846 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 06:29:55,846 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 06:29:55,846 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 06:29:55,846 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 06:29:55,848 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 06:29:56,002 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 06:29:56,002 [INFO] Initializing EMA state from current model weights.
2025-09-21 06:29:56,148 [INFO] Fold 2 - Epoch 1/6 started.
2025-09-21 06:30:08,204 [INFO] Train step 50/274 - Loss: 0.989820 (MSE 0.091077, CE 2.336365, Corr 0.984344, Cons 0.136471, CSim 0.113325, Rank 0.697126) | LR: 6.097561e-05
2025-09-21 06:30:19,986 [INFO] Train step 100/274 - Loss: 0.900689 (MSE 0.083283, CE 2.108675, Corr 0.937479, Cons 0.124813, CSim 0.095264, Rank 0.691772) | LR: 1.219512e-04
2025-09-21 06:30:31,769 [INFO] Train step 150/274 - Loss: 0.851414 (MSE 0.078760, CE 1.986667, Corr 0.897008, Cons 0.115250, CSim 0.085802, Rank 0.688140) | LR: 1.829268e-04
2025-09-21 06:30:43,475 [INFO] Train step 200/274 - Loss: 0.820904 (MSE 0.076419, CE 1.911111, Corr 0.869319, Cons 0.106335, CSim 0.081018, Rank 0.685241) | LR: 1.997082e-04
2025-09-21 06:30:55,160 [INFO] Train step 250/274 - Loss: 0.798097 (MSE 0.074174, CE 1.856517, Corr 0.843586, Cons 0.099116, CSim 0.077759, Rank 0.683275) | LR: 1.983384e-04
2025-09-21 06:31:00,846 [INFO] Epoch training completed in 64.70s with average loss 0.788756
2025-09-21 06:31:00,847 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:31:02,316 [INFO] Fold 2 - Epoch 1 - Train Loss: 0.788756 | EMA Val MSE: 0.066528 | EMA Val CE: 1.734646 | EMA Val Pearson: 0.188299
2025-09-21 06:31:02,320 [INFO] Fold 2 - Epoch 1 - New best EMA model with Pearson 0.188299.
2025-09-21 06:31:02,320 [INFO] Fold 2 - Epoch 2/6 started.
2025-09-21 06:31:14,399 [INFO] Train step 50/274 - Loss: 0.669023 (MSE 0.060691, CE 1.561522, Corr 0.645672, Cons 0.058613, CSim 0.061827, Rank 0.663857) | LR: 1.942877e-04
2025-09-21 06:31:26,272 [INFO] Train step 100/274 - Loss: 0.657744 (MSE 0.059167, CE 1.538555, Corr 0.619069, Cons 0.056073, CSim 0.061347, Rank 0.659874) | LR: 1.902280e-04
2025-09-21 06:31:38,160 [INFO] Train step 150/274 - Loss: 0.648695 (MSE 0.058403, CE 1.519198, Corr 0.599306, Cons 0.052393, CSim 0.061269, Rank 0.655958) | LR: 1.851529e-04
2025-09-21 06:31:50,062 [INFO] Train step 200/274 - Loss: 0.639912 (MSE 0.057451, CE 1.499656, Corr 0.583670, Cons 0.050422, CSim 0.061047, Rank 0.653609) | LR: 1.791195e-04
2025-09-21 06:32:01,951 [INFO] Train step 250/274 - Loss: 0.631079 (MSE 0.056627, CE 1.479819, Corr 0.567980, Cons 0.048393, CSim 0.060900, Rank 0.650924) | LR: 1.721956e-04
2025-09-21 06:32:07,641 [INFO] Epoch training completed in 65.32s with average loss 0.626528
2025-09-21 06:32:07,642 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:32:09,142 [INFO] Fold 2 - Epoch 2 - Train Loss: 0.626528 | EMA Val MSE: 0.057402 | EMA Val CE: 1.533955 | EMA Val Pearson: 0.390838
2025-09-21 06:32:09,147 [INFO] Fold 2 - Epoch 2 - New best EMA model with Pearson 0.390838.
2025-09-21 06:32:09,147 [INFO] Fold 2 - Epoch 3/6 started.
2025-09-21 06:32:21,119 [INFO] Train step 50/274 - Loss: 0.553414 (MSE 0.045824, CE 1.313672, Corr 0.422722, Cons 0.033796, CSim 0.054717, Rank 0.623915) | LR: 1.604825e-04
2025-09-21 06:32:32,787 [INFO] Train step 100/274 - Loss: 0.551384 (MSE 0.045296, CE 1.311071, Corr 0.413985, Cons 0.032897, CSim 0.054350, Rank 0.622049) | LR: 1.517058e-04
2025-09-21 06:32:44,473 [INFO] Train step 150/274 - Loss: 0.542901 (MSE 0.044878, CE 1.289891, Corr 0.404964, Cons 0.032141, CSim 0.054391, Rank 0.619943) | LR: 1.423473e-04
2025-09-21 06:32:56,354 [INFO] Train step 200/274 - Loss: 0.539435 (MSE 0.044161, CE 1.282476, Corr 0.399769, Cons 0.031534, CSim 0.053994, Rank 0.619357) | LR: 1.325122e-04
2025-09-21 06:33:08,114 [INFO] Train step 250/274 - Loss: 0.533923 (MSE 0.043432, CE 1.270182, Corr 0.390918, Cons 0.030854, CSim 0.053871, Rank 0.617745) | LR: 1.223112e-04
2025-09-21 06:33:13,948 [INFO] Epoch training completed in 64.80s with average loss 0.531797
2025-09-21 06:33:13,949 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:33:15,419 [INFO] Fold 2 - Epoch 3 - Train Loss: 0.531797 | EMA Val MSE: 0.054023 | EMA Val CE: 1.432033 | EMA Val Pearson: 0.504761
2025-09-21 06:33:15,423 [INFO] Fold 2 - Epoch 3 - New best EMA model with Pearson 0.504761.
2025-09-21 06:33:15,423 [INFO] Fold 2 - Epoch 4/6 started.
2025-09-21 06:33:27,580 [INFO] Train step 50/274 - Loss: 0.476728 (MSE 0.035745, CE 1.142336, Corr 0.304943, Cons 0.024952, CSim 0.049650, Rank 0.597460) | LR: 1.067874e-04
2025-09-21 06:33:39,295 [INFO] Train step 100/274 - Loss: 0.473918 (MSE 0.035684, CE 1.134424, Corr 0.303572, Cons 0.025433, CSim 0.050085, Rank 0.598895) | LR: 9.618007e-05
2025-09-21 06:33:51,152 [INFO] Train step 150/274 - Loss: 0.475711 (MSE 0.035815, CE 1.138428, Corr 0.307316, Cons 0.025100, CSim 0.049596, Rank 0.599922) | LR: 8.561573e-05
2025-09-21 06:34:02,982 [INFO] Train step 200/274 - Loss: 0.474322 (MSE 0.035448, CE 1.135215, Corr 0.306714, Cons 0.024652, CSim 0.049135, Rank 0.600263) | LR: 7.521326e-05
2025-09-21 06:34:14,661 [INFO] Train step 250/274 - Loss: 0.469576 (MSE 0.035097, CE 1.123462, Corr 0.302102, Cons 0.024386, CSim 0.049055, Rank 0.599054) | LR: 6.508975e-05
2025-09-21 06:34:20,350 [INFO] Epoch training completed in 64.93s with average loss 0.467965
2025-09-21 06:34:20,352 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:34:21,849 [INFO] Fold 2 - Epoch 4 - Train Loss: 0.467965 | EMA Val MSE: 0.051540 | EMA Val CE: 1.360325 | EMA Val Pearson: 0.562629
2025-09-21 06:34:21,853 [INFO] Fold 2 - Epoch 4 - New best EMA model with Pearson 0.562629.
2025-09-21 06:34:21,853 [INFO] Fold 2 - Epoch 5/6 started.
2025-09-21 06:34:33,850 [INFO] Train step 50/274 - Loss: 0.431786 (MSE 0.030568, CE 1.035674, Corr 0.255475, Cons 0.020875, CSim 0.046873, Rank 0.585774) | LR: 5.086032e-05
2025-09-21 06:34:45,670 [INFO] Train step 100/274 - Loss: 0.433350 (MSE 0.030504, CE 1.039554, Corr 0.257335, Cons 0.021110, CSim 0.046612, Rank 0.587777) | LR: 4.191050e-05
2025-09-21 06:34:57,355 [INFO] Train step 150/274 - Loss: 0.430260 (MSE 0.030214, CE 1.032014, Corr 0.253772, Cons 0.021441, CSim 0.046676, Rank 0.587452) | LR: 3.361443e-05
2025-09-21 06:35:09,018 [INFO] Train step 200/274 - Loss: 0.429514 (MSE 0.030089, CE 1.030512, Corr 0.252370, Cons 0.021323, CSim 0.046514, Rank 0.586887) | LR: 2.606547e-05
2025-09-21 06:35:20,685 [INFO] Train step 250/274 - Loss: 0.429596 (MSE 0.029912, CE 1.031255, Corr 0.251412, Cons 0.021447, CSim 0.046471, Rank 0.586788) | LR: 1.934857e-05
2025-09-21 06:35:26,357 [INFO] Epoch training completed in 64.50s with average loss 0.429750
2025-09-21 06:35:26,358 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:35:27,871 [INFO] Fold 2 - Epoch 5 - Train Loss: 0.429750 | EMA Val MSE: 0.051481 | EMA Val CE: 1.337057 | EMA Val Pearson: 0.588280
2025-09-21 06:35:27,875 [INFO] Fold 2 - Epoch 5 - New best EMA model with Pearson 0.588280.
2025-09-21 06:35:27,875 [INFO] Fold 2 - Epoch 6/6 started.
2025-09-21 06:35:40,051 [INFO] Train step 50/274 - Loss: 0.413808 (MSE 0.028033, CE 0.994217, Corr 0.230773, Cons 0.021246, CSim 0.046696, Rank 0.583849) | LR: 1.109289e-05
2025-09-21 06:35:51,715 [INFO] Train step 100/274 - Loss: 0.413854 (MSE 0.028279, CE 0.993074, Corr 0.234522, Cons 0.021158, CSim 0.046346, Rank 0.584442) | LR: 6.743759e-06
2025-09-21 06:36:03,426 [INFO] Train step 150/274 - Loss: 0.413696 (MSE 0.028230, CE 0.992769, Corr 0.234422, Cons 0.021026, CSim 0.046112, Rank 0.584418) | LR: 3.444140e-06
2025-09-21 06:36:15,282 [INFO] Train step 200/274 - Loss: 0.414325 (MSE 0.028256, CE 0.994312, Corr 0.235383, Cons 0.020933, CSim 0.045868, Rank 0.584648) | LR: 1.231166e-06
2025-09-21 06:36:27,114 [INFO] Train step 250/274 - Loss: 0.413778 (MSE 0.028171, CE 0.993085, Corr 0.234777, Cons 0.020869, CSim 0.045792, Rank 0.584304) | LR: 1.297403e-07
2025-09-21 06:36:32,867 [INFO] Epoch training completed in 64.99s with average loss 0.413587
2025-09-21 06:36:32,868 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:36:34,358 [INFO] Fold 2 - Epoch 6 - Train Loss: 0.413587 | EMA Val MSE: 0.052255 | EMA Val CE: 1.345313 | EMA Val Pearson: 0.598456
2025-09-21 06:36:34,362 [INFO] Fold 2 - Epoch 6 - New best EMA model with Pearson 0.598456.
2025-09-21 06:36:35,593 [INFO] Fold 2 - Final Val Pearson (no TTA): 0.598456 | Final Val MSE: 0.052255 | Final Val CE: 1.345313
2025-09-21 06:36:36,718 [INFO] Fold 2 - Final Val Pearson (with TTA swap): 0.604400
2025-09-21 06:36:38,027 [INFO] ========== Fold 4/5 ==========
2025-09-21 06:36:41,429 [INFO] Applied swap augmentation. New train size: 52522
2025-09-21 06:36:41,439 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 06:36:41,439 [INFO] Train split: 52522 samples; Val split: 6564 samples.
2025-09-21 06:36:41,439 [INFO] Initializing PatentDataset with 52522 samples.
2025-09-21 06:36:41,439 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 06:36:41,439 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 06:36:41,442 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 06:36:41,594 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 06:36:41,595 [INFO] Initializing EMA state from current model weights.
2025-09-21 06:36:41,743 [INFO] Fold 3 - Epoch 1/6 started.
2025-09-21 06:36:53,963 [INFO] Train step 50/274 - Loss: 0.972772 (MSE 0.103133, CE 2.269861, Corr 0.968484, Cons 0.147268, CSim 0.128433, Rank 0.695032) | LR: 6.097561e-05
2025-09-21 06:37:05,816 [INFO] Train step 100/274 - Loss: 0.897809 (MSE 0.089043, CE 2.093445, Corr 0.921108, Cons 0.135340, CSim 0.105056, Rank 0.689620) | LR: 1.219512e-04
2025-09-21 06:37:17,549 [INFO] Train step 150/274 - Loss: 0.853631 (MSE 0.082868, CE 1.984862, Corr 0.897887, Cons 0.124712, CSim 0.092997, Rank 0.687530) | LR: 1.829268e-04
2025-09-21 06:37:29,219 [INFO] Train step 200/274 - Loss: 0.824280 (MSE 0.079199, CE 1.914287, Corr 0.872440, Cons 0.116330, CSim 0.086479, Rank 0.685479) | LR: 1.997082e-04
2025-09-21 06:37:40,959 [INFO] Train step 250/274 - Loss: 0.802025 (MSE 0.076257, CE 1.862497, Corr 0.847861, Cons 0.105014, CSim 0.081936, Rank 0.683673) | LR: 1.983384e-04
2025-09-21 06:37:46,668 [INFO] Epoch training completed in 64.93s with average loss 0.792877
2025-09-21 06:37:46,670 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:37:48,156 [INFO] Fold 3 - Epoch 1 - Train Loss: 0.792877 | EMA Val MSE: 0.074926 | EMA Val CE: 1.761911 | EMA Val Pearson: 0.182910
2025-09-21 06:37:48,160 [INFO] Fold 3 - Epoch 1 - New best EMA model with Pearson 0.182910.
2025-09-21 06:37:48,160 [INFO] Fold 3 - Epoch 2/6 started.
2025-09-21 06:38:00,191 [INFO] Train step 50/274 - Loss: 0.661425 (MSE 0.058918, CE 1.547652, Corr 0.621693, Cons 0.059950, CSim 0.063439, Rank 0.664716) | LR: 1.942877e-04
2025-09-21 06:38:11,978 [INFO] Train step 100/274 - Loss: 0.653176 (MSE 0.058168, CE 1.527278, Corr 0.615897, Cons 0.056044, CSim 0.062667, Rank 0.662203) | LR: 1.902280e-04
2025-09-21 06:38:23,685 [INFO] Train step 150/274 - Loss: 0.646438 (MSE 0.057400, CE 1.513649, Corr 0.598941, Cons 0.054176, CSim 0.062426, Rank 0.659404) | LR: 1.851529e-04
2025-09-21 06:38:35,356 [INFO] Train step 200/274 - Loss: 0.636844 (MSE 0.056422, CE 1.492612, Corr 0.580436, Cons 0.052667, CSim 0.062007, Rank 0.656177) | LR: 1.791195e-04
2025-09-21 06:38:47,016 [INFO] Train step 250/274 - Loss: 0.628614 (MSE 0.055376, CE 1.475695, Corr 0.561958, Cons 0.049596, CSim 0.061412, Rank 0.652904) | LR: 1.721956e-04
2025-09-21 06:38:52,664 [INFO] Epoch training completed in 64.50s with average loss 0.625561
2025-09-21 06:38:52,665 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:38:54,164 [INFO] Fold 3 - Epoch 2 - Train Loss: 0.625561 | EMA Val MSE: 0.062579 | EMA Val CE: 1.542026 | EMA Val Pearson: 0.361303
2025-09-21 06:38:54,169 [INFO] Fold 3 - Epoch 2 - New best EMA model with Pearson 0.361303.
2025-09-21 06:38:54,169 [INFO] Fold 3 - Epoch 3/6 started.
2025-09-21 06:39:06,117 [INFO] Train step 50/274 - Loss: 0.547933 (MSE 0.043739, CE 1.305943, Corr 0.403513, Cons 0.035206, CSim 0.054318, Rank 0.621916) | LR: 1.604825e-04
2025-09-21 06:39:17,947 [INFO] Train step 100/274 - Loss: 0.546170 (MSE 0.043883, CE 1.301315, Corr 0.402092, Cons 0.032040, CSim 0.054303, Rank 0.621017) | LR: 1.517058e-04
2025-09-21 06:39:29,716 [INFO] Train step 150/274 - Loss: 0.541494 (MSE 0.043627, CE 1.289442, Corr 0.397573, Cons 0.032289, CSim 0.054236, Rank 0.620617) | LR: 1.423473e-04
2025-09-21 06:39:41,409 [INFO] Train step 200/274 - Loss: 0.536512 (MSE 0.042757, CE 1.278993, Corr 0.388512, Cons 0.031822, CSim 0.053811, Rank 0.618806) | LR: 1.325122e-04
2025-09-21 06:39:53,117 [INFO] Train step 250/274 - Loss: 0.531455 (MSE 0.042452, CE 1.266726, Corr 0.382474, Cons 0.030748, CSim 0.053672, Rank 0.617115) | LR: 1.223112e-04
2025-09-21 06:39:58,869 [INFO] Epoch training completed in 64.70s with average loss 0.530219
2025-09-21 06:39:58,870 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:40:00,354 [INFO] Fold 3 - Epoch 3 - Train Loss: 0.530219 | EMA Val MSE: 0.055534 | EMA Val CE: 1.426142 | EMA Val Pearson: 0.485625
2025-09-21 06:40:00,358 [INFO] Fold 3 - Epoch 3 - New best EMA model with Pearson 0.485625.
2025-09-21 06:40:00,358 [INFO] Fold 3 - Epoch 4/6 started.
2025-09-21 06:40:12,595 [INFO] Train step 50/274 - Loss: 0.474601 (MSE 0.034946, CE 1.137596, Corr 0.303453, Cons 0.025506, CSim 0.048924, Rank 0.599800) | LR: 1.067874e-04
2025-09-21 06:40:24,355 [INFO] Train step 100/274 - Loss: 0.475068 (MSE 0.035273, CE 1.138385, Corr 0.303314, Cons 0.025143, CSim 0.049310, Rank 0.600004) | LR: 9.618007e-05
2025-09-21 06:40:36,190 [INFO] Train step 150/274 - Loss: 0.471733 (MSE 0.034861, CE 1.130593, Corr 0.299511, Cons 0.024922, CSim 0.049023, Rank 0.598830) | LR: 8.561573e-05
2025-09-21 06:40:47,867 [INFO] Train step 200/274 - Loss: 0.469865 (MSE 0.034782, CE 1.125944, Corr 0.297428, Cons 0.024450, CSim 0.049141, Rank 0.598282) | LR: 7.521326e-05
2025-09-21 06:40:59,567 [INFO] Train step 250/274 - Loss: 0.467943 (MSE 0.034502, CE 1.121469, Corr 0.295449, Cons 0.024272, CSim 0.048867, Rank 0.597765) | LR: 6.508975e-05
2025-09-21 06:41:05,368 [INFO] Epoch training completed in 65.01s with average loss 0.467198
2025-09-21 06:41:05,368 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:41:06,863 [INFO] Fold 3 - Epoch 4 - Train Loss: 0.467198 | EMA Val MSE: 0.051670 | EMA Val CE: 1.353585 | EMA Val Pearson: 0.545483
2025-09-21 06:41:06,867 [INFO] Fold 3 - Epoch 4 - New best EMA model with Pearson 0.545483.
2025-09-21 06:41:06,867 [INFO] Fold 3 - Epoch 5/6 started.
2025-09-21 06:41:18,816 [INFO] Train step 50/274 - Loss: 0.435304 (MSE 0.031143, CE 1.041980, Corr 0.264627, Cons 0.021683, CSim 0.046391, Rank 0.591189) | LR: 5.086032e-05
2025-09-21 06:41:30,490 [INFO] Train step 100/274 - Loss: 0.436299 (MSE 0.030990, CE 1.045549, Corr 0.263277, Cons 0.021806, CSim 0.046196, Rank 0.590088) | LR: 4.191050e-05
2025-09-21 06:41:42,208 [INFO] Train step 150/274 - Loss: 0.436949 (MSE 0.030780, CE 1.047731, Corr 0.263145, Cons 0.021780, CSim 0.045948, Rank 0.590590) | LR: 3.361443e-05
2025-09-21 06:41:54,015 [INFO] Train step 200/274 - Loss: 0.434231 (MSE 0.030691, CE 1.040897, Corr 0.259799, Cons 0.021923, CSim 0.046301, Rank 0.589990) | LR: 2.606547e-05
2025-09-21 06:42:05,955 [INFO] Train step 250/274 - Loss: 0.434971 (MSE 0.030671, CE 1.042931, Corr 0.260179, Cons 0.021789, CSim 0.046169, Rank 0.590346) | LR: 1.934857e-05
2025-09-21 06:42:11,670 [INFO] Epoch training completed in 64.80s with average loss 0.433546
2025-09-21 06:42:11,671 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:42:13,172 [INFO] Fold 3 - Epoch 5 - Train Loss: 0.433546 | EMA Val MSE: 0.050811 | EMA Val CE: 1.329746 | EMA Val Pearson: 0.570927
2025-09-21 06:42:13,176 [INFO] Fold 3 - Epoch 5 - New best EMA model with Pearson 0.570927.
2025-09-21 06:42:13,177 [INFO] Fold 3 - Epoch 6/6 started.
2025-09-21 06:42:25,221 [INFO] Train step 50/274 - Loss: 0.415153 (MSE 0.028867, CE 0.993765, Corr 0.242198, Cons 0.021861, CSim 0.045585, Rank 0.587318) | LR: 1.109289e-05
2025-09-21 06:42:37,080 [INFO] Train step 100/274 - Loss: 0.415841 (MSE 0.029047, CE 0.995879, Corr 0.241359, Cons 0.021481, CSim 0.045587, Rank 0.585808) | LR: 6.743759e-06
2025-09-21 06:42:48,980 [INFO] Train step 150/274 - Loss: 0.418616 (MSE 0.028975, CE 1.003737, Corr 0.242334, Cons 0.021383, CSim 0.045319, Rank 0.585825) | LR: 3.444140e-06
2025-09-21 06:43:00,703 [INFO] Train step 200/274 - Loss: 0.419021 (MSE 0.028927, CE 1.004972, Corr 0.242160, Cons 0.021299, CSim 0.045233, Rank 0.586221) | LR: 1.231166e-06
2025-09-21 06:43:12,493 [INFO] Train step 250/274 - Loss: 0.418436 (MSE 0.028803, CE 1.003760, Corr 0.241496, Cons 0.021290, CSim 0.044942, Rank 0.585716) | LR: 1.297403e-07
2025-09-21 06:43:18,207 [INFO] Epoch training completed in 65.03s with average loss 0.418574
2025-09-21 06:43:18,208 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:43:19,726 [INFO] Fold 3 - Epoch 6 - Train Loss: 0.418574 | EMA Val MSE: 0.051690 | EMA Val CE: 1.336896 | EMA Val Pearson: 0.581066
2025-09-21 06:43:19,730 [INFO] Fold 3 - Epoch 6 - New best EMA model with Pearson 0.581066.
2025-09-21 06:43:20,943 [INFO] Fold 3 - Final Val Pearson (no TTA): 0.581066 | Final Val MSE: 0.051690 | Final Val CE: 1.336896
2025-09-21 06:43:22,044 [INFO] Fold 3 - Final Val Pearson (with TTA swap): 0.585572
2025-09-21 06:43:23,375 [INFO] ========== Fold 5/5 ==========
2025-09-21 06:43:24,245 [INFO] Applied swap augmentation. New train size: 52526
2025-09-21 06:43:24,257 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 06:43:24,257 [INFO] Train split: 52526 samples; Val split: 6562 samples.
2025-09-21 06:43:24,257 [INFO] Initializing PatentDataset with 52526 samples.
2025-09-21 06:43:24,257 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 06:43:24,257 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 06:43:24,259 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=224, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 06:43:24,411 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 06:43:24,411 [INFO] Initializing EMA state from current model weights.
2025-09-21 06:43:24,557 [INFO] Fold 4 - Epoch 1/6 started.
2025-09-21 06:43:36,731 [INFO] Train step 50/274 - Loss: 1.000293 (MSE 0.096563, CE 2.357981, Corr 0.967093, Cons 0.150822, CSim 0.134040, Rank 0.696084) | LR: 6.097561e-05
2025-09-21 06:43:48,550 [INFO] Train step 100/274 - Loss: 0.909225 (MSE 0.086766, CE 2.126732, Corr 0.930099, Cons 0.139708, CSim 0.108009, Rank 0.690891) | LR: 1.219512e-04
2025-09-21 06:44:00,389 [INFO] Train step 150/274 - Loss: 0.857387 (MSE 0.081430, CE 1.999271, Corr 0.890754, Cons 0.125215, CSim 0.094634, Rank 0.687198) | LR: 1.829268e-04
2025-09-21 06:44:12,175 [INFO] Train step 200/274 - Loss: 0.823383 (MSE 0.077738, CE 1.917436, Corr 0.858028, Cons 0.114394, CSim 0.087287, Rank 0.684609) | LR: 1.997082e-04
2025-09-21 06:44:24,011 [INFO] Train step 250/274 - Loss: 0.799714 (MSE 0.075319, CE 1.860569, Corr 0.834135, Cons 0.104497, CSim 0.082985, Rank 0.682390) | LR: 1.983384e-04
2025-09-21 06:44:29,706 [INFO] Epoch training completed in 65.15s with average loss 0.790017
2025-09-21 06:44:29,707 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:44:31,188 [INFO] Fold 4 - Epoch 1 - Train Loss: 0.790017 | EMA Val MSE: 0.074574 | EMA Val CE: 1.817123 | EMA Val Pearson: 0.202430
2025-09-21 06:44:31,192 [INFO] Fold 4 - Epoch 1 - New best EMA model with Pearson 0.202430.
2025-09-21 06:44:31,192 [INFO] Fold 4 - Epoch 2/6 started.
2025-09-21 06:44:43,410 [INFO] Train step 50/274 - Loss: 0.647130 (MSE 0.057124, CE 1.516014, Corr 0.600925, Cons 0.050970, CSim 0.061957, Rank 0.658306) | LR: 1.942877e-04
2025-09-21 06:44:55,213 [INFO] Train step 100/274 - Loss: 0.641432 (MSE 0.056591, CE 1.503769, Corr 0.588660, Cons 0.049118, CSim 0.061898, Rank 0.656345) | LR: 1.902280e-04
2025-09-21 06:45:06,974 [INFO] Train step 150/274 - Loss: 0.630409 (MSE 0.056004, CE 1.478590, Corr 0.568158, Cons 0.045687, CSim 0.062061, Rank 0.652611) | LR: 1.851529e-04
2025-09-21 06:45:18,756 [INFO] Train step 200/274 - Loss: 0.625003 (MSE 0.055333, CE 1.467113, Corr 0.557283, Cons 0.044228, CSim 0.061591, Rank 0.650674) | LR: 1.791195e-04
2025-09-21 06:45:30,518 [INFO] Train step 250/274 - Loss: 0.618066 (MSE 0.054618, CE 1.452134, Corr 0.542712, Cons 0.043562, CSim 0.061515, Rank 0.648306) | LR: 1.721956e-04
2025-09-21 06:45:36,185 [INFO] Epoch training completed in 64.99s with average loss 0.614931
2025-09-21 06:45:36,186 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:45:37,660 [INFO] Fold 4 - Epoch 2 - Train Loss: 0.614931 | EMA Val MSE: 0.058978 | EMA Val CE: 1.508713 | EMA Val Pearson: 0.411790
2025-09-21 06:45:37,664 [INFO] Fold 4 - Epoch 2 - New best EMA model with Pearson 0.411790.
2025-09-21 06:45:37,664 [INFO] Fold 4 - Epoch 3/6 started.
2025-09-21 06:45:49,720 [INFO] Train step 50/274 - Loss: 0.541957 (MSE 0.044242, CE 1.288508, Corr 0.401691, Cons 0.032076, CSim 0.055380, Rank 0.622003) | LR: 1.604825e-04
2025-09-21 06:46:01,451 [INFO] Train step 100/274 - Loss: 0.539601 (MSE 0.044001, CE 1.283453, Corr 0.397267, Cons 0.031564, CSim 0.055375, Rank 0.620216) | LR: 1.517058e-04
2025-09-21 06:46:13,117 [INFO] Train step 150/274 - Loss: 0.537718 (MSE 0.043902, CE 1.278817, Corr 0.395211, Cons 0.031457, CSim 0.055175, Rank 0.619626) | LR: 1.423473e-04
2025-09-21 06:46:24,780 [INFO] Train step 200/274 - Loss: 0.533636 (MSE 0.043386, CE 1.269586, Corr 0.389392, Cons 0.030547, CSim 0.054942, Rank 0.618184) | LR: 1.325122e-04
2025-09-21 06:46:36,456 [INFO] Train step 250/274 - Loss: 0.530291 (MSE 0.043084, CE 1.261402, Corr 0.386058, Cons 0.029888, CSim 0.054946, Rank 0.617501) | LR: 1.223112e-04
2025-09-21 06:46:42,169 [INFO] Epoch training completed in 64.50s with average loss 0.528241
2025-09-21 06:46:42,169 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:46:43,678 [INFO] Fold 4 - Epoch 3 - Train Loss: 0.528241 | EMA Val MSE: 0.054216 | EMA Val CE: 1.400805 | EMA Val Pearson: 0.505906
2025-09-21 06:46:43,682 [INFO] Fold 4 - Epoch 3 - New best EMA model with Pearson 0.505906.
2025-09-21 06:46:43,682 [INFO] Fold 4 - Epoch 4/6 started.
2025-09-21 06:46:55,685 [INFO] Train step 50/274 - Loss: 0.471913 (MSE 0.035927, CE 1.127961, Corr 0.303936, Cons 0.026147, CSim 0.051619, Rank 0.598967) | LR: 1.067874e-04
2025-09-21 06:47:07,356 [INFO] Train step 100/274 - Loss: 0.469536 (MSE 0.035454, CE 1.122619, Corr 0.301807, Cons 0.026033, CSim 0.050886, Rank 0.598190) | LR: 9.618007e-05
2025-09-21 06:47:19,206 [INFO] Train step 150/274 - Loss: 0.469218 (MSE 0.035350, CE 1.121766, Corr 0.302203, Cons 0.025377, CSim 0.050807, Rank 0.598678) | LR: 8.561573e-05
2025-09-21 06:47:30,886 [INFO] Train step 200/274 - Loss: 0.468031 (MSE 0.035029, CE 1.119445, Corr 0.299859, Cons 0.025186, CSim 0.050694, Rank 0.598655) | LR: 7.521326e-05
2025-09-21 06:47:42,555 [INFO] Train step 250/274 - Loss: 0.466743 (MSE 0.034871, CE 1.116353, Corr 0.298796, Cons 0.025097, CSim 0.050549, Rank 0.598157) | LR: 6.508975e-05
2025-09-21 06:47:48,241 [INFO] Epoch training completed in 64.56s with average loss 0.466725
2025-09-21 06:47:48,242 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:47:49,709 [INFO] Fold 4 - Epoch 4 - Train Loss: 0.466725 | EMA Val MSE: 0.051642 | EMA Val CE: 1.339730 | EMA Val Pearson: 0.554163
2025-09-21 06:47:49,714 [INFO] Fold 4 - Epoch 4 - New best EMA model with Pearson 0.554163.
2025-09-21 06:47:49,714 [INFO] Fold 4 - Epoch 5/6 started.
2025-09-21 06:48:01,856 [INFO] Train step 50/274 - Loss: 0.425752 (MSE 0.031100, CE 1.017039, Corr 0.254898, Cons 0.023883, CSim 0.049123, Rank 0.587165) | LR: 5.086032e-05
2025-09-21 06:48:13,661 [INFO] Train step 100/274 - Loss: 0.430475 (MSE 0.031165, CE 1.028943, Corr 0.261325, Cons 0.023423, CSim 0.048465, Rank 0.588125) | LR: 4.191050e-05
2025-09-21 06:48:25,520 [INFO] Train step 150/274 - Loss: 0.432370 (MSE 0.030834, CE 1.035526, Corr 0.258804, Cons 0.022957, CSim 0.048298, Rank 0.587985) | LR: 3.361443e-05
2025-09-21 06:48:37,345 [INFO] Train step 200/274 - Loss: 0.429781 (MSE 0.030688, CE 1.028999, Corr 0.256342, Cons 0.022457, CSim 0.048152, Rank 0.587768) | LR: 2.606547e-05
2025-09-21 06:48:49,120 [INFO] Train step 250/274 - Loss: 0.429582 (MSE 0.030621, CE 1.028404, Corr 0.256453, Cons 0.022260, CSim 0.048197, Rank 0.588553) | LR: 1.934857e-05
2025-09-21 06:48:54,846 [INFO] Epoch training completed in 65.13s with average loss 0.430359
2025-09-21 06:48:54,847 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:48:56,352 [INFO] Fold 4 - Epoch 5 - Train Loss: 0.430359 | EMA Val MSE: 0.051315 | EMA Val CE: 1.323279 | EMA Val Pearson: 0.576312
2025-09-21 06:48:56,356 [INFO] Fold 4 - Epoch 5 - New best EMA model with Pearson 0.576312.
2025-09-21 06:48:56,356 [INFO] Fold 4 - Epoch 6/6 started.
2025-09-21 06:49:08,423 [INFO] Train step 50/274 - Loss: 0.416452 (MSE 0.028730, CE 0.998517, Corr 0.239725, Cons 0.021083, CSim 0.047601, Rank 0.583793) | LR: 1.109289e-05
2025-09-21 06:49:20,161 [INFO] Train step 100/274 - Loss: 0.418235 (MSE 0.028944, CE 1.002279, Corr 0.243487, Cons 0.021298, CSim 0.047531, Rank 0.584729) | LR: 6.743759e-06
2025-09-21 06:49:31,917 [INFO] Train step 150/274 - Loss: 0.416888 (MSE 0.028797, CE 0.999094, Corr 0.241303, Cons 0.021515, CSim 0.047724, Rank 0.584927) | LR: 3.444140e-06
2025-09-21 06:49:43,915 [INFO] Train step 200/274 - Loss: 0.416951 (MSE 0.028722, CE 0.999216, Corr 0.241929, Cons 0.021523, CSim 0.047418, Rank 0.585450) | LR: 1.231166e-06
2025-09-21 06:49:55,697 [INFO] Train step 250/274 - Loss: 0.416323 (MSE 0.028652, CE 0.997973, Corr 0.240577, Cons 0.021559, CSim 0.047355, Rank 0.584582) | LR: 1.297403e-07
2025-09-21 06:50:01,401 [INFO] Epoch training completed in 65.04s with average loss 0.415508
2025-09-21 06:50:01,401 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 06:50:02,905 [INFO] Fold 4 - Epoch 6 - Train Loss: 0.415508 | EMA Val MSE: 0.052024 | EMA Val CE: 1.334439 | EMA Val Pearson: 0.584888
2025-09-21 06:50:02,910 [INFO] Fold 4 - Epoch 6 - New best EMA model with Pearson 0.584888.
2025-09-21 06:50:04,154 [INFO] Fold 4 - Final Val Pearson (no TTA): 0.584888 | Final Val MSE: 0.052024 | Final Val CE: 1.334439
2025-09-21 06:50:05,300 [INFO] Fold 4 - Final Val Pearson (with TTA swap): 0.592866
2025-09-21 06:50:06,648 [INFO] OOF Pearson correlation across all folds (pre-rule, pre-calibration): 0.589346
2025-09-21 06:50:06,648 [INFO] OOF MSE across all folds (pre-rule, pre-calibration): 0.050547
2025-09-21 06:50:06,648 [INFO] Ensembling test predictions from all folds by averaging.
2025-09-21 06:50:06,649 [INFO] Applying rule-based post-processing for anchor == target -> score = 1.0
2025-09-21 06:50:06,650 [INFO] Found 24 exact anchor==target pairs in test set.
2025-09-21 06:50:06,653 [INFO] Found 255 exact anchor==target pairs in train (OOF) set; applying same rule to OOF predictions.
2025-09-21 06:50:06,654 [INFO] OOF Pearson after rule (pre-calibration): 0.590599
2025-09-21 06:50:06,654 [INFO] OOF MSE after rule (pre-calibration): 0.050528
2025-09-21 06:50:06,654 [INFO] Fitting isotonic regression (PAV) calibrator on OOF predictions.
2025-09-21 06:50:06,696 [INFO] Isotonic produced 372 monotonic blocks.
2025-09-21 06:50:06,698 [INFO] OOF Pearson after isotonic calibration: 0.601614
2025-09-21 06:50:06,698 [INFO] OOF MSE after isotonic calibration: 0.042752
2025-09-21 06:50:06,698 [INFO] Logging final validation results (OOF, post-rule, post-calibration):
2025-09-21 06:50:06,698 [INFO] Final Validation Pearson: 0.601614 | Final Validation MSE: 0.042752
2025-09-21 06:50:06,701 [INFO] Writing submission to task/us-patent-phrase-to-phrase-matching/outputs/1/submission_7.csv
2025-09-21 06:50:06,709 [INFO] Submission file created successfully.
2025-09-21 06:50:06,709 [INFO] All done (v7).
