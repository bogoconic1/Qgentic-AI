2025-09-21 07:32:54,271 [INFO] Initialized logging and created output directories for v9 pipeline.
2025-09-21 07:32:54,271 [INFO] Setting all random seeds to 42.
2025-09-21 07:32:54,271 [INFO] Reading train data from task/us-patent-phrase-to-phrase-matching/train.csv.
2025-09-21 07:32:54,309 [INFO] Reading test data from task/us-patent-phrase-to-phrase-matching/test.csv.
2025-09-21 07:32:54,315 [INFO] Train shape: (32825, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 07:32:54,315 [INFO] Test shape:  (3648, 5), columns: ['id', 'anchor', 'target', 'context', 'score']
2025-09-21 07:32:54,320 [INFO] Train missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 07:32:54,320 [INFO] Test missing values by column: {'id': 0, 'anchor': 0, 'target': 0, 'context': 0, 'score': 0}
2025-09-21 07:32:54,320 [INFO] Building vocabulary from segmented corpus.
2025-09-21 07:32:54,668 [INFO] Built vocabulary of size 9042.
2025-09-21 07:32:54,668 [INFO] Tokenizing and numericalizing dataframe with 32825 rows (with token types).
2025-09-21 07:32:54,791 [INFO] Processed 5000 rows.
2025-09-21 07:32:54,910 [INFO] Processed 10000 rows.
2025-09-21 07:32:55,030 [INFO] Processed 15000 rows.
2025-09-21 07:32:55,150 [INFO] Processed 20000 rows.
2025-09-21 07:32:55,270 [INFO] Processed 25000 rows.
2025-09-21 07:32:55,390 [INFO] Processed 30000 rows.
2025-09-21 07:32:55,458 [INFO] Tokenizing and numericalizing dataframe with 3648 rows (with token types).
2025-09-21 07:32:55,546 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 3648 rows (anchor<->target).
2025-09-21 07:32:55,633 [INFO] Prepared swapped TTA arrays for test data.
2025-09-21 07:32:55,633 [INFO] Tokenizing and numericalizing SWAPPED dataframe with 32825 rows (anchor<->target).
2025-09-21 07:32:55,754 [INFO] Processed 5000 swapped rows.
2025-09-21 07:32:55,880 [INFO] Processed 10000 swapped rows.
2025-09-21 07:32:56,008 [INFO] Processed 15000 swapped rows.
2025-09-21 07:32:56,147 [INFO] Processed 20000 swapped rows.
2025-09-21 07:32:56,283 [INFO] Processed 25000 swapped rows.
2025-09-21 07:32:56,417 [INFO] Processed 30000 swapped rows.
2025-09-21 07:32:56,485 [INFO] Prepared swapped augmentation arrays for training.
2025-09-21 07:32:56,486 [INFO] Computing handcrafted similarity features.
2025-09-21 07:32:56,920 [INFO] Computed handcrafted features for 5000 rows.
2025-09-21 07:32:57,346 [INFO] Computed handcrafted features for 10000 rows.
2025-09-21 07:32:57,781 [INFO] Computed handcrafted features for 15000 rows.
2025-09-21 07:32:58,212 [INFO] Computed handcrafted features for 20000 rows.
2025-09-21 07:32:58,643 [INFO] Computed handcrafted features for 25000 rows.
2025-09-21 07:32:59,071 [INFO] Computed handcrafted features for 30000 rows.
2025-09-21 07:32:59,319 [INFO] Finished computing handcrafted features.
2025-09-21 07:32:59,319 [INFO] Computing handcrafted similarity features.
2025-09-21 07:32:59,633 [INFO] Finished computing handcrafted features.
2025-09-21 07:32:59,633 [INFO] Handcrafted feature dimension: 8
2025-09-21 07:32:59,634 [INFO] Labels statistics: min=0.0, max=1.0, mean=0.3619, std=0.2588
2025-09-21 07:32:59,638 [INFO] Found 106 unique context codes across train+test.
2025-09-21 07:32:59,895 [INFO] Class counts: [6774.0, 10306.0, 11068.0, 3634.0, 1043.0] | CE weights: [0.15397107601165771, 0.10120318084955215, 0.0942356288433075, 0.2870115637779236, 1.0]
2025-09-21 07:32:59,897 [INFO] Creating stratified folds on 5-class bins.
2025-09-21 07:32:59,912 [INFO] Fold 0: 6567 samples.
2025-09-21 07:32:59,912 [INFO] Fold 1: 6566 samples.
2025-09-21 07:32:59,912 [INFO] Fold 2: 6566 samples.
2025-09-21 07:32:59,912 [INFO] Fold 3: 6564 samples.
2025-09-21 07:32:59,912 [INFO] Fold 4: 6562 samples.
2025-09-21 07:32:59,913 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 07:32:59,913 [INFO] Initializing PatentDataset with 3648 samples.
2025-09-21 07:32:59,913 [INFO] ========== Fold 1/5 ==========
2025-09-21 07:33:05,024 [INFO] Applied swap augmentation. New train size: 52516
2025-09-21 07:33:05,036 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 07:33:05,036 [INFO] Train split: 52516 samples; Val split: 6567 samples.
2025-09-21 07:33:05,036 [INFO] Initializing PatentDataset with 52516 samples.
2025-09-21 07:33:05,036 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 07:33:05,036 [INFO] Initializing PatentDataset with 6567 samples.
2025-09-21 07:33:05,036 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 07:33:06,041 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 07:33:06,042 [INFO] Initializing EMA state from current model weights.
2025-09-21 07:33:06,203 [INFO] Fold 0 - Epoch 1/6 started.
2025-09-21 07:33:20,662 [INFO] Train step 50/274 - Loss: 0.337415 (wMSE 0.138789, SoftCE 0.411808, Corr 0.973554, Cons 0.224662, CSim 0.128807, Rank 0.696016, RDropCLS 0.652584, RDropREG 0.037489, GateReg 0.098658) | LR: 6.097561e-05
2025-09-21 07:33:34,008 [INFO] Train step 100/274 - Loss: 0.306111 (wMSE 0.122562, SoftCE 0.371615, Corr 0.912255, Cons 0.197638, CSim 0.101776, Rank 0.688792, RDropCLS 0.501409, RDropREG 0.029587, GateReg 0.086371) | LR: 1.219512e-04
2025-09-21 07:33:47,345 [INFO] Train step 150/274 - Loss: 0.289023 (wMSE 0.113946, SoftCE 0.351993, Corr 0.873174, Cons 0.175655, CSim 0.090383, Rank 0.684646, RDropCLS 0.399020, RDropREG 0.025707, GateReg 0.079855) | LR: 1.829268e-04
2025-09-21 07:34:00,667 [INFO] Train step 200/274 - Loss: 0.277511 (wMSE 0.108473, SoftCE 0.339106, Corr 0.843210, Cons 0.160328, CSim 0.084160, Rank 0.681492, RDropCLS 0.328372, RDropREG 0.023181, GateReg 0.074849) | LR: 1.997082e-04
2025-09-21 07:34:14,042 [INFO] Train step 250/274 - Loss: 0.268308 (wMSE 0.103945, SoftCE 0.329899, Corr 0.813408, Cons 0.150833, CSim 0.080317, Rank 0.678393, RDropCLS 0.278756, RDropREG 0.020789, GateReg 0.072168) | LR: 1.983384e-04
2025-09-21 07:34:20,447 [INFO] Epoch training completed in 74.24s with average loss 0.264520
2025-09-21 07:34:20,448 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:34:21,606 [INFO] Fold 0 - Epoch 1 - Train Loss: 0.264520 | EMA Val wMSE: 0.100134 | EMA Val SoftCE: 0.309094 | EMA Val Pearson: 0.180261
2025-09-21 07:34:21,611 [INFO] Fold 0 - Epoch 1 - New best EMA model with Pearson 0.180261.
2025-09-21 07:34:21,611 [INFO] Fold 0 - Epoch 2/6 started.
2025-09-21 07:34:35,275 [INFO] Train step 50/274 - Loss: 0.210760 (wMSE 0.073244, SoftCE 0.284877, Corr 0.566015, Cons 0.111676, CSim 0.061906, Rank 0.653185, RDropCLS 0.064158, RDropREG 0.007130, GateReg 0.077848) | LR: 1.942877e-04
2025-09-21 07:34:48,611 [INFO] Train step 100/274 - Loss: 0.208504 (wMSE 0.072729, SoftCE 0.282910, Corr 0.552274, Cons 0.105615, CSim 0.061329, Rank 0.651154, RDropCLS 0.062222, RDropREG 0.006645, GateReg 0.080136) | LR: 1.902280e-04
2025-09-21 07:35:01,974 [INFO] Train step 150/274 - Loss: 0.205336 (wMSE 0.071005, SoftCE 0.281920, Corr 0.530508, Cons 0.107006, CSim 0.061266, Rank 0.647709, RDropCLS 0.060057, RDropREG 0.006295, GateReg 0.082869) | LR: 1.851529e-04
2025-09-21 07:35:15,374 [INFO] Train step 200/274 - Loss: 0.203167 (wMSE 0.069674, SoftCE 0.280382, Corr 0.520876, Cons 0.103579, CSim 0.060600, Rank 0.646219, RDropCLS 0.058325, RDropREG 0.006180, GateReg 0.084231) | LR: 1.791195e-04
2025-09-21 07:35:28,812 [INFO] Train step 250/274 - Loss: 0.200401 (wMSE 0.068035, SoftCE 0.278347, Corr 0.508504, Cons 0.099414, CSim 0.059891, Rank 0.643920, RDropCLS 0.056407, RDropREG 0.006111, GateReg 0.085716) | LR: 1.721956e-04
2025-09-21 07:35:35,266 [INFO] Epoch training completed in 73.65s with average loss 0.199187
2025-09-21 07:35:35,267 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:35:36,407 [INFO] Fold 0 - Epoch 2 - Train Loss: 0.199187 | EMA Val wMSE: 0.083070 | EMA Val SoftCE: 0.281628 | EMA Val Pearson: 0.415562
2025-09-21 07:35:36,411 [INFO] Fold 0 - Epoch 2 - New best EMA model with Pearson 0.415562.
2025-09-21 07:35:36,412 [INFO] Fold 0 - Epoch 3/6 started.
2025-09-21 07:35:50,153 [INFO] Train step 50/274 - Loss: 0.175808 (wMSE 0.053871, SoftCE 0.263911, Corr 0.376843, Cons 0.082296, CSim 0.054442, Rank 0.616377, RDropCLS 0.056190, RDropREG 0.006232, GateReg 0.097150) | LR: 1.604825e-04
2025-09-21 07:36:03,508 [INFO] Train step 100/274 - Loss: 0.175479 (wMSE 0.054057, SoftCE 0.264253, Corr 0.372032, Cons 0.080774, CSim 0.054372, Rank 0.614606, RDropCLS 0.054362, RDropREG 0.006378, GateReg 0.098790) | LR: 1.517058e-04
2025-09-21 07:36:16,885 [INFO] Train step 150/274 - Loss: 0.173804 (wMSE 0.053057, SoftCE 0.262435, Corr 0.365662, Cons 0.079447, CSim 0.053961, Rank 0.613513, RDropCLS 0.055875, RDropREG 0.006354, GateReg 0.099131) | LR: 1.423473e-04
2025-09-21 07:36:30,223 [INFO] Train step 200/274 - Loss: 0.172193 (wMSE 0.052016, SoftCE 0.261367, Corr 0.359521, Cons 0.076082, CSim 0.052997, Rank 0.611177, RDropCLS 0.055707, RDropREG 0.006459, GateReg 0.100554) | LR: 1.325122e-04
2025-09-21 07:36:43,596 [INFO] Train step 250/274 - Loss: 0.170663 (wMSE 0.051162, SoftCE 0.259741, Corr 0.354534, Cons 0.072911, CSim 0.052170, Rank 0.609668, RDropCLS 0.055707, RDropREG 0.006648, GateReg 0.101615) | LR: 1.223112e-04
2025-09-21 07:36:50,066 [INFO] Epoch training completed in 73.65s with average loss 0.170111
2025-09-21 07:36:50,067 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:36:51,208 [INFO] Fold 0 - Epoch 3 - Train Loss: 0.170111 | EMA Val wMSE: 0.070243 | EMA Val SoftCE: 0.272846 | EMA Val Pearson: 0.521870
2025-09-21 07:36:51,213 [INFO] Fold 0 - Epoch 3 - New best EMA model with Pearson 0.521870.
2025-09-21 07:36:51,213 [INFO] Fold 0 - Epoch 4/6 started.
2025-09-21 07:37:04,904 [INFO] Train step 50/274 - Loss: 0.156379 (wMSE 0.042368, SoftCE 0.250870, Corr 0.286738, Cons 0.061219, CSim 0.046540, Rank 0.590573, RDropCLS 0.059397, RDropREG 0.007219, GateReg 0.107345) | LR: 1.067874e-04
2025-09-21 07:37:18,268 [INFO] Train step 100/274 - Loss: 0.156223 (wMSE 0.042194, SoftCE 0.250741, Corr 0.285138, Cons 0.059591, CSim 0.047127, Rank 0.592935, RDropCLS 0.059098, RDropREG 0.007159, GateReg 0.106983) | LR: 9.618007e-05
2025-09-21 07:37:31,672 [INFO] Train step 150/274 - Loss: 0.155997 (wMSE 0.042348, SoftCE 0.249638, Corr 0.285924, Cons 0.057559, CSim 0.047199, Rank 0.592683, RDropCLS 0.059252, RDropREG 0.007097, GateReg 0.107859) | LR: 8.561573e-05
2025-09-21 07:37:45,246 [INFO] Train step 200/274 - Loss: 0.155669 (wMSE 0.042180, SoftCE 0.249003, Corr 0.285953, Cons 0.056447, CSim 0.047056, Rank 0.592223, RDropCLS 0.059321, RDropREG 0.007153, GateReg 0.108368) | LR: 7.521326e-05
2025-09-21 07:37:58,574 [INFO] Train step 250/274 - Loss: 0.154785 (wMSE 0.041761, SoftCE 0.248006, Corr 0.282737, Cons 0.054774, CSim 0.046807, Rank 0.591264, RDropCLS 0.059143, RDropREG 0.007270, GateReg 0.108307) | LR: 6.508975e-05
2025-09-21 07:38:05,071 [INFO] Epoch training completed in 73.86s with average loss 0.154364
2025-09-21 07:38:05,072 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:38:06,150 [INFO] Fold 0 - Epoch 4 - Train Loss: 0.154364 | EMA Val wMSE: 0.064392 | EMA Val SoftCE: 0.264676 | EMA Val Pearson: 0.571719
2025-09-21 07:38:06,158 [INFO] Fold 0 - Epoch 4 - New best EMA model with Pearson 0.571719.
2025-09-21 07:38:06,159 [INFO] Fold 0 - Epoch 5/6 started.
2025-09-21 07:38:19,799 [INFO] Train step 50/274 - Loss: 0.147228 (wMSE 0.037495, SoftCE 0.241949, Corr 0.249968, Cons 0.045590, CSim 0.044676, Rank 0.580867, RDropCLS 0.059708, RDropREG 0.008103, GateReg 0.111904) | LR: 5.086032e-05
2025-09-21 07:38:33,094 [INFO] Train step 100/274 - Loss: 0.147270 (wMSE 0.037424, SoftCE 0.242148, Corr 0.248823, Cons 0.048216, CSim 0.044815, Rank 0.581987, RDropCLS 0.060082, RDropREG 0.007633, GateReg 0.110831) | LR: 4.191050e-05
2025-09-21 07:38:46,471 [INFO] Train step 150/274 - Loss: 0.146805 (wMSE 0.036999, SoftCE 0.242272, Corr 0.245876, Cons 0.048055, CSim 0.044543, Rank 0.581614, RDropCLS 0.060149, RDropREG 0.007557, GateReg 0.111202) | LR: 3.361443e-05
2025-09-21 07:38:59,933 [INFO] Train step 200/274 - Loss: 0.146310 (wMSE 0.036583, SoftCE 0.241806, Corr 0.244600, Cons 0.047767, CSim 0.044329, Rank 0.581525, RDropCLS 0.060074, RDropREG 0.007459, GateReg 0.111087) | LR: 2.606547e-05
2025-09-21 07:39:13,307 [INFO] Train step 250/274 - Loss: 0.145893 (wMSE 0.036309, SoftCE 0.241459, Corr 0.242693, Cons 0.047839, CSim 0.044237, Rank 0.581327, RDropCLS 0.060338, RDropREG 0.007453, GateReg 0.110853) | LR: 1.934857e-05
2025-09-21 07:39:19,795 [INFO] Epoch training completed in 73.64s with average loss 0.145695
2025-09-21 07:39:19,796 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:39:20,929 [INFO] Fold 0 - Epoch 5 - Train Loss: 0.145695 | EMA Val wMSE: 0.064837 | EMA Val SoftCE: 0.260650 | EMA Val Pearson: 0.591259
2025-09-21 07:39:20,934 [INFO] Fold 0 - Epoch 5 - New best EMA model with Pearson 0.591259.
2025-09-21 07:39:20,934 [INFO] Fold 0 - Epoch 6/6 started.
2025-09-21 07:39:34,646 [INFO] Train step 50/274 - Loss: 0.140044 (wMSE 0.032596, SoftCE 0.235559, Corr 0.221641, Cons 0.043697, CSim 0.041281, Rank 0.578373, RDropCLS 0.061619, RDropREG 0.007380, GateReg 0.110849) | LR: 1.109289e-05
2025-09-21 07:39:47,973 [INFO] Train step 100/274 - Loss: 0.141440 (wMSE 0.033482, SoftCE 0.237901, Corr 0.223977, Cons 0.044039, CSim 0.042631, Rank 0.577203, RDropCLS 0.061573, RDropREG 0.007519, GateReg 0.111546) | LR: 6.743759e-06
2025-09-21 07:40:01,473 [INFO] Train step 150/274 - Loss: 0.141288 (wMSE 0.033275, SoftCE 0.238248, Corr 0.222512, Cons 0.043784, CSim 0.042640, Rank 0.576702, RDropCLS 0.061856, RDropREG 0.007587, GateReg 0.111137) | LR: 3.444140e-06
2025-09-21 07:40:14,930 [INFO] Train step 200/274 - Loss: 0.141326 (wMSE 0.033283, SoftCE 0.238280, Corr 0.222644, Cons 0.043767, CSim 0.042586, Rank 0.577109, RDropCLS 0.061738, RDropREG 0.007602, GateReg 0.110918) | LR: 1.231166e-06
2025-09-21 07:40:28,311 [INFO] Train step 250/274 - Loss: 0.141516 (wMSE 0.033481, SoftCE 0.238200, Corr 0.223713, Cons 0.043877, CSim 0.042731, Rank 0.577186, RDropCLS 0.062020, RDropREG 0.007643, GateReg 0.110963) | LR: 1.297403e-07
2025-09-21 07:40:34,766 [INFO] Epoch training completed in 73.83s with average loss 0.141520
2025-09-21 07:40:34,767 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:40:35,898 [INFO] Fold 0 - Epoch 6 - Train Loss: 0.141520 | EMA Val wMSE: 0.067336 | EMA Val SoftCE: 0.260086 | EMA Val Pearson: 0.596883
2025-09-21 07:40:35,903 [INFO] Fold 0 - Epoch 6 - New best EMA model with Pearson 0.596883.
2025-09-21 07:40:36,745 [INFO] Fold 0 - Final Val Pearson (no TTA): 0.596883 | Final Val wMSE: 0.067336 | Final Val SoftCE: 0.260086
2025-09-21 07:40:37,487 [INFO] Fold 0 - Final Val Pearson (with TTA swap): 0.598778
2025-09-21 07:40:38,506 [INFO] ========== Fold 2/5 ==========
2025-09-21 07:40:39,141 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 07:40:39,151 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 07:40:39,151 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 07:40:39,151 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 07:40:39,151 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 07:40:39,151 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 07:40:39,153 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 07:40:39,326 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 07:40:39,327 [INFO] Initializing EMA state from current model weights.
2025-09-21 07:40:39,484 [INFO] Fold 1 - Epoch 1/6 started.
2025-09-21 07:40:53,275 [INFO] Train step 50/274 - Loss: 0.326087 (wMSE 0.129552, SoftCE 0.401792, Corr 0.938450, Cons 0.193755, CSim 0.131559, Rank 0.692285, RDropCLS 0.651109, RDropREG 0.054704, GateReg 0.116633) | LR: 6.097561e-05
2025-09-21 07:41:06,673 [INFO] Train step 100/274 - Loss: 0.298998 (wMSE 0.115315, SoftCE 0.365912, Corr 0.897489, Cons 0.182361, CSim 0.104004, Rank 0.686799, RDropCLS 0.499025, RDropREG 0.039025, GateReg 0.088769) | LR: 1.219512e-04
2025-09-21 07:41:20,025 [INFO] Train step 150/274 - Loss: 0.283612 (wMSE 0.108649, SoftCE 0.347686, Corr 0.859182, Cons 0.171138, CSim 0.092888, Rank 0.683476, RDropCLS 0.392661, RDropREG 0.031047, GateReg 0.081941) | LR: 1.829268e-04
2025-09-21 07:41:33,509 [INFO] Train step 200/274 - Loss: 0.273024 (wMSE 0.104232, SoftCE 0.335657, Corr 0.830963, Cons 0.156137, CSim 0.086108, Rank 0.680594, RDropCLS 0.320379, RDropREG 0.026265, GateReg 0.078859) | LR: 1.997082e-04
2025-09-21 07:41:46,902 [INFO] Train step 250/274 - Loss: 0.265148 (wMSE 0.100876, SoftCE 0.327443, Corr 0.805867, Cons 0.146720, CSim 0.081974, Rank 0.677828, RDropCLS 0.271449, RDropREG 0.022954, GateReg 0.076117) | LR: 1.983384e-04
2025-09-21 07:41:53,328 [INFO] Epoch training completed in 73.84s with average loss 0.261631
2025-09-21 07:41:53,329 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:41:54,441 [INFO] Fold 1 - Epoch 1 - Train Loss: 0.261631 | EMA Val wMSE: 0.088053 | EMA Val SoftCE: 0.301580 | EMA Val Pearson: 0.271256
2025-09-21 07:41:54,446 [INFO] Fold 1 - Epoch 1 - New best EMA model with Pearson 0.271256.
2025-09-21 07:41:54,446 [INFO] Fold 1 - Epoch 2/6 started.
2025-09-21 07:42:08,167 [INFO] Train step 50/274 - Loss: 0.210187 (wMSE 0.073013, SoftCE 0.284182, Corr 0.566607, Cons 0.107350, CSim 0.061583, Rank 0.652285, RDropCLS 0.057475, RDropREG 0.007648, GateReg 0.075953) | LR: 1.942877e-04
2025-09-21 07:42:21,709 [INFO] Train step 100/274 - Loss: 0.207850 (wMSE 0.072088, SoftCE 0.283208, Corr 0.549392, Cons 0.107424, CSim 0.061414, Rank 0.649925, RDropCLS 0.055390, RDropREG 0.007203, GateReg 0.078415) | LR: 1.902280e-04
2025-09-21 07:42:35,081 [INFO] Train step 150/274 - Loss: 0.205072 (wMSE 0.070676, SoftCE 0.281465, Corr 0.534759, Cons 0.102008, CSim 0.060495, Rank 0.647389, RDropCLS 0.053458, RDropREG 0.006882, GateReg 0.081588) | LR: 1.851529e-04
2025-09-21 07:42:48,566 [INFO] Train step 200/274 - Loss: 0.202141 (wMSE 0.069108, SoftCE 0.279476, Corr 0.519430, Cons 0.097954, CSim 0.059723, Rank 0.645199, RDropCLS 0.052294, RDropREG 0.006624, GateReg 0.083410) | LR: 1.791195e-04
2025-09-21 07:43:02,177 [INFO] Train step 250/274 - Loss: 0.199421 (wMSE 0.067541, SoftCE 0.277694, Corr 0.505730, Cons 0.096093, CSim 0.059085, Rank 0.642023, RDropCLS 0.052081, RDropREG 0.006513, GateReg 0.084496) | LR: 1.721956e-04
2025-09-21 07:43:08,678 [INFO] Epoch training completed in 74.23s with average loss 0.198118
2025-09-21 07:43:08,679 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:43:09,780 [INFO] Fold 1 - Epoch 2 - Train Loss: 0.198118 | EMA Val wMSE: 0.076211 | EMA Val SoftCE: 0.285090 | EMA Val Pearson: 0.423182
2025-09-21 07:43:09,784 [INFO] Fold 1 - Epoch 2 - New best EMA model with Pearson 0.423182.
2025-09-21 07:43:09,784 [INFO] Fold 1 - Epoch 3/6 started.
2025-09-21 07:43:23,476 [INFO] Train step 50/274 - Loss: 0.172517 (wMSE 0.051941, SoftCE 0.262831, Corr 0.360010, Cons 0.073008, CSim 0.052249, Rank 0.611649, RDropCLS 0.054859, RDropREG 0.006769, GateReg 0.095878) | LR: 1.604825e-04
2025-09-21 07:43:36,992 [INFO] Train step 100/274 - Loss: 0.172618 (wMSE 0.052170, SoftCE 0.262500, Corr 0.360859, Cons 0.073653, CSim 0.052126, Rank 0.612210, RDropCLS 0.053474, RDropREG 0.006558, GateReg 0.096146) | LR: 1.517058e-04
2025-09-21 07:43:50,330 [INFO] Train step 150/274 - Loss: 0.171685 (wMSE 0.051546, SoftCE 0.260546, Corr 0.361973, Cons 0.070413, CSim 0.051363, Rank 0.612443, RDropCLS 0.053227, RDropREG 0.006484, GateReg 0.097098) | LR: 1.423473e-04
2025-09-21 07:44:03,758 [INFO] Train step 200/274 - Loss: 0.170099 (wMSE 0.050748, SoftCE 0.258928, Corr 0.355281, Cons 0.067754, CSim 0.051076, Rank 0.611042, RDropCLS 0.053370, RDropREG 0.006457, GateReg 0.098668) | LR: 1.325122e-04
2025-09-21 07:44:17,249 [INFO] Train step 250/274 - Loss: 0.169079 (wMSE 0.050195, SoftCE 0.258201, Corr 0.350309, Cons 0.066338, CSim 0.050885, Rank 0.609750, RDropCLS 0.053392, RDropREG 0.006353, GateReg 0.099416) | LR: 1.223112e-04
2025-09-21 07:44:23,792 [INFO] Epoch training completed in 74.01s with average loss 0.168603
2025-09-21 07:44:23,793 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:44:24,963 [INFO] Fold 1 - Epoch 3 - Train Loss: 0.168603 | EMA Val wMSE: 0.070055 | EMA Val SoftCE: 0.273669 | EMA Val Pearson: 0.510671
2025-09-21 07:44:24,967 [INFO] Fold 1 - Epoch 3 - New best EMA model with Pearson 0.510671.
2025-09-21 07:44:24,967 [INFO] Fold 1 - Epoch 4/6 started.
2025-09-21 07:44:38,776 [INFO] Train step 50/274 - Loss: 0.154315 (wMSE 0.041632, SoftCE 0.247699, Corr 0.277959, Cons 0.058121, CSim 0.046935, Rank 0.592275, RDropCLS 0.057572, RDropREG 0.006842, GateReg 0.105698) | LR: 1.067874e-04
2025-09-21 07:44:52,090 [INFO] Train step 100/274 - Loss: 0.154151 (wMSE 0.041414, SoftCE 0.248308, Corr 0.276450, Cons 0.054637, CSim 0.046579, Rank 0.592164, RDropCLS 0.056951, RDropREG 0.006528, GateReg 0.106208) | LR: 9.618007e-05
2025-09-21 07:45:05,467 [INFO] Train step 150/274 - Loss: 0.153780 (wMSE 0.041219, SoftCE 0.247851, Corr 0.275323, Cons 0.054543, CSim 0.046470, Rank 0.591768, RDropCLS 0.056441, RDropREG 0.006467, GateReg 0.106426) | LR: 8.561573e-05
2025-09-21 07:45:18,834 [INFO] Train step 200/274 - Loss: 0.153334 (wMSE 0.040880, SoftCE 0.247567, Corr 0.273498, Cons 0.053496, CSim 0.046235, Rank 0.591728, RDropCLS 0.056649, RDropREG 0.006542, GateReg 0.106639) | LR: 7.521326e-05
2025-09-21 07:45:32,382 [INFO] Train step 250/274 - Loss: 0.152976 (wMSE 0.040597, SoftCE 0.247087, Corr 0.273334, Cons 0.052509, CSim 0.045896, Rank 0.591439, RDropCLS 0.056981, RDropREG 0.006531, GateReg 0.106620) | LR: 6.508975e-05
2025-09-21 07:45:38,877 [INFO] Epoch training completed in 73.91s with average loss 0.152684
2025-09-21 07:45:38,878 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:45:40,037 [INFO] Fold 1 - Epoch 4 - Train Loss: 0.152684 | EMA Val wMSE: 0.065589 | EMA Val SoftCE: 0.265698 | EMA Val Pearson: 0.559063
2025-09-21 07:45:40,041 [INFO] Fold 1 - Epoch 4 - New best EMA model with Pearson 0.559063.
2025-09-21 07:45:40,042 [INFO] Fold 1 - Epoch 5/6 started.
2025-09-21 07:45:53,732 [INFO] Train step 50/274 - Loss: 0.143957 (wMSE 0.035495, SoftCE 0.239915, Corr 0.231508, Cons 0.047123, CSim 0.044009, Rank 0.580464, RDropCLS 0.057910, RDropREG 0.006785, GateReg 0.109386) | LR: 5.086032e-05
2025-09-21 07:46:07,096 [INFO] Train step 100/274 - Loss: 0.143928 (wMSE 0.035363, SoftCE 0.239303, Corr 0.234769, Cons 0.045526, CSim 0.043030, Rank 0.581556, RDropCLS 0.057532, RDropREG 0.006790, GateReg 0.109501) | LR: 4.191050e-05
2025-09-21 07:46:20,470 [INFO] Train step 150/274 - Loss: 0.144272 (wMSE 0.035471, SoftCE 0.239854, Corr 0.235896, Cons 0.046400, CSim 0.043049, Rank 0.581222, RDropCLS 0.059039, RDropREG 0.006821, GateReg 0.108901) | LR: 3.361443e-05
2025-09-21 07:46:33,830 [INFO] Train step 200/274 - Loss: 0.144564 (wMSE 0.035621, SoftCE 0.240195, Corr 0.237245, Cons 0.045809, CSim 0.043090, Rank 0.581270, RDropCLS 0.059375, RDropREG 0.006813, GateReg 0.108928) | LR: 2.606547e-05
2025-09-21 07:46:47,170 [INFO] Train step 250/274 - Loss: 0.144452 (wMSE 0.035507, SoftCE 0.240020, Corr 0.237458, Cons 0.045270, CSim 0.042995, Rank 0.581395, RDropCLS 0.059196, RDropREG 0.006738, GateReg 0.108842) | LR: 1.934857e-05
2025-09-21 07:46:53,684 [INFO] Epoch training completed in 73.64s with average loss 0.144387
2025-09-21 07:46:53,685 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:46:54,854 [INFO] Fold 1 - Epoch 5 - Train Loss: 0.144387 | EMA Val wMSE: 0.065769 | EMA Val SoftCE: 0.261923 | EMA Val Pearson: 0.579633
2025-09-21 07:46:54,858 [INFO] Fold 1 - Epoch 5 - New best EMA model with Pearson 0.579633.
2025-09-21 07:46:54,858 [INFO] Fold 1 - Epoch 6/6 started.
2025-09-21 07:47:08,557 [INFO] Train step 50/274 - Loss: 0.140939 (wMSE 0.033624, SoftCE 0.236926, Corr 0.221749, Cons 0.042834, CSim 0.042118, Rank 0.576979, RDropCLS 0.060231, RDropREG 0.006953, GateReg 0.108446) | LR: 1.109289e-05
2025-09-21 07:47:21,923 [INFO] Train step 100/274 - Loss: 0.140848 (wMSE 0.033366, SoftCE 0.237032, Corr 0.221453, Cons 0.043272, CSim 0.042045, Rank 0.578116, RDropCLS 0.059387, RDropREG 0.006874, GateReg 0.108153) | LR: 6.743759e-06
2025-09-21 07:47:35,276 [INFO] Train step 150/274 - Loss: 0.140860 (wMSE 0.033251, SoftCE 0.237556, Corr 0.220146, Cons 0.043725, CSim 0.041988, Rank 0.578447, RDropCLS 0.059374, RDropREG 0.006877, GateReg 0.108147) | LR: 3.444140e-06
2025-09-21 07:47:48,588 [INFO] Train step 200/274 - Loss: 0.140667 (wMSE 0.033147, SoftCE 0.237251, Corr 0.220121, Cons 0.044018, CSim 0.041888, Rank 0.577543, RDropCLS 0.059490, RDropREG 0.006884, GateReg 0.108056) | LR: 1.231166e-06
2025-09-21 07:48:01,853 [INFO] Train step 250/274 - Loss: 0.140655 (wMSE 0.033170, SoftCE 0.237217, Corr 0.219904, Cons 0.044249, CSim 0.041984, Rank 0.577197, RDropCLS 0.059983, RDropREG 0.006919, GateReg 0.108302) | LR: 1.297403e-07
2025-09-21 07:48:08,290 [INFO] Epoch training completed in 73.43s with average loss 0.140547
2025-09-21 07:48:08,291 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:48:09,469 [INFO] Fold 1 - Epoch 6 - Train Loss: 0.140547 | EMA Val wMSE: 0.068078 | EMA Val SoftCE: 0.261266 | EMA Val Pearson: 0.586711
2025-09-21 07:48:09,473 [INFO] Fold 1 - Epoch 6 - New best EMA model with Pearson 0.586711.
2025-09-21 07:48:10,327 [INFO] Fold 1 - Final Val Pearson (no TTA): 0.586711 | Final Val wMSE: 0.068078 | Final Val SoftCE: 0.261266
2025-09-21 07:48:11,091 [INFO] Fold 1 - Final Val Pearson (with TTA swap): 0.590393
2025-09-21 07:48:12,118 [INFO] ========== Fold 3/5 ==========
2025-09-21 07:48:12,179 [INFO] Applied swap augmentation. New train size: 52518
2025-09-21 07:48:12,184 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 07:48:12,184 [INFO] Train split: 52518 samples; Val split: 6566 samples.
2025-09-21 07:48:12,184 [INFO] Initializing PatentDataset with 52518 samples.
2025-09-21 07:48:12,184 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 07:48:12,184 [INFO] Initializing PatentDataset with 6566 samples.
2025-09-21 07:48:12,186 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 07:48:12,361 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 07:48:12,361 [INFO] Initializing EMA state from current model weights.
2025-09-21 07:48:12,518 [INFO] Fold 2 - Epoch 1/6 started.
2025-09-21 07:48:26,224 [INFO] Train step 50/274 - Loss: 0.333364 (wMSE 0.133020, SoftCE 0.406016, Corr 0.971128, Cons 0.260636, CSim 0.140431, Rank 0.699503, RDropCLS 0.632769, RDropREG 0.033816, GateReg 0.101042) | LR: 6.097561e-05
2025-09-21 07:48:39,528 [INFO] Train step 100/274 - Loss: 0.303956 (wMSE 0.118446, SoftCE 0.369349, Corr 0.910495, Cons 0.227010, CSim 0.109344, Rank 0.691242, RDropCLS 0.493731, RDropREG 0.026354, GateReg 0.091786) | LR: 1.219512e-04
2025-09-21 07:48:52,844 [INFO] Train step 150/274 - Loss: 0.288203 (wMSE 0.111614, SoftCE 0.351022, Corr 0.874168, Cons 0.201579, CSim 0.095747, Rank 0.686445, RDropCLS 0.388734, RDropREG 0.023823, GateReg 0.084253) | LR: 1.829268e-04
2025-09-21 07:49:06,126 [INFO] Train step 200/274 - Loss: 0.275977 (wMSE 0.105919, SoftCE 0.337801, Corr 0.839706, Cons 0.182950, CSim 0.088471, Rank 0.682798, RDropCLS 0.318832, RDropREG 0.021124, GateReg 0.078934) | LR: 1.997082e-04
2025-09-21 07:49:19,383 [INFO] Train step 250/274 - Loss: 0.266511 (wMSE 0.101201, SoftCE 0.328445, Corr 0.810229, Cons 0.167939, CSim 0.083589, Rank 0.679841, RDropCLS 0.269941, RDropREG 0.018624, GateReg 0.076628) | LR: 1.983384e-04
2025-09-21 07:49:25,789 [INFO] Epoch training completed in 73.27s with average loss 0.262706
2025-09-21 07:49:25,790 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:49:26,929 [INFO] Fold 2 - Epoch 1 - Train Loss: 0.262706 | EMA Val wMSE: 0.093335 | EMA Val SoftCE: 0.303824 | EMA Val Pearson: 0.219292
2025-09-21 07:49:26,933 [INFO] Fold 2 - Epoch 1 - New best EMA model with Pearson 0.219292.
2025-09-21 07:49:26,933 [INFO] Fold 2 - Epoch 2/6 started.
2025-09-21 07:49:40,554 [INFO] Train step 50/274 - Loss: 0.211548 (wMSE 0.074311, SoftCE 0.284306, Corr 0.571471, Cons 0.113959, CSim 0.061446, Rank 0.652286, RDropCLS 0.063508, RDropREG 0.007677, GateReg 0.070861) | LR: 1.942877e-04
2025-09-21 07:49:53,837 [INFO] Train step 100/274 - Loss: 0.210223 (wMSE 0.073879, SoftCE 0.283482, Corr 0.563975, Cons 0.107393, CSim 0.062007, Rank 0.651168, RDropCLS 0.058454, RDropREG 0.007422, GateReg 0.075429) | LR: 1.902280e-04
2025-09-21 07:50:07,158 [INFO] Train step 150/274 - Loss: 0.206644 (wMSE 0.071861, SoftCE 0.280913, Corr 0.546627, Cons 0.102447, CSim 0.061217, Rank 0.648665, RDropCLS 0.055625, RDropREG 0.007174, GateReg 0.077841) | LR: 1.851529e-04
2025-09-21 07:50:20,483 [INFO] Train step 200/274 - Loss: 0.203929 (wMSE 0.070211, SoftCE 0.279599, Corr 0.531495, Cons 0.100530, CSim 0.060733, Rank 0.645798, RDropCLS 0.055388, RDropREG 0.007041, GateReg 0.079623) | LR: 1.791195e-04
2025-09-21 07:50:33,760 [INFO] Train step 250/274 - Loss: 0.201506 (wMSE 0.069093, SoftCE 0.278460, Corr 0.515641, Cons 0.098129, CSim 0.060463, Rank 0.643392, RDropCLS 0.054359, RDropREG 0.006824, GateReg 0.081109) | LR: 1.721956e-04
2025-09-21 07:50:40,252 [INFO] Epoch training completed in 73.32s with average loss 0.200344
2025-09-21 07:50:40,253 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:50:41,410 [INFO] Fold 2 - Epoch 2 - Train Loss: 0.200344 | EMA Val wMSE: 0.078982 | EMA Val SoftCE: 0.283841 | EMA Val Pearson: 0.424432
2025-09-21 07:50:41,416 [INFO] Fold 2 - Epoch 2 - New best EMA model with Pearson 0.424432.
2025-09-21 07:50:41,416 [INFO] Fold 2 - Epoch 3/6 started.
2025-09-21 07:50:55,091 [INFO] Train step 50/274 - Loss: 0.176625 (wMSE 0.053962, SoftCE 0.262921, Corr 0.390915, Cons 0.075737, CSim 0.053370, Rank 0.619694, RDropCLS 0.055065, RDropREG 0.007429, GateReg 0.090580) | LR: 1.604825e-04
2025-09-21 07:51:08,380 [INFO] Train step 100/274 - Loss: 0.174935 (wMSE 0.053118, SoftCE 0.262496, Corr 0.378890, Cons 0.076143, CSim 0.053228, Rank 0.616848, RDropCLS 0.054996, RDropREG 0.007415, GateReg 0.091831) | LR: 1.517058e-04
2025-09-21 07:51:21,696 [INFO] Train step 150/274 - Loss: 0.173932 (wMSE 0.052876, SoftCE 0.261508, Corr 0.372761, Cons 0.076083, CSim 0.053128, Rank 0.615286, RDropCLS 0.054521, RDropREG 0.007332, GateReg 0.094121) | LR: 1.423473e-04
2025-09-21 07:51:35,027 [INFO] Train step 200/274 - Loss: 0.173461 (wMSE 0.052874, SoftCE 0.260967, Corr 0.370109, Cons 0.073179, CSim 0.053173, Rank 0.614581, RDropCLS 0.053967, RDropREG 0.007219, GateReg 0.095809) | LR: 1.325122e-04
2025-09-21 07:51:48,301 [INFO] Train step 250/274 - Loss: 0.172273 (wMSE 0.052267, SoftCE 0.260156, Corr 0.364440, Cons 0.071222, CSim 0.052669, Rank 0.612662, RDropCLS 0.053974, RDropREG 0.007057, GateReg 0.096719) | LR: 1.223112e-04
2025-09-21 07:51:54,709 [INFO] Epoch training completed in 73.29s with average loss 0.171697
2025-09-21 07:51:54,710 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:51:55,875 [INFO] Fold 2 - Epoch 3 - Train Loss: 0.171697 | EMA Val wMSE: 0.070150 | EMA Val SoftCE: 0.273459 | EMA Val Pearson: 0.524496
2025-09-21 07:51:55,879 [INFO] Fold 2 - Epoch 3 - New best EMA model with Pearson 0.524496.
2025-09-21 07:51:55,879 [INFO] Fold 2 - Epoch 4/6 started.
2025-09-21 07:52:09,491 [INFO] Train step 50/274 - Loss: 0.156857 (wMSE 0.042427, SoftCE 0.249533, Corr 0.295204, Cons 0.061007, CSim 0.046836, Rank 0.595795, RDropCLS 0.055951, RDropREG 0.006982, GateReg 0.101107) | LR: 1.067874e-04
2025-09-21 07:52:22,752 [INFO] Train step 100/274 - Loss: 0.156471 (wMSE 0.042279, SoftCE 0.249502, Corr 0.291939, Cons 0.058808, CSim 0.047001, Rank 0.595574, RDropCLS 0.056490, RDropREG 0.007220, GateReg 0.100992) | LR: 9.618007e-05
2025-09-21 07:52:35,992 [INFO] Train step 150/274 - Loss: 0.155355 (wMSE 0.042023, SoftCE 0.248049, Corr 0.286797, Cons 0.055168, CSim 0.047022, Rank 0.594193, RDropCLS 0.056954, RDropREG 0.007344, GateReg 0.101310) | LR: 8.561573e-05
2025-09-21 07:52:49,411 [INFO] Train step 200/274 - Loss: 0.154530 (wMSE 0.041705, SoftCE 0.247400, Corr 0.282479, Cons 0.053308, CSim 0.046813, Rank 0.592886, RDropCLS 0.056850, RDropREG 0.007508, GateReg 0.102157) | LR: 7.521326e-05
2025-09-21 07:53:02,697 [INFO] Train step 250/274 - Loss: 0.154199 (wMSE 0.041595, SoftCE 0.247057, Corr 0.281026, Cons 0.052421, CSim 0.046822, Rank 0.592145, RDropCLS 0.056921, RDropREG 0.007546, GateReg 0.102452) | LR: 6.508975e-05
2025-09-21 07:53:09,119 [INFO] Epoch training completed in 73.24s with average loss 0.154361
2025-09-21 07:53:09,120 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:53:10,235 [INFO] Fold 2 - Epoch 4 - Train Loss: 0.154361 | EMA Val wMSE: 0.066039 | EMA Val SoftCE: 0.265149 | EMA Val Pearson: 0.575696
2025-09-21 07:53:10,239 [INFO] Fold 2 - Epoch 4 - New best EMA model with Pearson 0.575696.
2025-09-21 07:53:10,239 [INFO] Fold 2 - Epoch 5/6 started.
2025-09-21 07:53:23,849 [INFO] Train step 50/274 - Loss: 0.146975 (wMSE 0.036773, SoftCE 0.241974, Corr 0.250440, Cons 0.048938, CSim 0.043529, Rank 0.584097, RDropCLS 0.058466, RDropREG 0.007676, GateReg 0.103388) | LR: 5.086032e-05
2025-09-21 07:53:37,088 [INFO] Train step 100/274 - Loss: 0.145975 (wMSE 0.036561, SoftCE 0.240395, Corr 0.245674, Cons 0.045781, CSim 0.043735, Rank 0.584107, RDropCLS 0.059429, RDropREG 0.007761, GateReg 0.104084) | LR: 4.191050e-05
2025-09-21 07:53:50,392 [INFO] Train step 150/274 - Loss: 0.145292 (wMSE 0.036078, SoftCE 0.240234, Corr 0.242029, Cons 0.045323, CSim 0.043761, Rank 0.582491, RDropCLS 0.059904, RDropREG 0.007770, GateReg 0.104370) | LR: 3.361443e-05
2025-09-21 07:54:03,685 [INFO] Train step 200/274 - Loss: 0.145375 (wMSE 0.036203, SoftCE 0.239992, Corr 0.243107, Cons 0.044065, CSim 0.043919, Rank 0.582729, RDropCLS 0.060132, RDropREG 0.007852, GateReg 0.104433) | LR: 2.606547e-05
2025-09-21 07:54:17,010 [INFO] Train step 250/274 - Loss: 0.145495 (wMSE 0.036315, SoftCE 0.240246, Corr 0.242857, Cons 0.043638, CSim 0.044130, Rank 0.582589, RDropCLS 0.060496, RDropREG 0.007882, GateReg 0.104467) | LR: 1.934857e-05
2025-09-21 07:54:23,463 [INFO] Epoch training completed in 73.22s with average loss 0.145273
2025-09-21 07:54:23,464 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:54:24,612 [INFO] Fold 2 - Epoch 5 - Train Loss: 0.145273 | EMA Val wMSE: 0.065879 | EMA Val SoftCE: 0.260476 | EMA Val Pearson: 0.598369
2025-09-21 07:54:24,616 [INFO] Fold 2 - Epoch 5 - New best EMA model with Pearson 0.598369.
2025-09-21 07:54:24,616 [INFO] Fold 2 - Epoch 6/6 started.
2025-09-21 07:54:38,257 [INFO] Train step 50/274 - Loss: 0.142276 (wMSE 0.034369, SoftCE 0.238178, Corr 0.226814, Cons 0.041829, CSim 0.043749, Rank 0.578765, RDropCLS 0.060225, RDropREG 0.007828, GateReg 0.105919) | LR: 1.109289e-05
2025-09-21 07:54:51,540 [INFO] Train step 100/274 - Loss: 0.142544 (wMSE 0.034568, SoftCE 0.238153, Corr 0.228745, Cons 0.042843, CSim 0.043423, Rank 0.578798, RDropCLS 0.060612, RDropREG 0.007805, GateReg 0.105516) | LR: 6.743759e-06
2025-09-21 07:55:04,873 [INFO] Train step 150/274 - Loss: 0.141507 (wMSE 0.033858, SoftCE 0.236898, Corr 0.226009, Cons 0.042222, CSim 0.043000, Rank 0.578187, RDropCLS 0.061319, RDropREG 0.007849, GateReg 0.105297) | LR: 3.444140e-06
2025-09-21 07:55:18,139 [INFO] Train step 200/274 - Loss: 0.141534 (wMSE 0.033834, SoftCE 0.237325, Corr 0.225267, Cons 0.042084, CSim 0.042953, Rank 0.577561, RDropCLS 0.061663, RDropREG 0.007927, GateReg 0.105341) | LR: 1.231166e-06
2025-09-21 07:55:31,384 [INFO] Train step 250/274 - Loss: 0.141186 (wMSE 0.033580, SoftCE 0.237095, Corr 0.224062, Cons 0.041979, CSim 0.042742, Rank 0.576905, RDropCLS 0.061736, RDropREG 0.007971, GateReg 0.105543) | LR: 1.297403e-07
2025-09-21 07:55:37,821 [INFO] Epoch training completed in 73.20s with average loss 0.141270
2025-09-21 07:55:37,822 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:55:38,997 [INFO] Fold 2 - Epoch 6 - Train Loss: 0.141270 | EMA Val wMSE: 0.067623 | EMA Val SoftCE: 0.259514 | EMA Val Pearson: 0.606123
2025-09-21 07:55:39,002 [INFO] Fold 2 - Epoch 6 - New best EMA model with Pearson 0.606123.
2025-09-21 07:55:39,891 [INFO] Fold 2 - Final Val Pearson (no TTA): 0.606123 | Final Val wMSE: 0.067623 | Final Val SoftCE: 0.259514
2025-09-21 07:55:40,642 [INFO] Fold 2 - Final Val Pearson (with TTA swap): 0.610523
2025-09-21 07:55:41,663 [INFO] ========== Fold 4/5 ==========
2025-09-21 07:55:45,950 [INFO] Applied swap augmentation. New train size: 52522
2025-09-21 07:55:45,960 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 07:55:45,960 [INFO] Train split: 52522 samples; Val split: 6564 samples.
2025-09-21 07:55:45,960 [INFO] Initializing PatentDataset with 52522 samples.
2025-09-21 07:55:45,960 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 07:55:45,960 [INFO] Initializing PatentDataset with 6564 samples.
2025-09-21 07:55:45,965 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 07:55:46,138 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 07:55:46,138 [INFO] Initializing EMA state from current model weights.
2025-09-21 07:55:46,295 [INFO] Fold 3 - Epoch 1/6 started.
2025-09-21 07:55:59,918 [INFO] Train step 50/274 - Loss: 0.331579 (wMSE 0.125547, SoftCE 0.415687, Corr 0.974492, Cons 0.176319, CSim 0.130117, Rank 0.696972, RDropCLS 0.675003, RDropREG 0.061754, GateReg 0.172848) | LR: 6.097561e-05
2025-09-21 07:56:13,219 [INFO] Train step 100/274 - Loss: 0.300445 (wMSE 0.112270, SoftCE 0.374363, Corr 0.902545, Cons 0.166144, CSim 0.100837, Rank 0.687975, RDropCLS 0.513259, RDropREG 0.042136, GateReg 0.146873) | LR: 1.219512e-04
2025-09-21 07:56:26,480 [INFO] Train step 150/274 - Loss: 0.285171 (wMSE 0.106331, SoftCE 0.353770, Corr 0.869471, Cons 0.158044, CSim 0.089126, Rank 0.685028, RDropCLS 0.406700, RDropREG 0.033702, GateReg 0.132151) | LR: 1.829268e-04
2025-09-21 07:56:39,768 [INFO] Train step 200/274 - Loss: 0.274110 (wMSE 0.102377, SoftCE 0.340304, Corr 0.837504, Cons 0.146479, CSim 0.083230, Rank 0.681865, RDropCLS 0.332779, RDropREG 0.028656, GateReg 0.117049) | LR: 1.997082e-04
2025-09-21 07:56:52,995 [INFO] Train step 250/274 - Loss: 0.264857 (wMSE 0.098631, SoftCE 0.330936, Corr 0.802302, Cons 0.141667, CSim 0.079700, Rank 0.678134, RDropCLS 0.281712, RDropREG 0.024859, GateReg 0.107337) | LR: 1.983384e-04
2025-09-21 07:56:59,414 [INFO] Epoch training completed in 73.12s with average loss 0.261449
2025-09-21 07:56:59,415 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:57:00,557 [INFO] Fold 3 - Epoch 1 - Train Loss: 0.261449 | EMA Val wMSE: 0.096445 | EMA Val SoftCE: 0.308047 | EMA Val Pearson: 0.244685
2025-09-21 07:57:00,561 [INFO] Fold 3 - Epoch 1 - New best EMA model with Pearson 0.244685.
2025-09-21 07:57:00,561 [INFO] Fold 3 - Epoch 2/6 started.
2025-09-21 07:57:14,187 [INFO] Train step 50/274 - Loss: 0.213474 (wMSE 0.075079, SoftCE 0.287522, Corr 0.575558, Cons 0.119981, CSim 0.063229, Rank 0.654470, RDropCLS 0.056926, RDropREG 0.007229, GateReg 0.076703) | LR: 1.942877e-04
2025-09-21 07:57:27,455 [INFO] Train step 100/274 - Loss: 0.209293 (wMSE 0.073313, SoftCE 0.284979, Corr 0.550067, Cons 0.113426, CSim 0.062110, Rank 0.650443, RDropCLS 0.055579, RDropREG 0.006660, GateReg 0.079518) | LR: 1.902280e-04
2025-09-21 07:57:40,752 [INFO] Train step 150/274 - Loss: 0.206322 (wMSE 0.071508, SoftCE 0.283116, Corr 0.536053, Cons 0.107356, CSim 0.061119, Rank 0.648453, RDropCLS 0.053399, RDropREG 0.006354, GateReg 0.083310) | LR: 1.851529e-04
2025-09-21 07:57:54,061 [INFO] Train step 200/274 - Loss: 0.203039 (wMSE 0.069534, SoftCE 0.280734, Corr 0.520622, Cons 0.103867, CSim 0.060466, Rank 0.645638, RDropCLS 0.052457, RDropREG 0.006436, GateReg 0.085892) | LR: 1.791195e-04
2025-09-21 07:58:07,361 [INFO] Train step 250/274 - Loss: 0.200226 (wMSE 0.067730, SoftCE 0.279113, Corr 0.506449, Cons 0.101603, CSim 0.059787, Rank 0.643361, RDropCLS 0.051517, RDropREG 0.006356, GateReg 0.088168) | LR: 1.721956e-04
2025-09-21 07:58:13,808 [INFO] Epoch training completed in 73.25s with average loss 0.198942
2025-09-21 07:58:13,809 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:58:14,928 [INFO] Fold 3 - Epoch 2 - Train Loss: 0.198942 | EMA Val wMSE: 0.088599 | EMA Val SoftCE: 0.283988 | EMA Val Pearson: 0.401761
2025-09-21 07:58:14,932 [INFO] Fold 3 - Epoch 2 - New best EMA model with Pearson 0.401761.
2025-09-21 07:58:14,932 [INFO] Fold 3 - Epoch 3/6 started.
2025-09-21 07:58:28,562 [INFO] Train step 50/274 - Loss: 0.175152 (wMSE 0.052882, SoftCE 0.264049, Corr 0.375149, Cons 0.084016, CSim 0.053374, Rank 0.616399, RDropCLS 0.054164, RDropREG 0.006502, GateReg 0.104643) | LR: 1.604825e-04
2025-09-21 07:58:41,908 [INFO] Train step 100/274 - Loss: 0.173704 (wMSE 0.052007, SoftCE 0.262510, Corr 0.370122, Cons 0.082268, CSim 0.052726, Rank 0.615490, RDropCLS 0.054877, RDropREG 0.006595, GateReg 0.103617) | LR: 1.517058e-04
2025-09-21 07:58:55,304 [INFO] Train step 150/274 - Loss: 0.173490 (wMSE 0.052339, SoftCE 0.261879, Corr 0.368362, Cons 0.080240, CSim 0.053028, Rank 0.614645, RDropCLS 0.054761, RDropREG 0.006430, GateReg 0.104547) | LR: 1.423473e-04
2025-09-21 07:59:08,766 [INFO] Train step 200/274 - Loss: 0.171813 (wMSE 0.051425, SoftCE 0.260425, Corr 0.361145, Cons 0.077498, CSim 0.052540, Rank 0.612834, RDropCLS 0.055020, RDropREG 0.006400, GateReg 0.105452) | LR: 1.325122e-04
2025-09-21 07:59:22,225 [INFO] Train step 250/274 - Loss: 0.170683 (wMSE 0.050799, SoftCE 0.259712, Corr 0.355858, Cons 0.075055, CSim 0.052150, Rank 0.611115, RDropCLS 0.055153, RDropREG 0.006407, GateReg 0.105961) | LR: 1.223112e-04
2025-09-21 07:59:28,687 [INFO] Epoch training completed in 73.75s with average loss 0.170099
2025-09-21 07:59:28,688 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 07:59:29,848 [INFO] Fold 3 - Epoch 3 - Train Loss: 0.170099 | EMA Val wMSE: 0.079260 | EMA Val SoftCE: 0.275869 | EMA Val Pearson: 0.497533
2025-09-21 07:59:29,852 [INFO] Fold 3 - Epoch 3 - New best EMA model with Pearson 0.497533.
2025-09-21 07:59:29,852 [INFO] Fold 3 - Epoch 4/6 started.
2025-09-21 07:59:43,624 [INFO] Train step 50/274 - Loss: 0.156680 (wMSE 0.042684, SoftCE 0.249694, Corr 0.289682, Cons 0.061378, CSim 0.047948, Rank 0.594481, RDropCLS 0.057468, RDropREG 0.006785, GateReg 0.110965) | LR: 1.067874e-04
2025-09-21 07:59:56,946 [INFO] Train step 100/274 - Loss: 0.155683 (wMSE 0.041977, SoftCE 0.249420, Corr 0.285045, Cons 0.059356, CSim 0.047344, Rank 0.592743, RDropCLS 0.057468, RDropREG 0.006849, GateReg 0.111363) | LR: 9.618007e-05
2025-09-21 08:00:10,246 [INFO] Train step 150/274 - Loss: 0.155346 (wMSE 0.041692, SoftCE 0.248973, Corr 0.284825, Cons 0.060509, CSim 0.047086, Rank 0.591639, RDropCLS 0.058277, RDropREG 0.006958, GateReg 0.111230) | LR: 8.561573e-05
2025-09-21 08:00:23,567 [INFO] Train step 200/274 - Loss: 0.154720 (wMSE 0.041365, SoftCE 0.248818, Corr 0.280595, Cons 0.058722, CSim 0.047136, Rank 0.591012, RDropCLS 0.058258, RDropREG 0.007028, GateReg 0.111799) | LR: 7.521326e-05
2025-09-21 08:00:36,858 [INFO] Train step 250/274 - Loss: 0.154140 (wMSE 0.041002, SoftCE 0.248211, Corr 0.278621, Cons 0.057593, CSim 0.046863, Rank 0.590929, RDropCLS 0.058203, RDropREG 0.007025, GateReg 0.112100) | LR: 6.508975e-05
2025-09-21 08:00:43,282 [INFO] Epoch training completed in 73.43s with average loss 0.153939
2025-09-21 08:00:43,283 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:00:44,462 [INFO] Fold 3 - Epoch 4 - Train Loss: 0.153939 | EMA Val wMSE: 0.069889 | EMA Val SoftCE: 0.268280 | EMA Val Pearson: 0.548567
2025-09-21 08:00:44,467 [INFO] Fold 3 - Epoch 4 - New best EMA model with Pearson 0.548567.
2025-09-21 08:00:44,467 [INFO] Fold 3 - Epoch 5/6 started.
2025-09-21 08:00:58,094 [INFO] Train step 50/274 - Loss: 0.145929 (wMSE 0.036180, SoftCE 0.243286, Corr 0.237333, Cons 0.048091, CSim 0.044593, Rank 0.580761, RDropCLS 0.059296, RDropREG 0.007273, GateReg 0.113779) | LR: 5.086032e-05
2025-09-21 08:01:11,391 [INFO] Train step 100/274 - Loss: 0.145418 (wMSE 0.035834, SoftCE 0.241784, Corr 0.238705, Cons 0.048281, CSim 0.044299, Rank 0.581529, RDropCLS 0.058901, RDropREG 0.007240, GateReg 0.114085) | LR: 4.191050e-05
2025-09-21 08:01:24,732 [INFO] Train step 150/274 - Loss: 0.144918 (wMSE 0.035563, SoftCE 0.240730, Corr 0.238452, Cons 0.048731, CSim 0.044056, Rank 0.581237, RDropCLS 0.058994, RDropREG 0.007222, GateReg 0.114238) | LR: 3.361443e-05
2025-09-21 08:01:38,063 [INFO] Train step 200/274 - Loss: 0.145071 (wMSE 0.035529, SoftCE 0.241244, Corr 0.238791, Cons 0.049412, CSim 0.043957, Rank 0.580979, RDropCLS 0.058767, RDropREG 0.007206, GateReg 0.114219) | LR: 2.606547e-05
2025-09-21 08:01:51,321 [INFO] Train step 250/274 - Loss: 0.144975 (wMSE 0.035518, SoftCE 0.241240, Corr 0.237821, Cons 0.048666, CSim 0.044036, Rank 0.581118, RDropCLS 0.058390, RDropREG 0.007175, GateReg 0.114342) | LR: 1.934857e-05
2025-09-21 08:01:57,761 [INFO] Epoch training completed in 73.29s with average loss 0.145007
2025-09-21 08:01:57,762 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:01:58,906 [INFO] Fold 3 - Epoch 5 - Train Loss: 0.145007 | EMA Val wMSE: 0.066923 | EMA Val SoftCE: 0.264125 | EMA Val Pearson: 0.572503
2025-09-21 08:01:58,910 [INFO] Fold 3 - Epoch 5 - New best EMA model with Pearson 0.572503.
2025-09-21 08:01:58,910 [INFO] Fold 3 - Epoch 6/6 started.
2025-09-21 08:02:12,514 [INFO] Train step 50/274 - Loss: 0.143853 (wMSE 0.034327, SoftCE 0.240745, Corr 0.233579, Cons 0.048402, CSim 0.042618, Rank 0.582765, RDropCLS 0.059279, RDropREG 0.007010, GateReg 0.114098) | LR: 1.109289e-05
2025-09-21 08:02:25,819 [INFO] Train step 100/274 - Loss: 0.142803 (wMSE 0.034086, SoftCE 0.239523, Corr 0.228161, Cons 0.046589, CSim 0.042916, Rank 0.580675, RDropCLS 0.059533, RDropREG 0.007213, GateReg 0.114328) | LR: 6.743759e-06
2025-09-21 08:02:39,154 [INFO] Train step 150/274 - Loss: 0.142021 (wMSE 0.033645, SoftCE 0.238575, Corr 0.225712, Cons 0.046314, CSim 0.042717, Rank 0.579679, RDropCLS 0.059931, RDropREG 0.007286, GateReg 0.113870) | LR: 3.444140e-06
2025-09-21 08:02:52,458 [INFO] Train step 200/274 - Loss: 0.141944 (wMSE 0.033673, SoftCE 0.238292, Corr 0.225950, Cons 0.046006, CSim 0.042605, Rank 0.579502, RDropCLS 0.059606, RDropREG 0.007310, GateReg 0.113878) | LR: 1.231166e-06
2025-09-21 08:03:05,798 [INFO] Train step 250/274 - Loss: 0.141723 (wMSE 0.033537, SoftCE 0.238429, Corr 0.224206, Cons 0.045753, CSim 0.042686, Rank 0.578563, RDropCLS 0.059531, RDropREG 0.007322, GateReg 0.114247) | LR: 1.297403e-07
2025-09-21 08:03:12,306 [INFO] Epoch training completed in 73.40s with average loss 0.141660
2025-09-21 08:03:12,308 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:03:13,494 [INFO] Fold 3 - Epoch 6 - Train Loss: 0.141660 | EMA Val wMSE: 0.067121 | EMA Val SoftCE: 0.262968 | EMA Val Pearson: 0.582774
2025-09-21 08:03:13,498 [INFO] Fold 3 - Epoch 6 - New best EMA model with Pearson 0.582774.
2025-09-21 08:03:14,340 [INFO] Fold 3 - Final Val Pearson (no TTA): 0.582774 | Final Val wMSE: 0.067121 | Final Val SoftCE: 0.262968
2025-09-21 08:03:15,099 [INFO] Fold 3 - Final Val Pearson (with TTA swap): 0.586635
2025-09-21 08:03:16,093 [INFO] ========== Fold 5/5 ==========
2025-09-21 08:03:20,459 [INFO] Applied swap augmentation. New train size: 52526
2025-09-21 08:03:20,470 [INFO] Prepared swapped TTA arrays for validation data.
2025-09-21 08:03:20,470 [INFO] Train split: 52526 samples; Val split: 6562 samples.
2025-09-21 08:03:20,470 [INFO] Initializing PatentDataset with 52526 samples.
2025-09-21 08:03:20,470 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 08:03:20,470 [INFO] Initializing PatentDataset with 6562 samples.
2025-09-21 08:03:20,472 [INFO] Initializing TransformerRegressor++ with vocab_size=9042, embed_dim=256, heads=8, layers=4, max_len=64, ctxs=106, feat_dim=8, msd=5
2025-09-21 08:03:20,646 [INFO] Creating Warmup+Cosine scheduler: warmup_steps=164, total_steps=1644.
2025-09-21 08:03:20,646 [INFO] Initializing EMA state from current model weights.
2025-09-21 08:03:20,803 [INFO] Fold 4 - Epoch 1/6 started.
2025-09-21 08:03:34,556 [INFO] Train step 50/274 - Loss: 0.335611 (wMSE 0.131421, SoftCE 0.421899, Corr 0.966663, Cons 0.152488, CSim 0.126785, Rank 0.695247, RDropCLS 0.677060, RDropREG 0.076159, GateReg 0.194541) | LR: 6.097561e-05
2025-09-21 08:03:47,850 [INFO] Train step 100/274 - Loss: 0.305587 (wMSE 0.117905, SoftCE 0.381037, Corr 0.894984, Cons 0.174123, CSim 0.100687, Rank 0.687469, RDropCLS 0.534710, RDropREG 0.048413, GateReg 0.152100) | LR: 1.219512e-04
2025-09-21 08:04:01,129 [INFO] Train step 150/274 - Loss: 0.288608 (wMSE 0.110288, SoftCE 0.357998, Corr 0.864376, Cons 0.159971, CSim 0.089324, Rank 0.683386, RDropCLS 0.426535, RDropREG 0.039420, GateReg 0.128015) | LR: 1.829268e-04
2025-09-21 08:04:14,411 [INFO] Train step 200/274 - Loss: 0.275839 (wMSE 0.104844, SoftCE 0.343226, Corr 0.825935, Cons 0.153392, CSim 0.083221, Rank 0.679537, RDropCLS 0.351696, RDropREG 0.033101, GateReg 0.112269) | LR: 1.997082e-04
2025-09-21 08:04:27,667 [INFO] Train step 250/274 - Loss: 0.266422 (wMSE 0.100537, SoftCE 0.333119, Corr 0.795679, Cons 0.148670, CSim 0.079671, Rank 0.677117, RDropCLS 0.296578, RDropREG 0.028187, GateReg 0.102160) | LR: 1.983384e-04
2025-09-21 08:04:34,130 [INFO] Epoch training completed in 73.33s with average loss 0.262143
2025-09-21 08:04:34,131 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:04:35,309 [INFO] Fold 4 - Epoch 1 - Train Loss: 0.262143 | EMA Val wMSE: 0.139392 | EMA Val SoftCE: 0.306872 | EMA Val Pearson: 0.271291
2025-09-21 08:04:35,313 [INFO] Fold 4 - Epoch 1 - New best EMA model with Pearson 0.271291.
2025-09-21 08:04:35,313 [INFO] Fold 4 - Epoch 2/6 started.
2025-09-21 08:04:48,958 [INFO] Train step 50/274 - Loss: 0.207024 (wMSE 0.071145, SoftCE 0.284092, Corr 0.537121, Cons 0.115169, CSim 0.062772, Rank 0.648597, RDropCLS 0.067765, RDropREG 0.008102, GateReg 0.077856) | LR: 1.942877e-04
2025-09-21 08:05:02,229 [INFO] Train step 100/274 - Loss: 0.205236 (wMSE 0.070445, SoftCE 0.281482, Corr 0.532615, Cons 0.113926, CSim 0.061902, Rank 0.646742, RDropCLS 0.062489, RDropREG 0.007619, GateReg 0.080185) | LR: 1.902280e-04
2025-09-21 08:05:15,523 [INFO] Train step 150/274 - Loss: 0.203794 (wMSE 0.069951, SoftCE 0.280579, Corr 0.523938, Cons 0.111216, CSim 0.061722, Rank 0.644945, RDropCLS 0.059540, RDropREG 0.007216, GateReg 0.082241) | LR: 1.851529e-04
2025-09-21 08:05:28,829 [INFO] Train step 200/274 - Loss: 0.201916 (wMSE 0.068929, SoftCE 0.279398, Corr 0.514537, Cons 0.106683, CSim 0.061306, Rank 0.643722, RDropCLS 0.057584, RDropREG 0.006964, GateReg 0.083247) | LR: 1.791195e-04
2025-09-21 08:05:42,137 [INFO] Train step 250/274 - Loss: 0.199607 (wMSE 0.067586, SoftCE 0.277954, Corr 0.503151, Cons 0.103252, CSim 0.060440, Rank 0.641720, RDropCLS 0.056643, RDropREG 0.006768, GateReg 0.084609) | LR: 1.721956e-04
2025-09-21 08:05:48,589 [INFO] Epoch training completed in 73.28s with average loss 0.198495
2025-09-21 08:05:48,590 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:05:49,738 [INFO] Fold 4 - Epoch 2 - Train Loss: 0.198495 | EMA Val wMSE: 0.087626 | EMA Val SoftCE: 0.280810 | EMA Val Pearson: 0.466274
2025-09-21 08:05:49,742 [INFO] Fold 4 - Epoch 2 - New best EMA model with Pearson 0.466274.
2025-09-21 08:05:49,742 [INFO] Fold 4 - Epoch 3/6 started.
2025-09-21 08:06:03,361 [INFO] Train step 50/274 - Loss: 0.175859 (wMSE 0.054341, SoftCE 0.263086, Corr 0.378857, Cons 0.077529, CSim 0.053954, Rank 0.616496, RDropCLS 0.055494, RDropREG 0.006895, GateReg 0.097108) | LR: 1.604825e-04
2025-09-21 08:06:16,629 [INFO] Train step 100/274 - Loss: 0.174227 (wMSE 0.053266, SoftCE 0.261298, Corr 0.374587, Cons 0.073538, CSim 0.053465, Rank 0.614913, RDropCLS 0.055856, RDropREG 0.007078, GateReg 0.099253) | LR: 1.517058e-04
2025-09-21 08:06:29,913 [INFO] Train step 150/274 - Loss: 0.173850 (wMSE 0.053250, SoftCE 0.260949, Corr 0.371991, Cons 0.073142, CSim 0.053396, Rank 0.614457, RDropCLS 0.054465, RDropREG 0.006878, GateReg 0.100985) | LR: 1.423473e-04
2025-09-21 08:06:43,183 [INFO] Train step 200/274 - Loss: 0.172499 (wMSE 0.052458, SoftCE 0.260239, Corr 0.364642, Cons 0.071786, CSim 0.053050, Rank 0.612943, RDropCLS 0.054482, RDropREG 0.006857, GateReg 0.102031) | LR: 1.325122e-04
2025-09-21 08:06:56,481 [INFO] Train step 250/274 - Loss: 0.171416 (wMSE 0.051716, SoftCE 0.259709, Corr 0.359793, Cons 0.069393, CSim 0.052489, Rank 0.611639, RDropCLS 0.054999, RDropREG 0.006932, GateReg 0.102425) | LR: 1.223112e-04
2025-09-21 08:07:02,924 [INFO] Epoch training completed in 73.18s with average loss 0.170761
2025-09-21 08:07:02,925 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:07:04,078 [INFO] Fold 4 - Epoch 3 - Train Loss: 0.170761 | EMA Val wMSE: 0.072388 | EMA Val SoftCE: 0.271345 | EMA Val Pearson: 0.542255
2025-09-21 08:07:04,082 [INFO] Fold 4 - Epoch 3 - New best EMA model with Pearson 0.542255.
2025-09-21 08:07:04,083 [INFO] Fold 4 - Epoch 4/6 started.
2025-09-21 08:07:17,723 [INFO] Train step 50/274 - Loss: 0.155515 (wMSE 0.041837, SoftCE 0.248393, Corr 0.287512, Cons 0.055388, CSim 0.047263, Rank 0.594667, RDropCLS 0.058534, RDropREG 0.007184, GateReg 0.107837) | LR: 1.067874e-04
2025-09-21 08:07:31,021 [INFO] Train step 100/274 - Loss: 0.156006 (wMSE 0.042342, SoftCE 0.248935, Corr 0.288278, Cons 0.055526, CSim 0.047595, Rank 0.594622, RDropCLS 0.056990, RDropREG 0.007298, GateReg 0.108192) | LR: 9.618007e-05
2025-09-21 08:07:44,282 [INFO] Train step 150/274 - Loss: 0.155842 (wMSE 0.042236, SoftCE 0.249129, Corr 0.286793, Cons 0.055664, CSim 0.047646, Rank 0.593800, RDropCLS 0.056500, RDropREG 0.007252, GateReg 0.108462) | LR: 8.561573e-05
2025-09-21 08:07:57,567 [INFO] Train step 200/274 - Loss: 0.155480 (wMSE 0.042148, SoftCE 0.248233, Corr 0.286260, Cons 0.055017, CSim 0.047507, Rank 0.594123, RDropCLS 0.056458, RDropREG 0.007166, GateReg 0.109064) | LR: 7.521326e-05
2025-09-21 08:08:10,893 [INFO] Train step 250/274 - Loss: 0.155010 (wMSE 0.041873, SoftCE 0.248003, Corr 0.284097, Cons 0.054137, CSim 0.047189, Rank 0.593106, RDropCLS 0.056645, RDropREG 0.007254, GateReg 0.109684) | LR: 6.508975e-05
2025-09-21 08:08:17,363 [INFO] Epoch training completed in 73.28s with average loss 0.154821
2025-09-21 08:08:17,364 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:08:18,511 [INFO] Fold 4 - Epoch 4 - Train Loss: 0.154821 | EMA Val wMSE: 0.067583 | EMA Val SoftCE: 0.264313 | EMA Val Pearson: 0.578880
2025-09-21 08:08:18,518 [INFO] Fold 4 - Epoch 4 - New best EMA model with Pearson 0.578880.
2025-09-21 08:08:18,518 [INFO] Fold 4 - Epoch 5/6 started.
2025-09-21 08:08:32,152 [INFO] Train step 50/274 - Loss: 0.147585 (wMSE 0.037467, SoftCE 0.242634, Corr 0.248185, Cons 0.050231, CSim 0.045475, Rank 0.584818, RDropCLS 0.059135, RDropREG 0.007732, GateReg 0.110693) | LR: 5.086032e-05
2025-09-21 08:08:45,425 [INFO] Train step 100/274 - Loss: 0.147353 (wMSE 0.037189, SoftCE 0.242738, Corr 0.247882, Cons 0.048742, CSim 0.044937, Rank 0.584512, RDropCLS 0.058399, RDropREG 0.007641, GateReg 0.111152) | LR: 4.191050e-05
2025-09-21 08:08:58,689 [INFO] Train step 150/274 - Loss: 0.146988 (wMSE 0.036985, SoftCE 0.242042, Corr 0.247509, Cons 0.048100, CSim 0.044782, Rank 0.584666, RDropCLS 0.058503, RDropREG 0.007528, GateReg 0.110661) | LR: 3.361443e-05
2025-09-21 08:09:11,960 [INFO] Train step 200/274 - Loss: 0.147022 (wMSE 0.036918, SoftCE 0.242182, Corr 0.248300, Cons 0.047503, CSim 0.044444, Rank 0.584586, RDropCLS 0.058412, RDropREG 0.007525, GateReg 0.110675) | LR: 2.606547e-05
2025-09-21 08:09:25,231 [INFO] Train step 250/274 - Loss: 0.146618 (wMSE 0.036771, SoftCE 0.241770, Corr 0.246088, Cons 0.047563, CSim 0.044530, Rank 0.584042, RDropCLS 0.058240, RDropREG 0.007444, GateReg 0.111264) | LR: 1.934857e-05
2025-09-21 08:09:31,699 [INFO] Epoch training completed in 73.18s with average loss 0.146530
2025-09-21 08:09:31,700 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:09:32,856 [INFO] Fold 4 - Epoch 5 - Train Loss: 0.146530 | EMA Val wMSE: 0.067043 | EMA Val SoftCE: 0.260728 | EMA Val Pearson: 0.593909
2025-09-21 08:09:32,860 [INFO] Fold 4 - Epoch 5 - New best EMA model with Pearson 0.593909.
2025-09-21 08:09:32,861 [INFO] Fold 4 - Epoch 6/6 started.
2025-09-21 08:09:46,455 [INFO] Train step 50/274 - Loss: 0.144636 (wMSE 0.035193, SoftCE 0.241619, Corr 0.234160, Cons 0.049338, CSim 0.043983, Rank 0.580588, RDropCLS 0.060410, RDropREG 0.007586, GateReg 0.112403) | LR: 1.109289e-05
2025-09-21 08:09:59,759 [INFO] Train step 100/274 - Loss: 0.143862 (wMSE 0.034786, SoftCE 0.240930, Corr 0.231403, Cons 0.046780, CSim 0.043549, Rank 0.579595, RDropCLS 0.060170, RDropREG 0.007683, GateReg 0.112993) | LR: 6.743759e-06
2025-09-21 08:10:13,062 [INFO] Train step 150/274 - Loss: 0.143371 (wMSE 0.034648, SoftCE 0.240179, Corr 0.229715, Cons 0.046994, CSim 0.043556, Rank 0.578240, RDropCLS 0.060642, RDropREG 0.007660, GateReg 0.112907) | LR: 3.444140e-06
2025-09-21 08:10:26,328 [INFO] Train step 200/274 - Loss: 0.143019 (wMSE 0.034460, SoftCE 0.239249, Corr 0.230252, Cons 0.046405, CSim 0.043220, Rank 0.578698, RDropCLS 0.060426, RDropREG 0.007610, GateReg 0.112518) | LR: 1.231166e-06
2025-09-21 08:10:39,575 [INFO] Train step 250/274 - Loss: 0.142851 (wMSE 0.034384, SoftCE 0.238897, Corr 0.229864, Cons 0.046183, CSim 0.043259, Rank 0.578935, RDropCLS 0.060448, RDropREG 0.007566, GateReg 0.112609) | LR: 1.297403e-07
2025-09-21 08:10:46,037 [INFO] Epoch training completed in 73.18s with average loss 0.142833
2025-09-21 08:10:46,038 [INFO] Evaluating with a provided state dict (e.g., EMA).
2025-09-21 08:10:47,192 [INFO] Fold 4 - Epoch 6 - Train Loss: 0.142833 | EMA Val wMSE: 0.068383 | EMA Val SoftCE: 0.259867 | EMA Val Pearson: 0.598755
2025-09-21 08:10:47,196 [INFO] Fold 4 - Epoch 6 - New best EMA model with Pearson 0.598755.
2025-09-21 08:10:48,075 [INFO] Fold 4 - Final Val Pearson (no TTA): 0.598755 | Final Val wMSE: 0.068383 | Final Val SoftCE: 0.259867
2025-09-21 08:10:48,835 [INFO] Fold 4 - Final Val Pearson (with TTA swap): 0.602128
2025-09-21 08:10:49,867 [INFO] OOF Pearson correlation across all folds (pre-rule, pre-calibration): 0.597402
2025-09-21 08:10:49,868 [INFO] OOF MSE across all folds (pre-rule, pre-calibration): 0.051229
2025-09-21 08:10:49,868 [INFO] Ensembling test predictions from all folds by averaging.
2025-09-21 08:10:49,868 [INFO] Applying rule-based post-processing for anchor == target -> score = 1.0
2025-09-21 08:10:49,869 [INFO] Found 24 exact anchor==target pairs in test set.
2025-09-21 08:10:49,871 [INFO] Found 255 exact anchor==target pairs in train (OOF) set; applying same rule to OOF predictions.
2025-09-21 08:10:49,871 [INFO] OOF Pearson after rule (pre-calibration): 0.598866
2025-09-21 08:10:49,871 [INFO] OOF MSE after rule (pre-calibration): 0.051210
2025-09-21 08:10:49,871 [INFO] Fitting isotonic regression (PAV) calibrator on OOF predictions.
2025-09-21 08:10:49,913 [INFO] Isotonic produced 367 monotonic blocks.
2025-09-21 08:10:49,915 [INFO] OOF Pearson after isotonic calibration: 0.611584
2025-09-21 08:10:49,915 [INFO] OOF MSE after isotonic calibration: 0.041942
2025-09-21 08:10:49,915 [INFO] Logging final validation results (OOF, post-rule, post-calibration):
2025-09-21 08:10:49,915 [INFO] Final Validation Pearson: 0.611584 | Final Validation MSE: 0.041942
2025-09-21 08:10:49,918 [INFO] Writing submission to task/us-patent-phrase-to-phrase-matching/outputs/1/submission_9.csv
2025-09-21 08:10:49,926 [INFO] Submission file created successfully.
2025-09-21 08:10:49,926 [INFO] All done (v9).
