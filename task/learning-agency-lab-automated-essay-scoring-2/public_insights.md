### Overall Approach
- Ensemble of transformer-based deBERTa models and gradient-boosted tree models (LightGBM) trained on rich textual and structural features derived from essay paragraphs, sentences, and words, with optimized post-processing thresholds to maximize quadratic weighted kappa score. (26 recommendations)
- Use semantic similarity between train/test essays and a labeled external corpus (Persuade 2.0) to transfer scores, while falling back to a length-based baseline for unmatched essays. (1 recommendations)
- Use a pre-trained DeBERTaV3 model for ordinal regression on essay texts, predicting scores by converting discrete scores into binary ordinal labels and optimizing for weighted kappa. (1 recommendations)
- Use BERTopic with InstructorEmbedding to cluster essays into semantic topics, then leverage topic assignments as engineered features to distinguish between training sources and improve scoring generalization. (1 recommendations)
- Predict essay prompt names using a fine-tuned DeBERTa model on labeled essays, leveraging cross-validation and ensemble averaging over folds to generalize to unlabeled essays. (1 recommendations)
- Generate sentence embeddings from multiple pre-trained transformer models, concatenate them into a high-dimensional feature vector, and use SVR with cross-validation to predict essay scores, followed by optimized threshold-based post-processing to map continuous predictions to discrete score bins. (1 recommendations)
- Fine-tune a modified Gemma causal language model using LoRA for regression-based essay scoring by treating the EOS token embedding as a representation for prediction. (1 recommendations)
- Fine-tune a Mistral-7B-Instruct model using LoRA to predict essay scores by framing the task as a conditional text generation problem where the model outputs a single score after a prompt. (1 recommendations)

### Data Preprocessing
- Split text into paragraphs, sentences, and words for granular feature extraction. (3 recommendations)
- Load Persuade 2.0 human scores and train/test datasets. (1 recommendations)
- Remove unnecessary columns ('essay_id', 'full_text', 'score') after tokenization. (1 recommendations)
- Load training data from Kaggle and Persuade 2.0 datasets. (1 recommendations)
- Convert score labels to 0-based index by subtracting 1. (2 recommendations)
- Separate merged (intersection) and non-merged (difference) essays. (1 recommendations)
- Concatenate non-overlapping Persuade 2.0 essays into the training set and update source labels accordingly. (1 recommendations)
- Append EOS token and corresponding attention mask to each tokenized sequence for use as a CLS-like representation. (1 recommendations)
- Tokenize full_text using AutoTokenizer with specified max_length and truncation. (1 recommendations)
- Replace multiple commas with a single comma. (1 recommendations)
- Load precomputed training features, features matrix (X), target labels (y), and stratified split labels (y_split) from pickle files. (2 recommendations)
- Split full text into sentences using '.' and filter sentences with length < 15. (1 recommendations)
- Normalize multiple spaces to single space. (4 recommendations)
- Generate unique IDs for essays using MD5 hash of full text to identify duplicates. (1 recommendations)
- Remove numbers. (4 recommendations)
- Reduce consecutive spaces to single spaces. (8 recommendations)
- Calculate essay length by counting characters in full_text for exploration purposes. (1 recommendations)
- Normalize consecutive periods and commas to single instances. (11 recommendations)
- Remove strings starting with @ (2 recommendations)
- Collapse consecutive spaces, periods, and commas into single instances. (1 recommendations)
- Expand contractions using a predefined mapping (e.g., 'can't' → 'cannot'). (2 recommendations)
- Clean text by stripping whitespace, replacing newlines with spaces, and collapsing multiple spaces. (1 recommendations)
- Strip leading/trailing whitespace. (15 recommendations)
- Load train, test, and submission datasets using Polars from CSV files. (1 recommendations)
- Load feature selection mask from pickle to filter relevant features. (3 recommendations)
- Map the original score column (1–6) to 0–5 by subtracting 1 for model compatibility. (1 recommendations)
- Normalize whitespace and punctuation (e.g., collapse multiple dots or commas). (1 recommendations)
- Verify dataset sizes and display summary statistics for numerical columns. (1 recommendations)
- Split full text into paragraphs by '\n\n' and sentences by '.'. (3 recommendations)
- Merge datasets by UID to identify overlapping essays and label their source as 'kaggle-only', 'persuade-only', or 'kaggle-persuade'. (1 recommendations)
- Expand common English contractions (e.g., 'can't' -> 'cannot'). (1 recommendations)
- Split the train data into training and validation sets using stratified sampling by score. (1 recommendations)
- Apply feature selection by filtering X using the predefined feature_select list. (1 recommendations)
- Split full text into words by ' ' for word-level analysis. (1 recommendations)
- Merge datasets on full_text to find overlapping essays. (1 recommendations)
- Create fold identifiers using StratifiedKFold with 5 splits based on the original score to maintain label distribution across folds. (1 recommendations)
- Normalize repeated punctuation (e.g., '...' -> '.', ',' -> ','). (1 recommendations)
- Load preprocessed training features, features, targets, and split targets from pickled files. (1 recommendations)
- Create a combined score-prompt identifier for stratified splitting. (1 recommendations)
- Clean and normalize essay text by removing spaces and 'E' characters to compute length-based features. (1 recommendations)
- Create a fold column by hashing essay_id modulo 5 for cross-validation. (1 recommendations)
- Handle missing values in test data by filling with empty strings. (1 recommendations)
- Remove URLs. (12 recommendations)
- Remove email handles (@...). (1 recommendations)
- Filter out sentences or words with zero or insufficient length. (1 recommendations)
- Convert text to lowercase. (15 recommendations)
- Create a fold column using stratified k-fold splitting based on the original score to ensure balanced splits. (2 recommendations)
- Initialize a fold column in the training dataframe and assign folds using StratifiedKFold based on the adjusted score to ensure label balance across folds. (1 recommendations)
- Fill missing values in 'full_text' column with empty strings. (1 recommendations)
- Split the training data into a validation set (first 1024 rows) and a training set (next 3072 rows). (1 recommendations)
- Remove HTML tags. (15 recommendations)
- Convert the label column to float32 if using regression, or int32 if using classification. (1 recommendations)
- Remove standalone digits. (7 recommendations)
- Load test dataset from CSV file. (11 recommendations)
- Remove all punctuation characters from text. (1 recommendations)
- Adjust essay scores by subtracting 1 to convert from 1-6 range to 0-5 range for compatibility with zero-based classification. (1 recommendations)
- Convert selected feature matrix to float32 dtype for memory efficiency. (1 recommendations)
- Remove numeric characters (1 recommendations)
- Load external feedback dataset for fine-tuned embeddings. (1 recommendations)
- Load training data and external persuade corpus. (1 recommendations)
- Remove mentions (e.g., @user). (12 recommendations)
- Split full text into words using ' ' and filter out zero-length words. (1 recommendations)
- Create a dummy 'score' column in test data to match train format. (1 recommendations)
- Convert pandas DataFrame to Hugging Face Dataset. (1 recommendations)
- Tokenize text into words for word-level feature extraction. (1 recommendations)
- Replace multiple periods with a single period. (1 recommendations)

### Feature Engineering
- Filter paragraphs with length >= 20 characters. (1 recommendations)
- Construct label2id and id2label mappings for prompt classification. (1 recommendations)
- Count spelling errors per paragraph using spaCy lemmatization and a predefined English vocabulary. (2 recommendations)
- Filter out empty words. (1 recommendations)
- Identify topics present in Kaggle-only and Kaggle-Persuade data, and flag essays with 'missing' topics not found in Kaggle training. (1 recommendations)
- Calculate spell check features: misspelled word count and ratio using pyspellchecker. (1 recommendations)
- Calculate normalized Levenshtein distance between each train/test essay and its top-matched Persuade essay. (1 recommendations)
- Combine all static features (textstat, linguistic, spell check) with averaged sentence embeddings. (1 recommendations)
- Incorporate out-of-fold predictions from a pre-trained deBERTa-v3-large model as meta-features (6 probability dimensions). (9 recommendations)
- Construct a system prompt instructing the model to output only a score from 1 to 6. (1 recommendations)
- Initialize five different transformer models (DeBERTa variants, Longformer, BigBird) to generate text embeddings. (1 recommendations)
- Load and incorporate out-of-fold predictions (DeBERTa) as 6 additional feature columns. (1 recommendations)
- Tokenize full text using a pre-trained DeBERTa tokenizer to compute sequence lengths. (2 recommendations)
- Apply tokenization with padding, truncation to max length (1024 for training, 2048 for inference), and set labels equal to input_ids for causal language modeling. (1 recommendations)
- Calculate and log the distribution of tokenized text lengths to validate maximum sequence length choice. (1 recommendations)
- Tokenize each essay text using DeBERTa-v3-base tokenizer with max length 512, padding to maximum length, and truncation. (1 recommendations)
- Generate TF-IDF-style embeddings (CountVectorizer with 1024 features) for all essays after removing English stop words. (1 recommendations)
- Apply CountVectorizer on n-grams (3–5) to full text, producing additional dense numerical features. (10 recommendations)
- Generate word frequency distributions and visualize word clouds per data source. (1 recommendations)
- Apply UMAP for dimensionality reduction and HDBSCAN for clustering to detect semantic topics. (1 recommendations)
- Extract sentence embeddings via mean pooling of transformer outputs with attention masking. (1 recommendations)
- Project the EOS token embedding through a final linear layer to produce a single scalar prediction. (1 recommendations)
- Split full_text by '\n\n' to extract paragraphs. (88 recommendations)
- Format each essay as a chat template with user input (prompt + essay) and assistant output (score), appending 'The score is: ' as a prefix for generation. (1 recommendations)
- Normalize embeddings using L2 normalization for cosine similarity computation. (1 recommendations)
- Extract the hidden state corresponding to the EOS token position from the model's output logits. (1 recommendations)
- Create a length-based baseline predictor by grouping train essays by character length (after cleaning) and assigning the most frequent score per group, then applying cumulative maximum smoothing. (1 recommendations)
- Generate DeBERTa transformer embeddings from pre-trained models and compute softmax-averaged class probabilities. (1 recommendations)
- Prepare input sequences with tokenization, padding to max length, and truncation. (1 recommendations)
- Tokenize the 'full_text' column using a pre-trained tokenizer with max length 1024 and truncation. (2 recommendations)
- Compute token ratio features by dividing raw tag/POS/col counts by word count. (1 recommendations)
- Select top 13,000 most important features per fold using feature importance from prior LightGBM models. (3 recommendations)
- Generate a fold column using StratifiedKFold on score-prompt combinations to preserve label distribution. (1 recommendations)
- Encode essays into 768-dimensional embeddings using the Instructor-XL model. (1 recommendations)
- Assign each essay a topic ID and store topic keywords as a feature. (1 recommendations)
- Extract textstat features (e.g., Flesch-Kincaid, Gunning Fog, reading time) from full_text. (1 recommendations)
- Extract linguistic features using spaCy: NER counts, POS tags, dependency tags, tense ratios, word/sentence/paragraph counts, lexical diversity, and paragraph statistics. (1 recommendations)
- Convert prompt_name to numeric label codes for classification. (1 recommendations)
- Extract topic keywords per cluster using KeyBERTInspired with n-grams (1-3). (1 recommendations)
- Select final feature set using a pre-saved feature list from training. (1 recommendations)
- Load and merge out-of-fold predictions from an external FB3 DeBERTa family model as features. (1 recommendations)
- Normalize embeddings using L2 normalization. (1 recommendations)
- Convert discrete essay scores into ordinal regression labels using a custom function that creates binary indicator vectors for each rank threshold. (1 recommendations)
- Merge all extracted features (paragraph, sentence, word, TF-IDF, DeBERTa) into a unified feature matrix by essay_id. (1 recommendations)
- Add special tokens for newline (\n) and double space (  ) to the tokenizer to better capture essay structure. (1 recommendations)
- Apply TF-IDF vectorization on full_text with ngram_range=(1,4), min_df=0.05, max_df=0.95. (15 recommendations)
- Concatenate all embeddings from the six models along the feature axis to form a unified feature vector for each essay. (1 recommendations)
- Apply TfidfVectorizer with 3-6 n-grams to full text and create tfidf features. (1 recommendations)
- Prepare documents with an instruction prefix for InstructorEmbedding ('Represent the essay statement: '). (1 recommendations)
- Tokenize and encode full essay texts using each model's tokenizer with max length 1024. (1 recommendations)
- Create input dictionaries with token IDs, attention masks, and token type IDs as PyTorch tensors for each essay. (1 recommendations)
- Load precomputed DeBERTa out-of-fold predictions and add them as 6 additional features per essay. (1 recommendations)
- Compute the closest matching essay from Persuade 2.0 for each train/test essay via argmax of cosine similarities. (1 recommendations)
- Remove 'essay_id' and 'full_text' columns from the dataset after tokenization. (1 recommendations)
- Generate sentence embeddings using 5 folded fine-tuned DeBERTa-small models from external input, averaged across folds. (1 recommendations)
- Apply CountVectorizer on full_text with ngram_range=(2,3), min_df=0.10, max_df=0.85. (1 recommendations)
- Compute cumulative paragraph length counts at predefined thresholds (e.g., paragraphs >50, <25 chars). (1 recommendations)

### Validation Strategy
- Use stratified 5-fold cross-validation with each fold trained and validated separately, where validation performance is measured by quadratic weighted kappa score on out-of-fold predictions. (20 recommendations)
- Uses 15-fold stratified K-fold cross-validation (RSKF) for model training and evaluation, with predictions aggregated across folds for robustness. (1 recommendations)
- Use a 5-fold cross-validation approach with fold 0 as the validation set, evaluating model performance on a held-out subset during training. (1 recommendations)
- Use 5-fold KFold cross-validation with shuffling to train and validate the LightGBM model on essay scores. (1 recommendations)
- Use 2-fold stratified cross-validation based on score-prompt combinations to evaluate model performance within the intersection dataset. (1 recommendations)
- Stratified 16-fold cross-validation using essay score bins to ensure balanced distribution across folds for both LightGBM and XGBoost models. (1 recommendations)
- Implicit validation via source-based split analysis, where topic distribution overlap between Kaggle and Persuade datasets is assessed to identify generalizable vs. source-specific patterns. (1 recommendations)
- Use the first 1024 rows of the train set as a held-out validation set to evaluate model performance via quadratic weighted kappa (QWK) on predicted scores. (1 recommendations)
- Use an 80-20 stratified train-validation split to evaluate model performance on unseen data during training. (1 recommendations)
- AutoML internally uses cross-validation to optimize hyperparameters based on macro F1 score within a 600-second time budget. (1 recommendations)
- The model implicitly validates via the quadratic weighted kappa (QWK) score on the training set using the length-based baseline as a proxy. (1 recommendations)
- Use stratified 5-fold cross-validation with train/valid splits based on essay score to evaluate model performance. (1 recommendations)

### Modeling
- LightGBM regressor with custom QWK objective and evaluation metric, trained in multiple folds with early stopping and extra trees, and ensembled via model averaging. (17 recommendations)
- Use a frozen Mistral-7B-Instruct-v0.2 model with PEFT LoRA adapter for efficient fine-tuning, trained with causal language modeling objective and dynamic padding. (1 recommendations)
- Fine-tune the DeBERTa-v3-base pretrained transformer model using Hugging Face Transformers with sequence classification head for 6 score classes, trained with AdamW optimizer and early stopping based on validation QWK. (1 recommendations)
- Use a fine-tuned DeBERTa-v3-base transformer with mean pooling and a linear classification head, trained with AdamW optimizer, cosine learning rate scheduling, gradient checkpointing, and mixed-precision training, optimized for cross-entropy loss on 6 score classes. (1 recommendations)
- Use a VotingRegressor ensemble of three LightGBM regressors, each with custom quadratic weighted kappa objective function and evaluation metric, to predict continuous scores later rounded to integer essay ratings. (1 recommendations)
- Fine-tune microsoft/deberta-v3-large for sequence classification with 6 output labels using the Hugging Face Transformers Trainer API. (1 recommendations)
- Use a fine-tuned DeBERTa-v3-base model with mean pooling and a linear classifier trained for 6-class classification, loaded from pre-trained weights. (1 recommendations)
- Use a pre-trained DeBERTa-large model for sequence classification, loaded from a local checkpoint, with no retraining (inference only). (1 recommendations)
- Fine-tune a pretrained DeBERTa-v3-xsmall model for multi-class classification of prompt names using Hugging Face Transformers with AdamW optimizer and sequence length truncation to 1024 tokens. (1 recommendations)
- Use Support Vector Regression (SVR) with C=10 from RAPIDS cuML to predict continuous essay scores from concatenated transformer embeddings. (1 recommendations)
- LightGBM regressor with custom QWK objective and metric, using multi-output Ridge regression on feedback embeddings as a preliminary step to generate additional features. (1 recommendations)
- Use LightGBM classifier with hyperparameters optimized by FLAML's AutoML for classification. (1 recommendations)
- Fine-tune a Gemma-1.1-2B-It causal language model with LoRA adaptation and layer removal, using a linear head for regression and optimizing with AdamW and OneCycleLR. (1 recommendations)
- No traditional ML model is trained; instead, a hybrid approach combines nearest-neighbor score transfer from Persuade 2.0 with a deterministic length-to-score mapping baseline. (1 recommendations)
- Fine-tune DeBERTa-v3-small using Hugging Face Transformers Trainer, with regression (single output) or classification (6-class output) depending on USE_REGRESSION flag, disabling dropout during regression training, and employing AdamW optimizer with linear learning rate decay. (1 recommendations)
- Use BERTopic with InstructorEmbedding, UMAP, and HDBSCAN to perform unsupervised topic modeling on essay embeddings. (1 recommendations)
- Use a fine-tuned DeBERTaV3Classifier with a sigmoid output layer for multi-label binary classification of ordinal thresholds, optimized with Adam and trained using a custom weighted Kappa metric. (1 recommendations)

### Post Processing
- Select 'essay_id' and 'score' columns for submission. (1 recommendations)
- Export results to 'submission.csv' without index. (1 recommendations)
- Shift predicted scores up by 1 to match the original scoring range (1–6). (1 recommendations)
- Compute the mean of the 15-fold test predictions to produce a final continuous prediction for each test sample. (1 recommendations)
- Export final predictions to CSV with essay_id, prompt_name, and predicted flags. (1 recommendations)
- Predict continuous scores using averaged LightGBM models. (1 recommendations)
- Optimize five decision thresholds between score bins (1–2, 2–3, ..., 5–6) using grid search to maximize QWK on out-of-fold predictions. (1 recommendations)
- For LightGBM: Add constant offset 'a' (2.948) to predictions, clip to [1,6], and round to nearest integer (1 recommendations)
- Merge predicted scores back with test essay_ids for submission. (1 recommendations)
- Cast all predictions to integer type before submission. (6 recommendations)
- Convert predicted probabilities to discrete class labels by taking the argmax. (1 recommendations)
- Convert scores to integer type for submission. (2 recommendations)
- Combine predictions from LightGBM and XGBoost using weighted average (74.9% LightGBM, 25.1% XGBoost). (1 recommendations)
- Aggregate predictions across all fold models via average before applying final binning for submission. (1 recommendations)
- Convert model sigmoid outputs to predicted score classes by summing binary thresholds > 0.5. (1 recommendations)
- Add DeBERTa probabilities as features to LightGBM feature set for test data. (1 recommendations)
- Adjust model predictions by adding a constant 'a' to shift the output range. (1 recommendations)
- For Deep Learning (DeBERTa): Apply softmax to logits and average predictions across models, then take argmax and add 1 to map to score range [1,6] (1 recommendations)
- Create a submission DataFrame combining predicted prompts for difference essays and original prompts for intersection essays. (1 recommendations)
- Write the resulting DataFrame to a CSV file named 'submission.csv'. (1 recommendations)
- Refine predictions using optimized thresholds (e.g., [1.5, 2.5, 3.5, 4.5, 5.5]) to map continuous outputs to discrete score bins. (3 recommendations)
- Save the labeled dataset with topics and keywords to a parquet file. (1 recommendations)
- Convert model output logits to predicted class labels using argmax. (2 recommendations)
- Analyze topic distribution across sources and flag essays belonging to topics absent in Kaggle training data. (1 recommendations)
- Convert predicted label indices back to prompt names using id2label mapping. (1 recommendations)
- Round to nearest integer to produce discrete essay scores. (1 recommendations)
- Add offset 'a' (2.998) to predictions to remap from regression output to score scale. (6 recommendations)
- Format final predictions into sample submission format with essay_id and score columns. (1 recommendations)
- Ensemble predictions across all 5 folds by taking the mean of logits/predictions before applying final post-processing. (4 recommendations)
- Decode model-generated text and extract the score after the substring 'The score is: '. (1 recommendations)
- Post-process LightGBM predictions by adding offset 'a' (2.998) to reconstitute score scale. (1 recommendations)
- Add a constant 'a' (undefined variable, likely error) to predictions. (1 recommendations)
- For classification: extract predicted class via argmax and add 1 to map from 0–5 to 1–6. (1 recommendations)
- Construct submission DataFrame using essay_id and predicted scores, then save to CSV. (1 recommendations)
- Add offset (a=2.998) to revert the shifted target. (1 recommendations)
- Select 'essay_id' from test data and append predicted scores as a new column. (1 recommendations)
- Shift predicted class indices back to 1-based scoring (add 1) to match original score range. (1 recommendations)
- Apply offset adjustment (add a=2.948) to model predictions. (4 recommendations)
- Round continuous predictions to the nearest integer to align with score scale. (1 recommendations)
- Map continuous test predictions to discrete integer scores (1–6) using the optimized thresholds via pd.cut(). (1 recommendations)
- Fine-tune thresholds via grid search over small increments to maximize Cohen’s kappa on out-of-fold predictions. (1 recommendations)
- Cast final submission scores to int32 before saving. (1 recommendations)
- Apply softmax to raw model outputs to convert logits to class probabilities. (2 recommendations)
- Convert predicted class indices to discrete scores by taking the argmax. (1 recommendations)
- Default predicted score to 3 if extraction fails. (1 recommendations)
- Save the best model per fold based on validation score and load its predictions for final OOF aggregation. (1 recommendations)
- For each test essay, replace the baseline score with the Persuade 2.0 matched score if normalized Levenshtein distance is below a threshold (0.1). (1 recommendations)
- Adjust class indices by adding 1 to match the expected score range (1-6). (1 recommendations)
- Assign rounded predictions to essay_id in submission DataFrame. (1 recommendations)
- Write final rounded predictions to submission CSV. (1 recommendations)
- Round predictions to nearest integer. (33 recommendations)
- Combine DeBERTa model predictions by averaging softmax outputs to generate 6-class probabilities. (2 recommendations)
- Add 2.998 to predictions to shift scale back to 1-6 range. (1 recommendations)
- Average predictions across all 15 LightGBM models. (10 recommendations)
- Round to nearest integer to produce final scores. (1 recommendations)
- Use DeBERTa model predictions (averaged) as additional input features for LightGBM, but final submission uses only LightGBM ensemble predictions. (2 recommendations)

### Technical Stack
- accelerate (2 recommendations)
- numba (1 recommendations)
- pyspellchecker (1 recommendations)
- sys (2 recommendations)
- pathlib (1 recommendations)
- tokenizers (4 recommendations)
- umap-learn (1 recommendations)
- sklearn.feature_extraction.text.TfidfVectorizer (2 recommendations)
- scikit-learn (11 recommendations)
- collections (1 recommendations)
- catboost (2 recommendations)
- pyphen (1 recommendations)
- math (3 recommendations)
- xgboost (6 recommendations)
- sklearn (15 recommendations)
- cupy (1 recommendations)
- sklearn.metrics.cohen_kappa_score (3 recommendations)
- datasets (20 recommendations)
- itertools (2 recommendations)
- peft (2 recommendations)
- bertopic (1 recommendations)
- pandas (32 recommendations)
- keras (1 recommendations)
- openai (1 recommendations)
- warnings (4 recommendations)
- tensorflow (1 recommendations)
- spacy (9 recommendations)
- multiprocessing (2 recommendations)
- glob (10 recommendations)
- sklearn.linear_model.LinearRegression (1 recommendations)
- sklearn.metrics (5 recommendations)
- colorama (1 recommendations)
- sklearn.model_selection (7 recommendations)
- hdbscan (1 recommendations)
- re (17 recommendations)
- wandb (1 recommendations)
- random (5 recommendations)
- plotly.express (1 recommendations)
- pickle (14 recommendations)
- joblib (14 recommendations)
- einops (1 recommendations)
- ctransformers (1 recommendations)
- matplotlib (27 recommendations)
- tqdm (8 recommendations)
- cuml (1 recommendations)
- lightgbm (20 recommendations)
- psutil (1 recommendations)
- datetime (1 recommendations)
- seaborn (5 recommendations)
- polars (17 recommendations)
- json (4 recommendations)
- sklearn.metrics.ConfusionMatrixDisplay (4 recommendations)
- scipy (12 recommendations)
- wordcloud (1 recommendations)
- polyleven (1 recommendations)
- os (5 recommendations)
- hashlib (1 recommendations)
- gc (14 recommendations)
- numpy (32 recommendations)
- textstat (1 recommendations)
- torch (25 recommendations)
- optuna (2 recommendations)
- plotly (1 recommendations)
- builtins (2 recommendations)
- sklearn.ensemble.VotingRegressor (1 recommendations)
- sklearn.metrics.f1_score (2 recommendations)
- transformers (25 recommendations)
- keras_nlp (1 recommendations)
- bitsandbytes (1 recommendations)
- ctypes (1 recommendations)
- string (5 recommendations)
- logging (1 recommendations)
- copy (1 recommendations)
- InstructorEmbedding (1 recommendations)
- nltk (11 recommendations)
- flaml (1 recommendations)
- sentence-transformers (2 recommendations)
- gensim (1 recommendations)
- subprocess (1 recommendations)

