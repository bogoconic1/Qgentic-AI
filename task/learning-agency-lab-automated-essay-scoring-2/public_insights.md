### Overall Approach
- Fine-tune a pre-trained DeBERTa-v3-large model on essay text to predict scores using a quadratic weighted kappa metric for evaluation. (7 recommendations)
- Ensemble of transformer-based DeBERTa models and LGBM regressors trained on hand-crafted text features (paragraph, sentence, word statistics, TF-IDF, CountVect) to predict essay scores with optimized QWK objective. (13 recommendations)
- Ensemble of LightGBM and XGBoost models trained with a custom quadratic weighted kappa objective function, using stratified k-fold cross-validation and post-processed predictions via optimized thresholds to map continuous outputs to discrete essay scores. (3 recommendations)

### Data Preprocessing
- Remove numbers (7 recommendations)
- Load train, test, and Persuade Corpus 2.0 datasets. (2 recommendations)
- Normalize consecutive spaces, commas, and periods (6 recommendations)
- Load training data from CSV file. (9 recommendations)
- Splitting full text into paragraphs using '\n\n' (3 recommendations)
- Remove HTML tags (13 recommendations)
- Replace multiple whitespace with single space (8 recommendations)
- Filter out paragraphs and sentences with length < 15 (3 recommendations)
- Remove URLs, email-like patterns, and numeric digits. (4 recommendations)
- Split full_text by "." into sentences (2 recommendations)
- Normalize punctuation (e.g., multiple dots or commas to single). (5 recommendations)
- Remove 'essay_id' and 'full_text' columns from the dataset after tokenization. (2 recommendations)
- Filter out words with length 0 (3 recommendations)
- Remove mentions (words starting with @) (7 recommendations)
- Split training data into 5 stratified folds based on the score labels to ensure balanced class distribution across folds. (3 recommendations)
- Convert text to lowercase (10 recommendations)
- Remove standalone numbers and numeric strings. (2 recommendations)
- Converted feature matrix to float32 for memory efficiency. (2 recommendations)
- Lowercasing all text (3 recommendations)
- Remove URLs (8 recommendations)
- Expand contractions using a predefined mapping (4 recommendations)
- Split text into paragraphs, sentences, and words for granular analysis. (2 recommendations)
- Loaded precomputed training features, features, and labels from pickle files. (3 recommendations)
- Applied feature selection by filtering features using a pre-loaded feature_select list. (2 recommendations)
- Strip leading/trailing whitespace (12 recommendations)

### Feature Engineering
- Tokenize full text essays using DeBERTa-v3-base tokenizer with max length 512, padding to max length, and truncation. (3 recommendations)
- Apply CountVectorizer on full_text with 3-5 gram range (25 recommendations)
- Count sentences by length thresholds (e.g., sentence_15_cnt) (2 recommendations)
- Including DeBERTa model out-of-fold predictions as features (6 probability dimensions) (11 recommendations)
- Select top 13,000 features using feature importance from LightGBM with StratifiedKFold (2 recommendations)
- Merge all paragraph, sentence, word, and TF-IDF features into a single feature set per essay (2 recommendations)
- Extracting paragraph-level features: count of paragraphs by length thresholds, min/mean/max/first/last of paragraph length, sentence count, and word count (68 recommendations)
- Tokenize full_text using DeBERTa-v3-large tokenizer with max length of 512. (5 recommendations)

### Validation Strategy
- 15-fold StratifiedKFold cross-validation using essay score classes to ensure balanced fold distribution and evaluate model performance via F1-score and Cohen's quadratic kappa. (6 recommendations)
- Stratified k-fold cross-validation with 5 folds to evaluate model performance, ensuring each fold has balanced score distributions. (6 recommendations)
- Use an 80-20 stratified train-validation split based on score to evaluate model performance on the weighted Kappa metric. (9 recommendations)

### Modeling
- LightGBM regressor trained with a custom quadratic weighted kappa objective function, ensembled across 15 folds with feature selection and early stopping. (14 recommendations)
- Fine-tune microsoft/deberta-v3-large as a sequence classification model with 6 output labels using Hugging Face Transformers Trainer. (2 recommendations)

### Post Processing
- Clip continuous predictions to range [1, 6] (17 recommendations)
- Round predictions to nearest integer (11 recommendations)
- Ensure final submission scores are cast to integer type. (7 recommendations)
- Convert final aggregated predictions to integer type for submission (2 recommendations)
- Apply softmax to model logits to convert to probabilities before argmax prediction. (2 recommendations)
- Ensembled predictions across all 5 folds by averaging logits before post-processing. (3 recommendations)
- Average predictions across all LightGBM models (14 recommendations)

### Technical Stack
- scikit-learn (10 recommendations)
- sklearn (18 recommendations)
- copy (2 recommendations)
- transformers (24 recommendations)
- multiprocessing (2 recommendations)
- matplotlib (26 recommendations)
- numpy (31 recommendations)
- nltk (11 recommendations)
- warnings (5 recommendations)
- datasets (19 recommendations)
- re (17 recommendations)
- tokenizers (4 recommendations)
- random (4 recommendations)
- sentence-transformers (2 recommendations)
- optuna (2 recommendations)
- torch (23 recommendations)
- tqdm (7 recommendations)
- string (3 recommendations)
- glob (7 recommendations)
- pickle (14 recommendations)
- peft (2 recommendations)
- seaborn (5 recommendations)
- joblib (13 recommendations)
- json (3 recommendations)
- sklearn.metrics.accuracy_score (2 recommendations)
- spacy (8 recommendations)
- pandas (31 recommendations)
- scipy (12 recommendations)
- os (4 recommendations)
- accelerate (2 recommendations)
- sklearn.metrics.ConfusionMatrixDisplay (2 recommendations)
- math (2 recommendations)
- gc (10 recommendations)
- sklearn.metrics.cohen_kappa_score (2 recommendations)
- lightgbm (19 recommendations)
- sys (2 recommendations)
- polars (16 recommendations)
- sklearn.model_selection.KFold (3 recommendations)
- xgboost (5 recommendations)

