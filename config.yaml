llm:
  developer_model: "gpt-5"
  developer_tool_model: "gemini-3-pro-preview"
  researcher_model: "claude-sonnet-4-5"
  model_selector_model: "gemini-3-pro-preview"
  model_recommender_model: "gemini-3-pro-preview"
  paper_summary_model: "gemini-3-pro-preview"
  starter_model: "gemini-3-pro-preview"
  leakage_review_model: "gemini-3-pro-preview"
  finetuned_code_api_model: "projects/134356426507/locations/us-central1/endpoints/7532524363963170816"
runtime:
  researcher_max_steps: 512
  llm_max_retries: 3
  max_developer_input_tokens: 250000  # Max input tokens before adaptive trimming (GPT-5 limit is 272K)
  directory_listing_max_files: 10
  # Parallel baseline execution
  baseline_max_parallel_workers: 3  # Max baselines running in parallel when GPU isolation is disabled (ignored if enable_mig or enable_multi_gpu is true)
  # GPU isolation strategy (choose ONE or neither):
  enable_mig: false  # Use NVIDIA MIG for single-GPU partitioning (auto-detects number of workers from MIG instances)
  enable_multi_gpu: true  # Use multi-GPU setup (auto-detects number of workers from available GPUs)
  allowed_gpu_ids: []  # Optional: Restrict to specific GPU IDs. If empty or null, uses all detected GPUs.
  enable_cpu_affinity: true  # Pin processes to specific CPU cores
  # Conda environment isolation
  reset_conda_envs_per_run: true # If true, delete and recreate conda environments on each run for clean slate (slower startup). If false, reuse existing environments (faster but accumulates packages over time)
  # Time limits (in seconds)
  baseline_time_limit: 432000  # 5 days for baseline development
  baseline_code_timeout: 43200  # 12 hours for baseline code execution
  # Grading strategy
  use_validation_score: true  # If true, use validation score from logs instead of MLE-bench grading (faster, no ground truth needed)
paths:
  task_root: "task"
  outputs_dirname: "outputs"
  external_data_dirname: "external-data"
guardrails:
  logging_basicconfig_order: true
  leakage_review: true
  enable_code_safety: true
tracking:
  wandb:
    entity: bogoconic1 # replace with your W&B entity
    project: qgentic-ai # replace with your W&B project
model_recommender:
  enable_web_search: true  # Enable web search for finding SOTA strategies
developer:
  hitl_sota: false  # When true, pause after SOTA suggestion for human accept/override (forces single worker)