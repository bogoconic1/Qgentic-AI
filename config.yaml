llm:
  developer_model: "gpt-5"
  developer_tool_model: "gpt-5"
  researcher_model: "gpt-5"
  researcher_tool_offline_model: "gpt-5"
  researcher_tool_online_model: "gpt-5"
  leakage_review_model: "gpt-5-mini"
  leakage_followup_model: "gpt-5-mini"
  model_recommender_model: "gpt-5"
  starter_model: "gpt-5"
runtime:
  ask_eda_max_attempts: 3
  researcher_max_steps: 512
  llm_max_retries: 3
  directory_listing_max_files: 10
  researcher_parallel_runs: 1
  patch_mode_enabled: false
  # Parallel baseline execution
  baseline_max_parallel_workers: 3  # Max baselines running in parallel when GPU isolation is disabled (ignored if enable_mig or enable_multi_gpu is true)
  # GPU isolation strategy (choose ONE or neither):
  enable_mig: false  # Use NVIDIA MIG for single-GPU partitioning (auto-detects number of workers from MIG instances)
  enable_multi_gpu: true  # Use multi-GPU setup (auto-detects number of workers from available GPUs)
  enable_cpu_affinity: true  # Pin processes to specific CPU cores
  # Conda environment isolation
  reset_conda_envs_per_run: true  # If true, delete and recreate conda environments on each run for clean slate (slower startup). If false, reuse existing environments (faster but accumulates packages over time)
  # Time limit for developer phase (in seconds)
  baseline_time_limit: 14400  # 4 hours for baseline development (extended from 2h since Phase 5 removed)
paths:
  task_root: "task"
  outputs_dirname: "outputs"
  external_data_dirname: "external-data"
guardrails:
  logging_basicconfig_order: true
  nan_guard: false
  leakage_review: false
tracking:
  wandb:
    entity: 520f8592abc # replace with your W&B entity
    project: qgentic-ai # replace with your W&B project
model_recommender:
  default_models: ["deberta-v3-large"]  # List of models to recommend strategies for
  enable_web_search: true  # Enable web search for finding SOTA strategies
