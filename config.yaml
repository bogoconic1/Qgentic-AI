llm:
  developer_model: "gpt-5"
  developer_tool_model: "gemini-3-pro-preview"
  researcher_model: "claude-sonnet-4-5"
  researcher_tool_offline_model: "gemini-3-pro-preview"
  researcher_tool_online_model: "gemini-3-pro-preview"
  model_selector_model: "gemini-3-pro-preview"
  model_recommender_model: "gemini-3-pro-preview"
  paper_summary_model: "gemini-3-pro-preview"
  starter_model: "gemini-3-pro-preview"
  ensembler_model: "gemini-3-pro-preview"
  leakage_review_model: "gemini-3-pro-preview"
  finetuned_code_api_model: "projects/134356426507/locations/us-central1/endpoints/7532524363963170816"
runtime:
  ask_eda_max_attempts: 5
  download_datasets_max_attempts: 1
  researcher_max_steps: 512
  llm_max_retries: 3
  directory_listing_max_files: 10
  researcher_parallel_runs: 1
  patch_mode_enabled: false
  # Parallel baseline execution
  baseline_max_parallel_workers: 3  # Max baselines running in parallel when GPU isolation is disabled (ignored if enable_mig or enable_multi_gpu is true)
  # GPU isolation strategy (choose ONE or neither):
  enable_mig: false  # Use NVIDIA MIG for single-GPU partitioning (auto-detects number of workers from MIG instances)
  enable_multi_gpu: true  # Use multi-GPU setup (auto-detects number of workers from available GPUs)
  allowed_gpu_ids: []  # Optional: Restrict to specific GPU IDs. If empty or null, uses all detected GPUs.
  enable_cpu_affinity: true  # Pin processes to specific CPU cores
  # Conda environment isolation
  reset_conda_envs_per_run: true # If true, delete and recreate conda environments on each run for clean slate (slower startup). If false, reuse existing environments (faster but accumulates packages over time)
  # Time limits (in seconds)
  baseline_time_limit: 432000  # 3 hours for baseline development
  ensemble_time_limit: 14400  # 4 hours for ensemble phase
  baseline_code_timeout: 43200 # 1.5 hours for baseline code execution
  ensemble_code_timeout: 10800  # 3 hours for ensemble code execution
  # Grading strategy
  use_validation_score: true  # If true, use validation score from logs instead of MLE-bench grading (faster, no ground truth needed)
paths:
  task_root: "task"
  outputs_dirname: "outputs"
  external_data_dirname: "external-data"
guardrails:
  logging_basicconfig_order: true
  leakage_review: true
  enable_code_safety: true
tracking:
  wandb:
    entity: bogoconic1 # replace with your W&B entity
    project: qgentic-ai # replace with your W&B project
researcher:
  hitl_instructions: []  # Human-In-The-Loop: If non-empty, these instructions are added to the researcher's system prompt. Example: ["Focus on time-series cross-validation", "Analyze seasonality patterns", "Consider external weather data"]
model_recommender:
  hitl_models: []  # Human-In-The-Loop: If non-empty, use these models instead of LLM selection. Example: ["xgboost", "lightgbm", "deberta-v3-large"]
  enable_web_search: true  # Enable web search for finding SOTA strategies
developer:
  hitl_instructions: []  # Human-In-The-Loop: If non-empty, these instructions are added to the developer's system prompt to guide implementation. Example: ["Use gradient clipping to prevent exploding gradients", "Implement mixed precision training", "Focus on domain-specific data augmentation"]