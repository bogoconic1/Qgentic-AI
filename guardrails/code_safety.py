"""
LLM-based code safety checker for generated code.
Uses Gemini 2.5 Flash for fast, intelligent security analysis.

Focuses on critical security issues only:
- Code execution (eval, exec, compile)
- Command injection (os.system, subprocess with shell=True)
- Credential leakage

Does NOT restrict file system access - trusts LLM code generation.
"""

import logging
from typing import Dict, Any

from tools.helpers import call_llm_with_retry_google
from schemas.guardrails import CodeSafetyCheck
from project_config import get_config

logger = logging.getLogger(__name__)

_CONFIG = get_config()
_LLM_CFG = _CONFIG.get("llm")
_SAFETY_MODEL = "gemini-2.5-flash"  # Latest Flash model (2025)


def build_safety_prompt() -> str:
    """Build the system prompt for code safety analysis."""
    return """You are a security analyzer for autonomous code generation in a Kaggle competition system.

Your job is to determine if generated Python code is SAFE to execute.

**Context:**
- Code is generated by an AI agent for Kaggle ML competitions
- Code will execute with full Python privileges on the host machine
- System has timeouts (90 minutes) and conda environment isolation
- We trust the LLM to generate reasonable file operations

**Security Policy - ONLY BLOCK THESE:**

CRITICAL (ALWAYS BLOCK):
1. Direct code execution: eval(), exec(), compile()
2. Dynamic imports: __import__() with unsanitized input
3. OS command injection: os.system() with unsanitized/concatenated strings
4. Subprocess shell injection: subprocess with shell=True and string concatenation
5. Hardcoded secrets: API keys, passwords, tokens in plaintext
6. Obvious data exfiltration: requests.post to suspicious domains with sensitive data

**EXPLICITLY ALLOWED (DO NOT BLOCK):**
- File operations anywhere (open, read, write, delete) - trust LLM
- Path traversal (../, absolute paths) - trust LLM
- Network requests to any domain - trust LLM
- Standard library imports (os, subprocess, sys, etc.) - trust LLM
- subprocess calls WITHOUT shell=True - safe
- subprocess with shell=True IF using safe list arguments (not string concat)

**Analysis Guidelines:**
- Be LENIENT - only block obvious security disasters
- Trust the LLM to generate reasonable code
- File system access is NOT a security issue here
- Focus on: eval/exec, command injection, credential leakage
- If unsure, ALLOW (err on the side of permissiveness)

**Examples:**

BLOCK:
```python
eval(user_input)  # Direct code execution
os.system("curl " + url)  # Command injection via concat
api_key = "sk-1234567890abcdef"  # Hardcoded secret
```

ALLOW:
```python
open("/etc/passwd")  # File access is OK
open("../../../data.csv")  # Path traversal is OK
subprocess.run(["curl", url])  # Safe subprocess (no shell)
os.makedirs("/tmp/models")  # Directory creation is OK
```

Return your analysis as structured output."""


def build_safety_user_prompt(code: str) -> str:
    """Build the user prompt with code to analyze."""
    return f"""Analyze this Python code for CRITICAL security issues only:

**Focus on:**
- eval(), exec(), compile() usage
- os.system() with string concatenation
- subprocess with shell=True and unsafe input
- Hardcoded API keys/secrets

**Ignore:**
- File operations (allowed anywhere)
- Network requests (allowed anywhere)
- Import statements (all allowed)

**Code to Analyze:**
```python
{code}
```

Return:
- decision: "allow" or "block"
- confidence: 0.0-1.0 (how certain you are)
- reasoning: Explain your decision
- violations: List ONLY critical issues (empty if none)
- suggested_fix: How to fix critical issues (if any)"""


def check_code_safety(code: str) -> Dict[str, Any]:
    """
    Check if generated code is safe to execute using LLM analysis.

    Focuses on critical security issues only (eval, exec, command injection, secrets).
    Does NOT restrict file system access.

    Args:
        code: Python code to analyze

    Returns:
        Dict with:
        - decision: "proceed" or "block"
        - reasoning: Explanation
        - violations: List of critical issues
        - suggested_fix: How to fix (if blocked)
        - confidence: 0.0-1.0
    """
    logger.info("Running LLM-based code safety check (critical issues only)...")

    try:
        system_prompt = build_safety_prompt()
        user_prompt = build_safety_user_prompt(code)

        response = call_llm_with_retry_google(
            model=_SAFETY_MODEL,
            system_instruction=system_prompt,
            user_prompt=user_prompt,
            text_format=CodeSafetyCheck,
            temperature=0.0,  # Deterministic for security
            max_retries=3,
            enable_google_search=False,
            top_p=0.95,
            thinking_budget=None,
        )

        # Parse structured output
        if response and hasattr(response, 'decision'):
            decision = "proceed" if response.decision == "allow" else "block"

            logger.info(
                "Code safety check: %s (confidence: %.2f)",
                decision,
                response.confidence
            )

            if response.violations:
                logger.warning(
                    "CRITICAL security violations found: %s",
                    ", ".join(response.violations)
                )

            return {
                "decision": decision,
                "reasoning": response.reasoning,
                "violations": response.violations,
                "suggested_fix": response.suggested_fix,
                "confidence": response.confidence,
            }

        # Fallback if structured output fails
        logger.warning("Structured output parsing failed, defaulting to ALLOW (lenient)")
        return {
            "decision": "proceed",
            "reasoning": "Failed to parse LLM response, defaulting to allow",
            "violations": [],
            "suggested_fix": "",
            "confidence": 0.5,
        }

    except Exception as e:
        logger.exception("Code safety check failed with exception")
        # Fail open (lenient) - allow execution on errors
        logger.warning("Allowing code execution despite safety check failure")
        return {
            "decision": "proceed",
            "reasoning": f"Safety check error: {str(e)}, defaulting to allow",
            "violations": [],
            "suggested_fix": "",
            "confidence": 0.0,
        }


def format_safety_feedback(safety_result: Dict[str, Any]) -> str:
    """
    Format safety check results as feedback for the LLM to fix issues.

    Args:
        safety_result: Result from check_code_safety()

    Returns:
        Formatted message for LLM
    """
    if safety_result["decision"] == "proceed":
        return "Code passed security checks."

    lines = ["Code BLOCKED by security guardrails (CRITICAL ISSUES):\n"]

    # Add reasoning
    lines.append(f"**Reasoning:** {safety_result['reasoning']}\n")

    # List violations
    if safety_result["violations"]:
        lines.append("**Critical Violations:**")
        for violation in safety_result["violations"]:
            lines.append(f"  - {violation}")
        lines.append("")

    # Add fix suggestion
    if safety_result["suggested_fix"]:
        lines.append(f"**How to fix:**\n{safety_result['suggested_fix']}\n")

    lines.append("Please regenerate the code addressing these CRITICAL security issues.")

    return "\n".join(lines)
